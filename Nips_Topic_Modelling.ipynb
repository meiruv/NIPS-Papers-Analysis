{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nips_Topic_Modelling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAmEzznytBMm9/oSk4cyVg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meiruv/NIPS-Papers-Analysis/blob/main/Nips_Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqldDG4ENZKw"
      },
      "source": [
        "Taking a go on this dataset:\n",
        "https://www.kaggle.com/rowhitswami/nips-papers-1987-2019-updated/tasks?taskId=2960\n",
        "\n",
        "The task here is to model the topic of each NIPS paper using LDA and evaluate the results independently. I will try modelling with LDA and with BERT as proposed here: https://github.com/MaartenGr/BERTopic\n",
        "\n",
        "I will need to check if I the abstracts are enough for the modelling or if the full text should be used.\n",
        "I will want to check if the author's name can somehow be used for validation (if I know an author is an expert of one field and the paper is classified under another topic I might suspect the classification is wrong)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NctZ6uTFroZL"
      },
      "source": [
        "! python -m spacy download en_core_web_lg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni59vw6pO6C9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO-WM0hbNRdv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Ttk_-DOdqy"
      },
      "source": [
        "# Fetching the Data From Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZuKGGRYOglU",
        "outputId": "f43435b1-e717-4762-816d-df6f9d62c253"
      },
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjPKoI2NOg_s",
        "outputId": "c7b855e9-a35b-4e57-d1a5-402892769783"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/\n",
        "! kaggle datasets download -d rowhitswami/nips-papers-1987-2019-updated"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading nips-papers-1987-2019-updated.zip to /content\n",
            " 97% 103M/106M [00:03<00:00, 24.9MB/s] \n",
            "100% 106M/106M [00:03<00:00, 29.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3n7_KUfOqEl"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = 'nips-papers-1987-2019-updated.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('nips_papers')\n",
        "zip_ref.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_6zflumP2-8",
        "outputId": "def902be-3c3e-4f8c-df76-3047433987ee"
      },
      "source": [
        "!ls nips_papers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "authors.csv  papers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnb3fkRP6Xi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx8TvNS9P9Zv"
      },
      "source": [
        "# Exploratory Analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkIUIc5wQAse"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgvmGOE9igi4"
      },
      "source": [
        "## Authors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "86ybeqzlQrB0",
        "outputId": "885fa9a1-5447-42b6-9c7f-1412e32d4641"
      },
      "source": [
        "authors_df = pd.read_csv('nips_papers/authors.csv')\n",
        "print('Authors df Shape : ',authors_df.shape)\n",
        "authors_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authors df Shape :  (30237, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_id</th>\n",
              "      <th>first_name</th>\n",
              "      <th>last_name</th>\n",
              "      <th>institution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27</td>\n",
              "      <td>Alan</td>\n",
              "      <td>Murray</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27</td>\n",
              "      <td>Anthony</td>\n",
              "      <td>Smith</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27</td>\n",
              "      <td>Zoe</td>\n",
              "      <td>Butler</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>63</td>\n",
              "      <td>Yaser</td>\n",
              "      <td>Abu-Mostafa</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>Michael</td>\n",
              "      <td>Fleisher</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   source_id first_name    last_name institution\n",
              "0         27       Alan       Murray         NaN\n",
              "1         27    Anthony        Smith         NaN\n",
              "2         27        Zoe       Butler         NaN\n",
              "3         63      Yaser  Abu-Mostafa         NaN\n",
              "4         60    Michael     Fleisher         NaN"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV9kJ1WDQz40",
        "outputId": "749aef57-4531-46f3-95a0-a24d624c42a7"
      },
      "source": [
        "len(authors_df['institution'].unique())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2672"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM-7a2fHRDtt"
      },
      "source": [
        "insts_and_pubs = pd.DataFrame(authors_df['institution'].value_counts()).reset_index()\n",
        "insts_and_pubs.columns = ['institution', 'Publications']\n",
        "insts_and_pubs.head(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "8KXwIVu1ROOD",
        "outputId": "0952a8f6-8871-4848-9b12-fe3d25d9a9cf"
      },
      "source": [
        "a4_dims = (10, 8)\n",
        "fig, ax = plt.subplots(figsize=a4_dims)\n",
        "\n",
        "sns.set(style = \"darkgrid\", font_scale=1.3)\n",
        "ax = sns.barplot(x='Publications', y=\"institution\", data=insts_and_pubs.head(20) , palette = \"GnBu_d\")\n",
        "ax.axes.set_title(\"20 Most Publishing Institutions\",fontsize=20)\n",
        "ax.set_ylabel(\"Institution\",fontsize=13)\n",
        "ax.set_xlabel(\"# Publications Since 1987)\",fontsize=15)\n",
        "#ax.set_xticklabels(top_diff_30['Country'],rotation = 45, ha=\"right\")\n",
        "\n",
        "# fig.savefig('Biggeat_Leap_Sine_2000_Horizontal.jpg')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, '# Publications Since 1987)')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkG5zQbLT6Uf"
      },
      "source": [
        "I found it interesting to look at how many papers each institution published and compare all institutions to find top publishers. This list is limited though because some institutiona and spelled differently-for example MIT appears sometimes as MIT and sometimes as Massachusetts Institute of Technology - row 23, if we combine the amounts on those rows we get more than Stanford which means MIT is the world's leading institution in terms of publications since 1987. Another example is Google having several rows - one for brain, one for deepminds, etc. If we combine those rows it will probably be ranked higher in this table.\n",
        "Also - fun fact and a bit of pride for Israelis - the Technion is 31st in this list, which reminds us yet again that while such rankings can be calculated by absolute numbers it is more accurate to normalize them based on the institution's size and perhaps bring into the equation its amounts of funding.\n",
        "\n",
        "It would be interesting to see how many institutions got into this list from each country/continent as well as how many of them are GAMFA and corporates in general. I didn't find an easy way for calculating this so I skipped this idea for the time being."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdogNIj7TOXU"
      },
      "source": [
        "# Writers per Paper distribution\n",
        "\n",
        "writers_per_paper_df = pd.DataFrame(authors_df['source_id'].value_counts()).reset_index()\n",
        "writers_per_paper_df.columns = ['source_id','n_writers']\n",
        "writers_per_paper_df.head(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxpSHczIX9p0",
        "outputId": "75fd765d-faeb-48be-c45d-df3e83bc2c02"
      },
      "source": [
        "writers_per_paper_df[writers_per_paper_df['n_writers']>=10].shape[0] - 80"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "948"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUweGeszWFlW",
        "outputId": "6dc3bb94-de37-45f2-e190-3fd5b03191e3"
      },
      "source": [
        "writers_per_paper_df.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4522, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "MFpU_wwHWtgu",
        "outputId": "de1f1991-d016-4103-f04d-97f9678c30da"
      },
      "source": [
        "writers_per_paper_df['n_writers'].hist(figsize = (10,8));"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KZShj_HX0HL"
      },
      "source": [
        "There are 4522 papers in this dataset, out of which 80 had 20 authors or more. \n",
        "948 has between 10-20 authors and the rest (3494) had less than 10 authors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhDO4ryRYZAY",
        "outputId": "9694da52-6098-4d94-e1fc-593eccf085ee"
      },
      "source": [
        "4522 - 80 - 948"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3494"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFz98hDrWCB3"
      },
      "source": [
        "# Papers per author distribution\n",
        "authors_df['author'] = authors_df['first_name']+' ' + authors_df['last_name']"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "l2xLE92pdYAd",
        "outputId": "5d54d1af-63a0-4bf1-dbca-550bfcc9b101"
      },
      "source": [
        "papers_per_author_df = pd.DataFrame(authors_df['author'].value_counts()).reset_index()\n",
        "papers_per_author_df.columns = ['author','n_publications']\n",
        "papers_per_author_df.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>n_publications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Michael Jordan</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yoshua Bengio</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bernhard Schlkopf</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Geoffrey Hinton</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Francis Bach</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               author  n_publications\n",
              "0      Michael Jordan             111\n",
              "1       Yoshua Bengio              74\n",
              "2  Bernhard Schlkopf              69\n",
              "3     Geoffrey Hinton              63\n",
              "4        Francis Bach              59"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl3uiU3Ee42a"
      },
      "source": [
        "My conclusion from this is to name my child Michael Jordan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Tyyui5R8hTwS",
        "outputId": "8b5e6581-608b-429a-c448-4088e726d39d"
      },
      "source": [
        "# special request from a friend\n",
        "papers_per_author_df[papers_per_author_df['author']=='Beatrice Golomb']\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>n_publications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13306</th>\n",
              "      <td>Beatrice Golomb</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                author  n_publications\n",
              "13306  Beatrice Golomb               1"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "4xzpts5ZfydJ",
        "outputId": "e143ab8c-87e3-473c-bd10-71fdd79829c2"
      },
      "source": [
        "# Another way to query this, I find it useful to keep such snippets\n",
        "papers_per_author_df.loc[papers_per_author_df['author'].str.startswith('Bea', na=False)]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>n_publications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6414</th>\n",
              "      <td>Beat Buesser</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7326</th>\n",
              "      <td>Beat Pfister</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7980</th>\n",
              "      <td>Beata Jarosiewicz</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11416</th>\n",
              "      <td>Beat Flepp</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13306</th>\n",
              "      <td>Beatrice Golomb</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  author  n_publications\n",
              "6414        Beat Buesser               1\n",
              "7326        Beat Pfister               1\n",
              "7980   Beata Jarosiewicz               1\n",
              "11416         Beat Flepp               1\n",
              "13306    Beatrice Golomb               1"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMRKKdeCZB8a"
      },
      "source": [
        "### Are there communities here?\n",
        "\n",
        "Draw a graph from this data. the most basic way will be to draw an edge between each authors that published together and weight the edges based on the amount of publications they have together, then run community detection algorithm and see if there are any. also see top publishers and perhaps place the authors on a map based on their institution to see where those communities occur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azIFA-nEZmv2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbaZIGZMinwh"
      },
      "source": [
        "## Papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "WCl7jpX5jCRS",
        "outputId": "06029e97-4ebe-40d5-de16-271efb4a3a8b"
      },
      "source": [
        "papers_df = pd.read_csv('nips_papers/papers.csv')\n",
        "papers_df.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27</td>\n",
              "      <td>1987</td>\n",
              "      <td>Bit-Serial Neural Networks</td>\n",
              "      <td>NaN</td>\n",
              "      <td>573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63</td>\n",
              "      <td>1987</td>\n",
              "      <td>Connectivity Versus Entropy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>1987</td>\n",
              "      <td>The Hopfield Model with Multi-Level Neurons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59</td>\n",
              "      <td>1987</td>\n",
              "      <td>How Neural Nets Work</td>\n",
              "      <td>NaN</td>\n",
              "      <td>442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>1987</td>\n",
              "      <td>Spatial Organization of Neural Networks: A Pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   source_id  year  ... abstract                                          full_text\n",
              "0         27  1987  ...      NaN  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...\n",
              "1         63  1987  ...      NaN  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...\n",
              "2         60  1987  ...      NaN  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...\n",
              "3         59  1987  ...      NaN  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...\n",
              "4         69  1987  ...      NaN  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "rOC0-FqXjOID",
        "outputId": "67d164e5-ea15-4989-a725-72757cebb088"
      },
      "source": [
        "# papers per year bar plot\n",
        "papers_per_year_df = pd.DataFrame(papers_df['year'].value_counts()).reset_index()\n",
        "papers_per_year_df.columns = ['year','count']\n",
        "\n",
        "papers_per_year_df.head()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019</td>\n",
              "      <td>1428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018</td>\n",
              "      <td>1009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017</td>\n",
              "      <td>679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016</td>\n",
              "      <td>569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  count\n",
              "0  2019   1428\n",
              "1  2018   1009\n",
              "2  2017    679\n",
              "3  2016    569\n",
              "4  2014    411"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5le2P_WkYFK"
      },
      "source": [
        "papers_per_year_df_sorted = papers_per_year_df.sort_values(by = 'year',ascending=True)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "jM-Twpv6jxUi",
        "outputId": "d2a3dceb-9071-4d45-9288-090b594c4d6b"
      },
      "source": [
        "a4_dims = (15, 7)\n",
        "fig, ax = plt.subplots(figsize=a4_dims)\n",
        "\n",
        "sns.set(style = \"darkgrid\", font_scale=1.3)\n",
        "ax = sns.barplot(x=\"year\", y=\"count\", data = papers_per_year_df_sorted , palette = \"GnBu_d\")\n",
        "ax.axes.set_title(\"Papers Per Year 1987-2019\",fontsize=20)\n",
        "ax.set_ylabel(\"Year\",fontsize=7)\n",
        "ax.set_xlabel(\"Number Of Papers\",fontsize=15)\n",
        "ax.set_xticklabels(papers_per_year_df_sorted['year'],rotation = 45, ha=\"right\");"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zst7xgyTnw_h"
      },
      "source": [
        "\n",
        "*   tf - idf on the titles and then sum per year/ decade/ half-decades\n",
        "*   same on abstracts\n",
        "*   same on papers.\n",
        "*   keywors extraction and comparison between years, looking for timely trends, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH2HMnH8n7Zk"
      },
      "source": [
        "papers_df['title'].head(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bygXJehlol7r",
        "outputId": "bc33f24e-94fc-4ad0-fdd0-1725678e70f1"
      },
      "source": [
        "papers_df['full_text'][0]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan F.  Murray,  Anthony V . W.  Smith  and Zoe F.  Butler. \\n\\nDepartment of Electrical Engineering,  University of Edinburgh, \\n\\nThe King\\'s Buildings, Mayfield Road,  Edinburgh, \\n\\nScotland,  EH93JL. \\n\\nABSTRACT \\n\\nA  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a \\nsynapse array through to silicon layout and board design.  The issues surrounding bit \\n- serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel \\ndevelopment  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and \\nrecall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected \\nspecification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173)\\nnique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms, \\nusing  a  \"paging\"  technique  to  time  - multiplex  calculations  through  the  synapse \\narray. \\n\\n1. INTRODUCTION \\n\\nThe functions a  synthetic neural network may aspire to mimic are the ability to con(cid:173)\\nsider  many  solutions  simultaneously,  an  ability  to  work  with  corrupted  data  and  a \\nnatural  fault  tolerance.  This  arises  from  the  parallelism  and  distributed  knowledge \\nrepresentation  which  gives  rise  to  gentle  degradation  as  faults  appear.  These func(cid:173)\\ntions  are  attractive  to implementation  in VLSI  and  WSI.  For example,  the natural \\nfault  - tolerance  could  be  useful  in  silicon  wafers  with  imperfect  yield,  where  the \\nnetwork  degradation  is  approximately  proportional  to  the  non-functioning  silicon \\narea. \\nTo cast  neural networks in engineering language,  a  neuron is a  state machine that is \\neither  \"on\"  or  \"off\\',  which  in  general  assumes  intermediate  states  as  it  switches \\nsmoothly  between  these  extrema.  The  synapses  weighting  the  signals  from  a \\ntransmitting neuron  such that it is more or less excitatory or inhibitory to the receiv(cid:173)\\ning  neuron.  The  set  of synaptic weights  determines  the stable  states and  represents \\nthe learned  information in a system. \\nThe  neural  state,  VI\\'  is  related  to  the  total  neural  activity  stimulated  by  inputs  to \\nthe  neuron  through  an  activation junction,  F.  Neural  activity  is  the  level  of excita(cid:173)\\ntion  of the  neuron  and the  activation  is  the way  it  reacts  in a  response to a  change \\nin activation. The neural output state at time t, V[,  is related to x[ by \\n\\nV[  = F (xf) \\n\\n(1) \\n\\nThe  activation  function  is  a  \"squashing\"  function  ensuring  that  (say)  Vi  is  1  when \\nXi  is large  and  -1  when Xi  is  small.  The neural update function  is therefore straight(cid:173)\\nforward: \\n\\n. \\n\\n,+1  - ,   + ~  ~ T  V\\' \\nJ \\nXI \\n\\ni-n-l \\n0  ~  ii \\n\\n- XI \\n\\n  \\n\\nJ-O \\n\\n(2) \\n\\nwhere  8  represents  the  rate  of change  of neural  activity,  Tij \\nand n  is  the number of terms giving an n  - neuron array [1]. \\nAlthough  the  neural function  is  simple  enough,  in  a  totally  interconnected  n  - neu(cid:173)\\nron  network  there  are n 2  synapses requiring n 2  multiplications  and  summations and \\n\\nis  the  synaptic  weight \\n\\n American Institute of Physics 1988 \\n\\n\\x0c574 \\n\\na large number of interconnects.  The challenge in VLSI is therefore to design a  sim(cid:173)\\nple,  compact  synapse  that  can  be  repeated  to  build  a  VLSI  neural  network  with \\nIn  a  network  with  fixed  functionality,  this  is  relatively \\nmanageable  interconnect. \\nstraightforward.  H the  network  is to be able to learn,  however,  the synaptic weights \\nmust  be programmable, and therefore more complicated. \\n\\n2. DESIGNING  A NEURAL  NETWORK IN  VLSI \\n\\nThere  are  fundamentally  two  approaches  to  implementing  any  function  in  silicon  -\\ndigital and analog.  Each technique has its advantages and  disadvantages,  and these \\nare  listed  below,  along  with  the  merits  and  demerits  of bit  - serial  architectures  in \\ndigital (synchronous) systems. \\nDigital  vs.  analog:  The  primary  advantage  of digital  design  for  a  synapse  array  is \\nthat  digital  memory  is  well  understood,  and  can  be  incorporated  easily.  Learning \\nnetworks are  therefore  possible  without  recourse  to unusual  techniques  or technolo(cid:173)\\ngies.  Other strengths of a digital approach are that design techniques are advanced, \\nautomated  and  well  understood  and  noise  immunity  and  computational  speed  can \\nbe  high.  Unattractive features  are  that  digital  circuits  of this complexity need  to  be \\nsynchronous  and  all  states  and  activities  are  quantised,  while  real  neural  networks \\nare  asynchronous  and  unquantised.  Furthermore,  digital  multipliers  occupy  a  large \\nsilicon  area, giving a low synapse count on  a single chip. \\nThe  advantages  of  analog  circuitry  are  that  asynchronous  behaviour  and  smooth \\nneural  activation  are  automatic.  Circuit  elements can  be  small,  but  noise  immunity \\nis relatively  low  and  arbitrarily  high  precision is not  possible.  Most  importantly,  no \\nreliable  analog,  non  - volatile  memory  technology  is  as  yet  readily  available.  For \\nthis  reason,  learning  networks  lend  themselves  more  naturally to  digital  design  and \\nimplementation. \\nSeveral  groups  are  developing  neural  chips  and  boards,  and  the  following  listing \\ndoes  not  pretend  to  be  exhaustive.  It is  included,  rather,  to indicate  the spread  of \\nactivity  in  this  field.  Analog  techniques  have  been  used  to  build  resistor  I  opera(cid:173)\\ntional  amplifier  networks [2,3]  similar to  those  proposed  by  Hopfield  and Tank [4]. \\nA  large  group  at  Caltech  is  developing  networks  implementing  early  vision  and \\nauditory  processing  functions  using the intrinsic nonlinearities of MaS transistors in \\nthe subthreshold  regime  [5,6].  The problem of implementing analog  networks with \\nelectrically  programmable  synapses  has  been  addressed  using  CCDIMNOS technol(cid:173)\\nogy  [7].  Finally,  Garth  [8]  is  developing  a  digital  neural  accelerator  board  (\"Net(cid:173)\\nsim\")  that  is  effectively  a  fast  SIMD  processor  with  supporting  memory  and  com(cid:173)\\nmunications chips. \\nBit - serial  vs.  bit  - parallel:  Bit  - serial  arithmetic and  communication  is  efficient \\nfor  computational  processes,  allowing  good  communication  within  and  between \\nVLSI  chips  and  tightly  pipelined  arithmetic  structures.  It  is  ideal  for  neural  net(cid:173)\\nworks  as  it  minimises  the  interconnect  requirement  by  eliminating  multi  - wire \\nbusses.  Although  a  bit  - parallel  design  would  be  free  from  computational  latency \\n(delay  between  input  and  output),  pipelining  makes  optimal  use  of  the  high  bit  -\\nrates possible in serial systems,  and  makes for  efficient circuit usage. \\n2.1  An asynchronous pulse stream VLSI neural network: \\nIn  addition  to  the  digital  system  that  forms  the  substance  of  this  paper,  we  are \\ndeveloping  a  hybrid  analOg/digital  network  family.  This work  is  outlined  here,  and \\nhas  been  reported  in  greater  detail  elsewhere  [9, 10, 11].  The  generic  (logical  and \\nlayout)  architecture  of a  single  network  of n  totally  interconnected neurons is  shown \\n\\n\\x0c575 \\n\\nschematically  in  figure  1.  Neurons  are  represented  by  circles,  which  signal  their \\nstates,  Vi  upward  into  a  matrix  of  synaptic  operators.  The  state  signals  are  con(cid:173)\\nnected  to  a  n  - bit  horizontal  bus  running  through  the  synaptic  array,  with  a  con(cid:173)\\nnection  to  each  synaptic  operator  in  every  column.  All  columns  have  n  operators \\n(denoted  by  squares)  and  each  operator adds its synaptic contribution,  Tij V j\\n,  to the \\nrunning  total  of  activity  for  the  neuron  i  at  the  foot  of  the  column.  The  synaptic \\nfunction  is  therefore  to  multiply  the  signalling  neuron  state,  Vj\\n,  by  the  synaptic \\nweight,  Tij ,  and  to  add  this  product  to  the  running  total.  This  architecture  is com(cid:173)\\nmon to both  the bit - serial and pulse - stream networks. \\n\\nSynapse \\n\\nStates { Vj  } \\n\\nFigure 1. Generic architecture for  a  network of n totally interconnected neurons. \\n\\nNeurons \\n\\nj=O \\n\\nj=II -1 \\n\\nThis type of architecture has many attractions for  implementation in 2  - dimensional \\nsilicon  as  the  summation  2  Tij Vj  is  distributed  in  space.  The  interconnect \\nrequirement  (n  inputs  to  each  neuron)  is  therefore  distributed  through  a  column, \\nreducing the need  for  long - range wiring.  The architecture is modular,  regular and \\ncan be easily expanded. \\nIn  the  hybrid  analog/digital  system,  the  circuitry  uses  a  \"pulse  stream\"  signalling \\nmethod  similar  to  that  in  a  natural  neural  system.  Neurons  indicate  their  state  by \\nthe  presence  or  absence  of  pulses  on  their  outputs,  and  synaptic  weighting  is \\nachieved  by  time  - chopping  the  presynaptic  pulse  stream  prior  to  adding  it  to  the \\npostsynaptic  activity  summation.  It  is  therefore  asynchronous  and  imposes  no fun(cid:173)\\ndamental  limitations  on  the  activation  or  neural  state.  Figure  2  shows  the  pulse \\nstream  mechanism  in  more  detail.  The synaptic  weight  is  stored  in  digital  memory \\nlocal to the operator.  Each synaptic operator has an  excitatory and inhibitory  pulse \\nstream  input  and  output.  The  resultant  product  of  a  synaptic  operation,  Tij Vj\\n,  is \\nadded  to  the  running  total  propagating  down  either  the  excitatory  or  inhibitory \\nchannel.  One binary bit  (the  MSBit)  of the  stored  Tij  determines whether  the con(cid:173)\\ntribution  is excitatory or inhibitory. \\nThe  incoming  excitatory  and  inhibitory  pulse  stream  inputs  to  a  neuron  are \\nintegrated  to  give  a  neural  activation  potential  that varies  smoothly  from  0  to  5  V. \\nThis  potential controls a  feedback  loop with  an odd number of logic  inversions and \\n\\n\\x0c576 \\n\\n.   \\n\\nXT  \\n\\nV , \\n.u.u, \\n \\n\\nFigure  2.  Pulse  stream  arithmetic.  Neurons  are  denoted  by  0  and synaptic  operators \\nby  D. \\n\\nthus  forms  a  switched  \"ring - oscillator\".  H the inhibitory input dominates,  the feed(cid:173)\\nback  loop  is  broken.  H  excitatory  spikes  subsequently  dominate  at  the  input,  the \\nneural activity rises  to 5V and the feedback  loop oscillates with  a period determined \\nby a  delay  around  the loop.  The resultant  periodic waveform is then converted to a \\nseries  of voltage  spikes,  whose  pulse  rate  represents  the  neural  state,  Vi\\'  Interest(cid:173)\\ningly,  a  not  dissimilar  technique is  reported  elsewhere  in this volume,  although  the \\nsynapse function  is executed differently [12]. \\n\\n3. A 5  - STATE BIT - SERIAL NEURAL  NETWORK \\n\\nThe  overall  architecture  of  the  5  - state  bit  - serial  neural  network  is  identical  to \\nthat  of  the  pulse  stream  network.  It  is  an  array  of n 2  interconnected  synchronous \\nsynaptic  operators,  and  whereas  the  pulse  stream  method  allowed  Vj  to  assume  all \\nvalues  between  \"off\\' and  \"on\",  the  5 - state network VJ  is constrained  to 0,  0.5 Qr \\n 1.  The resultant  activation  function  is  shown  in  Figure 3.  Full  digital  multiplica(cid:173)\\ntion  is  costly  in  silicon  area,  but  multiplication  of  Tij  by  Vj  =  0.5  merely  requires \\nthe synaptic  weight  to be right  - shifted  by  1 bit.  Similarly,  multiplication  by  0.25 \\ninvolves  a  further  right  - shift  of Til\\'  and  multiplication  by 0.0  is  trivially  easy.  VJ \\n<  0 is not  problematic,  as  a  switchable adder/subtractor  is  not much  more complex \\nthan  an  adder.  Five  neural  states  are  therefore  feasible  with  circuitry  that  is  only \\nslightly more complex  than  a  simple serial adder.  The neural state expands from a  1 \\nbit  to  a  3  bit  (5  - state)  representation,  where  the  bits  represent  \"add/subtract?\", \\n\"shift?\" and \"multiply by O?\". \\nFigure 4  shows  part of the synaptic  array.  Each synaptic operator includes an 8 bit \\nshift  register  memory  block  holding  the  synaptic  weight,  Til\\'  A  3  bit  bus  for  the  5 \\nneural  states  runs  horizontally  above  each  synaptic  row.  Single  phase  dynamic \\nCMOS  has  been  used  with  a  clock  frequency  in  excess  of 20  MHz  [13).  Details of \\na synaptic operator are  shown  in  figure 5.  The synaptic weight  Til  cycles around the \\nshift  register  and  the  neural  state  Vj  is  present  on  the  state  bus.  During  the  first \\nclock  CYCle,  the  synaptic  weight  is  multiplied  by  the  neural  state  and  during  the \\nsecond,  the  most  significant  bit (MSBit)  of the resultant  Tij Vj  is sign  - extended for \\n\\n\\x0c577 \\n\\nlHRESHOLD \\n\\nState VJ \\n\\n..... -------=-------.. Activity sJ \\n\\ns \\n\\n\"5  STATE\" \\n\\n\"Sharper\" \\n\\n\"Smoother\" \\n\\n~.....::~-\"\\'--x.&..t------ Activity \"J \\n\\nFigure 3.  \"Hard - threshold\",  5  - state and sigmoid activation functions. \\n\\nJ-a-1T  v \\n~  ..  J \\nJ-li \\n\\nv, \\n\\nv, \\n\\nFigure 4.  Section  of the  synaptic  array  of the  5  - state activation function  neural net(cid:173)\\nwork. \\n\\n8  bits  to  allow  for  word  growth  in  the  running  summation.  A  least  significant  bit \\n(LSBit)  signal  running down  the  synaptic  columns indicates the arrival  of the LSBit \\nof  the  Xj  running  total.  If  the  neural  state  is  O.5  the  synaptic  weight  is  right \\nshifted  by  1 bit and then added to or subtracted from  the running total.  A  multipli(cid:173)\\ncation  of   1  adds  or  subtracts  the  weight  from  the  total  and  multiplication  by  0 \\n\\n\\x0c578 \\n\\n.0.5 \\n.0.0 \\n\\nAdd/Subtract \\n\\nAdd! \\nSubtract \\n\\nCarry \\n\\nFigure S.  The  synaptic operator with a 5 - state activation function. \\n\\ndoes not alter the running summation. \\nThe  final  summation  at  the  foot  of the  column  is  thresholded  externally  according \\nto  the  5  - state activation function  in  figure  3.  As  the  neuron activity Xj\\'  increases \\nthrough  a  threshold  value  x\" \\nideal  sigmoidal  activation  represents  a  smooth  switch \\nof  neural  state  from  -1  to  1.  The 5  - state  \"staircase\"  function  gives a  superficially \\nmuch  better  approximation  to  the  sigmoid  form  than  a  (much  simpler  to  imple(cid:173)\\nment)  threshold  function.  The  sharpness  of  the  transition  can  be  controlled  to \\n\"tune\"  the  neural dynamics for  learning and computation.  The control parameter is \\nreferred  to  as  temperature  by  analogy  with  statistical  functions  with  this  sigmoidal \\nform.  High  \"temperature\" gives a  smoother staircase and sigmoid,  while a tempera(cid:173)\\nture  of  0  reduces  both  to  the  \\'\\'Hopfield\\'\\'  - like  threshold  function.  The  effects  of \\ntemperature  on  both  learning  and  recall  for  the  threshold  and  5  - state  activation \\noptions are discussed in section 4. \\n\\n4. LEARNING AND  RECALL  WITH VLSI  CONSTRAINTS \\n\\nBefore  implementing  the  reduced  - arithmetic  network  in  VLSI,  simulation  experi(cid:173)\\nments  were  conducted  to  verify  that  the  5  - state  model  represented  a  worthwhile \\nenhancement  over  simple  threshold  activation.  The  \"benchmark\"  problem  was \\nchosen  for  its  ubiquitousness,  rather  than  for  its  intrinsic  value.  The  implications \\nfor  learning  and  recall  of the  5  - state  model,  the  threshold  (2  - state)  model  and \\n- state)  were  compared  at  varying  temperatures \\nsmooth  sigmoidal  activation  (  00 \\nIn  each  simulation  a  totally \\nwith  a  restricted  dynamic  range  for  the  weights  Tij  \\ninterconnected  64  node  network  attempted  to  learn  32  random  patterns  using  the \\ndelta  rule  learning  algorithm  (see  for  example  [14]).  Each  pattern  was  then  cor(cid:173)\\nrupted  with  25%  noise  and  recall  attempted  to  probe  the  content  addressable \\nmemory properties under the three different activation options. \\nDuring  learning,  individual  weights  can  become  large  (positive  or  negative).  When \\nweights  are  \"driven\"  beyond  the  maximum  value  in  a  hardware  implementation, \\n\\n\\x0c579 \\n\\nwhich  is  determined  by  the  size  of  the  synaptic  weight  blocks,  some  limiting \\nmechanism  must  be  introduced.  For  example,  with  eight  bit  weight  registers,  the \\nlimitation is  -128  S  Tij  S  127.  With integer weights,  this can be seen to be a prob(cid:173)\\nlem  of  dynamic  range,  where  it  is  the  relationship  between  the  smallest  possible \\nweight  ( 1) and the largest  (+ 127/-128) that is the issue. \\nResults:  Fig.  6  shows  examples  of the  results  obtained,  studying  learning  using  5  -\\nstate  activation  at  different  temperatures,  and  recall  using  both  5  - state  and  thres(cid:173)\\nhold  activation.  At  temperature  T=O,  the  5  - state  and  threshold  models  are \\ndegenerate,  and  the results identical.  Increasing smoothness of activation  (tempera(cid:173)\\nture)  during  learning  improves  the  quality  of  learning  regardless  of  the  activation \\nfunction  used  in  recall,  as more patterns are recognised  successfully.  Using 5 - state \\nactivation  in recall  is more effective  than simple  threshold  activation.  The effect of \\ndynamic  range  restrictions  can  be  assessed  from  the  horizontal  axis,  where  T/j:6.  is \\nshown.  The results  from  these and  many  other experiments may  be  summarised  as \\nfollows:-\\n5 - State activation  vs.  threshold: \\n1)  Learning with 5  - state activation was  protracted  over the threshold  activation, \\nas  binary  patterns  were  being  learnt,  and  the  inclusion  of  intermediate  values \\nadded extra degrees of freedom. \\n\\n2)  Weight  sets  learnt  using  the  5  - state  activation  function  were  \"better\"  than \\nthose  learnt  via  threshold  activation,  as  the  recall  properties  of both  5  - state \\nand  threshold  networks  using  such  a  weight  set  were  more  robust  against \\nnoise. \\nFull  sigmoidal  activation  was  better  than  5  - state,  but  the  enhancement  was \\nless  significant  than  that  incurred  by  moving  from  threshold  - 5 - state.  This \\nsuggests  that the law  of diminishing returns  applies to  addition of levels to the \\nneural  state  Vi\\'  This  issue  has  been  studied  mathematically  [15],  with  results \\nthat agree  qualitatively with  ours. \\n\\n3) \\n\\nWeight Saturation: \\nThree  methods  were  tried  to  deal  with  weight  saturation.  Firstly,  inclusion  of  a \\ndecay,  or  \"forgetting\"  term  was  included  in  the  learning  cycle  [1].  It  is  our  view \\nthat  this  technique can  produce the desired weight limiting property,  but in  the time \\navailable  for  experiments,  we  were  unable  to  \"tune\"  the  rate  of  decay  sufficiently \\nwell  to  confirm  it.  Renormalisation  of the  weights  (division  to  bring large  weights \\nback  into  the  dynamic  range)  was  very  unsuccessful,  suggesting  that  information \\ndistributed  throughout  the  numerically small  weights  was  being  destroyed.  Finally, \\nthe  weights were  allowed  to  \"clip\"  (ie any weight  outside the dynamic range  was  set \\nto  the  maximum  allowed  value).  This method  proved  very  successful,  as  the learn(cid:173)\\ning  algorithm  adjusted the weights  over which  it still  had control  to  compensate for \\nthe  saturation effect.  It is  interesting to note  that  other experiments have indicated \\nthat  Hopfield  nets  can  \"forget\"  in a  different  way,  under different learning control, \\ngiving  preference  to  recently acquired  memories [16].  The results  from  the  satura(cid:173)\\ntion experiments were:-\\n1) \\n\\nFor  the  32  pattemJ64  node  problem,  integer  weights  with  a  dynamic  range \\ngreater than  30 were necessary to give enough  storage capability. \\nFor weights  with  maximum  values  TiJ  = 50-70,  \"clipping\"  occurs,  but  net(cid:173)\\nwork  performance  is  not  seriously  degraded  over  that  with  an  unrestricted \\nweight set. \\n\\n2) \\n\\n\\x0c580 \\n\\n15 \\n\\n\"0  10 \\nc = \\n.2 \\nen e u \\n5 --~ \\n\\n0 \\n\\n0 \\n\\nI \\n\\n\".\\' \\n\\n., ... \\n\\n.... ----------\\n\\n,-\\ne  ~ ;A ....... ;.. f:\\'-:\\' :::::7.:::.::-:::-: f\\'-. \\n,  ,. \\ni \\n! \\n! , \\ni \\nI \\nI , \\n\\n20  30 \\n\\n40  50  60  70 \\n\\nLimit \\n\\n15 \\n\\nT=30  _._.-.-\\nT=20 \\nT=10 \\nT=O \\n\\n-.-._.-.. \\n\\n,.. .. -..... -.. _ .. \\n, \\n.. \\ni \\nj\\'\\'\\'\\'--\\n,,\\'i \\n\\n- . . .,. \\'\" \\n\\nj \\n\\n~-------------\\n   \\n\\nj \\nI \\n\\nO~~~~--~~ __ ~~ __ \\no \\n\\n20  30  40  50  60  70 \\n\\nLimit \\n\\n5 . state activation function  recal1 \\n\\ntlHopficld\" activation  function  recall \\n\\nFigure 6.  Recall  of patterns  learned  with  the  5  .  state  activation function  and  subse(cid:173)\\nquently restored using  the 5-state and the  hard - threshold activation functions. \\nT  is  the  \"temperature\",  or smoothness  of the  activation function,  and \"limit\"  the  value \\nofTI;   \\n\\nThese  results  showed  that  the  5  - state  model  was  worthy  of implementation  as  a \\nVLSI neural board, and suggested that 8 - bit weights were sufficient. \\n\\nS.  PROJECTED SPECIFICATION OF A HARDWARE NEURAL  BOARD \\n\\nThe specification of a  64  neuron board is  given  here,  using a  5 - state bit  - serial 64 \\nx 64  synapse array with  a derated clock speed  of 20 MHz.  The synaptic weights are \\n8  bit words and the word  length  of the running summation XI  is  16  bits to  allow for \\ngrowth.  A  64  synapse  column  has  a  computational  latency  of  80  clock  cycles  or \\nbits,  giving  an  update  time  of 4 .... s  for  the  network.  The  time  to  load  the  weights \\ninto  the  array  is  limited  to  6O .... s  by  the  supporting  RAM,  with  an  access  time  of \\n12Ons.  These  load  and  update  times  mean  that  the  network  is  executing  1  x  10\\' \\noperations/second,  where  one  operation  is    Tlj  Vj   This  is  much  faster  than  a \\nnatural  neural  network,  and  much  faster  than  is  necessary  in  a  hardware  accelera(cid:173)\\ntor.  We  have  therefore  developed  a  \"paging\"  architecture,  that  effectively  \"trades -\\noff\" some of this excessive speed against increased network size. \\nA  \"moving  - patch\"  neural  board:  An  array  of  the  5  - state  synapses  is  currently \\nbeing  fabricated  as  a  VLSI  integrated  circuit.  The  shift  registers  and \\nthe \\nadderlsubtractor for  each  synapse  occupy a  disappointingly large silicon  area,  allow(cid:173)\\ning only a  3  x 9 synaptic  array.  To achieve  a  suitable size  neural  network  from  this \\narray,  several chips need to be  included on a  board with  memory and control circu(cid:173)\\nitry.  The  \"moving  patch\"  concept  is  shown  in  figure  7,  where  a  small  array  of \\nsynapses is passed over a much larger n  x n  synaptic array. \\nEach  time  the  array  is  \"moved\"  to  represent  another set  of  synapses,  new  weights \\nmust be  loaded  into it.  For example,  the  first  set of weights will  be T 11  . ,  T;J  ... T 21 \\n...  T 2j  to Tjj ,  the second  set  Tj + 1,l  to T u  etc..  The final  weight  to be loaded will  be \\n\\n\\x0c581 \\n\\nn  neurons .. om synaptic array \\n\\nSmaller \"Patch\" \\n\\nmoves over array \\n\\nrr~ _____ ) __ -.. \\n> \\n~\\'-\\n\\nFigure 7.  The  \"moving  patch\" concept,  passing  a  small synaptic \"patch\"  over  a larger \\nrun synapse array. \\n\\nTNt  Static,  off - the  - shelf RAM is  used  to store the weights and the  whole opera(cid:173)\\ntion  is  pipelined for  maximum efficiency.  Figure 8 shows the board level design for \\nthe network. \\n\\nSynaptic  Accelerator Chips \\n\\nControl \\n\\nHOST \\nFigure 8. A  \"moving  patch\" neural network board. \\n\\nThe small  \"patch\" that moves  around  the array  to  give  n  neurons comprises 4 VLSI \\nsynaptic accelerator chips to give  a 6 x 18 synaptic array. The number of neurons to \\nbe  simulated  is 256  and  the weights for  these  are stored  in 0.5  Mb of RAM  with a \\nload  time  of 8ms.  For  each  \"patch\"  movement,  the  partial  runnin~ summatinn \\n\\n;. \\n\\n\\x0c582 \\n\\ncalculated  for  each  column,  is  stored  in  a  separate  RAM  until  it is  required  to  be \\nadded  into  the  next  appropriate  summation.  The  update  time  for  the  board  is  3ms \\ngiving  2  x  107  operations/second.  This  is  slower  than  the  64  neuron  specification, \\nbut  the  network  is  16  times  larger,  as  the  arithmetic  elements are  being  used  more \\nefficiently.  To  achieve  a  network  of  greater  than  256  neurons,  more  RAM  is \\nrequired to store the weights.  The network is then slower unless a larger number of \\naccelerator chips is  used  to give  a larger moving \"patch\". \\n\\n6.  CONCLUSIONS \\n\\nA  strategy  and  design  method  has  been  given  for  the  construction  of  bit  - serial \\nVLSI neural network chips and  circuit  boards.  Bit - serial  arithmetic,  coupled  to  a \\nreduced  arithmetic  style,  enhances  the  level  of  integration  possible  beyond  more \\nconventional digital,  bit - parallel schemes.  The restrictions imposed  on both synap(cid:173)\\ntic  weight  size  and  arithmetic  precision  by  VLSI  constraints  have  been  examined \\nand shown to be tolerable,  using the associative memory problem as a test. \\nWhile  we  believe  our  digital  approach  to  represent  a  good  compromise  between \\narithmetic  accuracy  and  circuit  complexity,  we  acknowledge  that  the  level  of \\nintegration  is  disappointingly  low. \\nIt  is  our  belief  that,  while  digital  approaches \\nmay  be interesting and  useful  in the medium  term,  essentially as  hardware accelera(cid:173)\\ntors for  neural simulations,  analog techniques represent the best  ultimate option in 2 \\n- dimensional  silicon.  To this  end,  we  are currently pursuing techniques for  analog \\nIn any  event,  the  full \\npseudo  - static  memory,  using  standard  CMOS  technology. \\ndevelopment  of a  nonvolatile  analog  memory  technology,  such  as  the  MNOS  tech(cid:173)\\nnique [7],  is key to the long - term  future of VLSI neural nets that can learn. \\n\\n7. ACKNOWLEDGEMENTS \\n\\nThe  authors  acknowledge  the  support  of  the  Science  and  Engineering  Research \\nCouncil (UK) in the execution of this work. \\n\\nReferences \\n\\n1. \\n\\nS.  Grossberg,  \"Some  Physiological  and  Biochemical  Consequences  of Psycho(cid:173)\\nlogical Postulates,\" Proc.  Natl.  Acad.  Sci.  USA,  vol.  60,  pp.  758  - 765,  1968. \\n\\n2.  H.  P.  Graf,  L.  D.  Jackel,  R.  E.  Howard,  B.  Straughn,  J.  S.  Denker,  W. \\nHubbard,  D.  M.  Tennant,  and  D.  Schwartz,  \"VLSI  Implementation  of  a \\nNeural  Network  Memory  with  Several  Hundreds  of  Neurons,\"  Proc.  AlP \\nConference on Neural Networks for  Computing.  Snowbird,  pp.  182 - 187,  1986. \\n3.  W.  S.  Mackie,  H.  P.  Graf,  and  J.  S.  Denker,  \"Microelectronic  Implementa(cid:173)\\n\\ntion  of  Connectionist  Neural  Network  Models,\"  IEEE  Conference  on  Neural \\nInformation Processing Systems.  Denver,  1987. \\nJ . J. Hopfield  and D.  W.  Tank, \"Neural\" Computation of Decisions in  Optim(cid:173)\\nisation Problems,\" BioI.  Cybern.,  vol.  52,  pp.  141  - 152,  1985. \\n\\n4. \\n\\n5.  M.  A.  Sivilotti,  M.  A.  Mahowald,  and  C.  A.  Mead, Real - Time  Visual Com(cid:173)\\n\\nputations Using  Analog CMOS  Processing Arrays, 1987.  To be published \\n\\n6.  C.  A.  Mead,  \"Networks  for  Real  - Time  Sensory  Processing,\"  IEEE  Confer(cid:173)\\n\\nence  on  Neural Information  Processing Systems,  Denver,  1987. \\n\\n\\x0c583 \\n\\n7. \\n\\n8. \\n\\nJ.  P.  Sage,  K.  Thompson.  and  R. S.  Withers,  \"An Artificial Neural  Network \\nIntegrated  Circuit  Based on MNOSlCCD  Principles,\"  Proc. AlP Conference on \\nNeural Networlcs for Computing,  Snowbird,  pp.  381  - 385,  1986. \\nS.  C.  J.  Garth, \"A Chipset for  High Speed  Simulation of Neural Network  Sys(cid:173)\\ntems,\"  IEEE Conference on Neural Networlc.s,  San Diego,  1987. \\n\\n9.  A.  F.  Murray and  A.  V.  W.  Smith,  \"A Novel  Computational  and  Signalling \\nMethod  for  VLSI Neural Networks,\"  European  Solid State Circuits Conference \\n, 1987. \\n\\n10.  A.  F.  Murray  and  A.  J.  W.  Smith,  \"Asynchronous  Arithmetic  for  VLSI \\n\\nNeural Systems,\"  Electronics Letters, vol.  23, no.  12, p.  642, June, 1987. \\n\\n11.  A.  F.  Murray  and  A.  V.  W.  Smith,  \"Asynchronous  VLSI  Neural  Networks \\n\\nusing  Pulse  Stream  Arithmetic,\"  IEEE  Journal  of Solid-State  Circuits  and Sys(cid:173)\\ntems,  1988.  To be published \\n\\n12.  M.  E.  Gaspar,  \"Pulsed  Neural  Networks:  Hardware,  Software  and  the  Hop(cid:173)\\nfield  AID  Converter  Example,\"  IEEE  Conference  on  Neural  Information  Pro(cid:173)\\ncessing Systems.  Denver,  1987. \\n\\n13.  M.  S.  McGregor,  P.  B.  Denyer,  and A.  F.  Murray,  \"A Single - Phase  Clock(cid:173)\\ning Scheme for  CMOS  VLSI,\"  Advanced Research  in  VLSI  \" Proceedings of the \\n1987 Stanford Conference,  1987. \\n\\n14.  D.  E.  Rumelhart,  G.  E.  Hinton,  and  R.  J.  Williams,  \"Learning  Internal \\nRepresentations  by  Error  Propagation,\"  Parallel  Distributed  Processing  \" \\nExplorations  in  the  Microstructure of Cognition,  vol.  1,  pp.  318 - 362,  1986. \\n\\n15.  M.  Fleisher  and  E.  Levin,  \"The  Hopfiled  Model  with  Multilevel  Neurons \\nModels,\"  IEEE  Conference  on  Neural  Information  Processing  Systems.  Denver, \\n1987. \\n\\n16.  G.  Parisi,  \"A  Memory  that  Forgets,\"  J.  Phys.  A  .\\'  Math.  Gen.,  vol.  19,  pp. \\n\\nL617  - L620,  1986. \\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTZn9jhfpKSy",
        "outputId": "2ec1af50-30bf-43ca-ecf0-906d7d6200c2"
      },
      "source": [
        "\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     || 827.9 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=1aba808ba1c732d5e9935a84cd0bb38727e066da35c5e400d1a9086578743eb7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rhg2ed6t/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgbQAvAfqSU4",
        "outputId": "0c699f9f-dafc-4069-eb5e-257c705ed65b"
      },
      "source": [
        "# Checking if I can fetch the institution's location using Spacy\n",
        "\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "gpe = [] # countries, cities, states\n",
        "loc = [] # non gpe locations, mountain ranges, bodies of water\n",
        "\n",
        "city1 = []\n",
        "country1 = []\n",
        "\n",
        "for i in range(10):\n",
        "  gpe = []\n",
        "  doc = nlp(papers_df['full_text'][i])\n",
        "  for ent in doc.ents:\n",
        "      if (ent.label_ == 'GPE'):\n",
        "          gpe.append(ent.text)\n",
        "  print('---------- Iteration Number ',i)\n",
        "  print(gpe)\n",
        "  city1.append(gpe[0])\n",
        "  country1.append(gpe[1])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Iteration Number  0\n",
            "['Edinburgh', 'Scotland', 'VLSI', 'Synapse \\n\\nStates', 'Tjj', 'UK', 'USA', 'Denver', 'Denver', 'San Diego', 'Sys(cid:173', 'Denver', 'Denver']\n",
            "---------- Iteration Number  1\n",
            "['California', 'Pasadena', 'L Pr(nl', 'VI']\n",
            "---------- Iteration Number  2\n",
            "['Israel', 'Ik-I(C', 'Xk', 'Hopfleld', 'sively', 'i.i.', 'netwock', 'Israel']\n",
            "---------- Iteration Number  3\n",
            "['Los  Alamos', 'z(t -\\n', 'T\"\\'i', 'J.Sidorowich', 'Physica', 'Takens', 'Cambridge', 'MA', 'Univ']\n",
            "---------- Iteration Number  4\n",
            "['Athens', 'Zographos', 'Athens', 'Greece', 'rea(cid:173', 'il(cid:173', 'Pr~(kM', 'Istanbul', 'Turkey', 'N.Y.', 'Scient', 'Amer.']\n",
            "---------- Iteration Number  5\n",
            "['U.S.', 'igned', 'neu(cid:173', 'Yij', 'neu(cid:173', 'Englewood', 'New  Jersey', 'Soc']\n",
            "---------- Iteration Number  6\n",
            "['Los Angeles', 'Barto', 'Barto', 'Barto, et al.', 'Stanislaus', 'Turlock', 'California', 'Barto', 'ADALINE', 'algorithmS.', 'Holland', 'Barto', 'Chambersl', 'Barto', 'Sutton', 'Barto', 'Edinburgh', 'Seattle', 'Amherst']\n",
            "---------- Iteration Number  7\n",
            "['maxi ai', 'c.f.', 'Qilxd', 'Lyapunov', 'trl', 'v(x', 'v(x', 'Utah', 'U.S.A.', 'Amsterdam', 'North Holland', 'Dumka', 'Kiev']\n",
            "---------- Iteration Number  8\n",
            "['Sherryl', 'Hampton VA', 'cycles1S', 'space(cid:173', 'North, South, East, \\n', 'limi', 'Cambridge', 'Mass', 'Greenbelt Maryland', 'Durham', 'R.P. Lippmann', 'Hampton', 'vol', 'Cambridge', 'MA']\n",
            "---------- Iteration Number  9\n",
            "['', 'us', 'non-quadrntic', 'compu(cid:173', 'consid(cid:173', 'Kanerva', 'vBME-20', 'Cambridge', 'ed', 'California', 'Denker']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "LvQUtdkU6cr-",
        "outputId": "421c8e99-f047-4d7a-859d-769f1cc65c65"
      },
      "source": [
        "i = 7\n",
        "papers_df['full_text'][i]"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'554 \\n\\nSTABILITY RESULTS  FOR NEURAL  NETWORKS \\n\\nA.  N.  Michell, J. A.  FarreUi  , and W.  Porod2 \\n\\nDepartment of Electrical and  Computer Engineering \\n\\nUniversity of Notre  Dame \\n\\nNotre Dame, IN 46556 \\n\\nABSTRACT \\n\\nIn the present paper we survey and utilize results from the qualitative theory of large \\nscale interconnected dynamical systems in order to develop  a  qualitative theory for  the \\nHopfield model of neural networks.  In our approach we  view such networks as  an inter(cid:173)\\nconnection of many single neurons.  Our results  are  phrased in  terms of the  qualitative \\nproperties of the individual neurons and in terms of the properties of the interconnecting \\nstructure of the neural  networks.  Aspects of neural networks which  we  address include \\nasymptotic stability,  exponential stability,  and instability  of an  equilibrium;  estimates \\nof trajectory bounds; estimates of the domain of attraction of an asymptotically stable \\nequilibrium;  and stability of neural networks  under structural perturbations. \\n\\nINTRODUCTION \\n\\nIn recent years, neural networks have attracted considerable attention as  candidates \\nfor  novel  computational systemsl- 3 .  These  types  of large-scale  dynamical  systems,  in \\nanalogy  to  biological  structures,  take  advantage  of distributed  information  processing \\nand  their  inherent  potential  for  parallel  computation4,5.  Clearly,  the  design  of such \\nneural-network-based  computational  systems  entails  a  detailed  understanding  of  the \\ndynamics  of large-scale  dynamical  systems.  In particular,  the stability  and instability \\nproperties  of the  various  equilibrium  points  in  such  networks  are  of interest,  as  well \\nas  the extent of associated  domains of attraction  (basins  of attraction)  and  trajectory \\nbounds. \\n\\nIn the present paper, we apply and survey results from the qualitative theory oflarge \\nscale  interconnected  dynamical systems6 - 9  in order to  develop  a  qualitative theory for \\nneural  networks.  We  will  concentrate here  on  the  popular  Hopfield  model3 ,  however, \\nthis type of analysis may also be applied to other models.  In particular, we  will  address \\nthe  following  problems:  (i)  determine  the  stability  properties  of a  given  equilibrium \\npoint;  (ii)  given  that a  specific equilibrium point of a  neural network is  asymptotically \\nstable, establish an estimate for its domain of attraction; (iii) given a set of initial condi(cid:173)\\ntions  and external inputs, establish estimates for  corresponding trajectory bounds;  (iv) \\ngive  conditions for  the instability of a  given equilibrium  point;  (v)  investigate  stability \\nproperties under structural perturbations.  The present  paper contains local  results.  A \\nmore detailed treatment of local stability results can be found in Ref.  10, whereas global \\nresults  are contained in Ref.  1l. \\n\\nIn arriving at the results of the present paper,  we  make use  of the method of anal(cid:173)\\n\\nysis  advanced  in  Ref.  6.  Specifically,  we  view  high  dimensional neural  network  as  an \\n\\nIThe work of A.  N.  Michel  and  J.  A.  Farrell was supported by  NSF  under grant  ECS84-19918. \\n2The work of W.  Porod was supported by  ONR under grant  NOOOI4-86-K-0506. \\n\\n American Institute of Physics 1988 \\n\\n\\x0c555 \\n\\ninterconnection of individual subsystems  (neurons).  This  interconnected systems  view(cid:173)\\npoint makes  our  results  distinct  from  others  derived  in  the literature1,12.  Our  results \\nare  phrased  in  terms  of  the  qualitative  properties  of the  free  subsystems  (individual \\nneurons, disconnected from the network) and in terms of the properties of the intercon(cid:173)\\nnecting  structure  of the  neural  network.  As  such,  these  results  may  constitute  useful \\ndesign tools.  This approach makes  possible the systematic analysis of high dimensional \\ncomplex systems and it frequently enables one  to circumvent difficulties encountered in \\nthe analysis of such systems  by conventional methods. \\n\\nThe  structure  of this  paper  is  as  follows.  We  start  out  by  defining  the  Hopfield \\nmodel  and  we  then introduce  the interconnected systems  viewpoint.  We  then  present \\nrepresentative stability results, including estimates of trajectory bounds and of domains \\nof attraction, results for instability, and conditions for stability under structural pertur(cid:173)\\nbations.  Finally, we  present concluding remarks. \\n\\nTHE HOPFIELD  MODEL  FOR NEURAL  NETWORKS \\n\\nIn the present paper we consider neural networks of the Hopfield type3   Such systems \\n\\ncan be represented by  equations of the form \\n\\nUi  = ..... biUi + I:Aij Gj(Uj) + Ui(t),  for  i  = 1, ... ,N, \\n\\nN \\n\\n(1) \\n\\nj=1 \\n\\nwhere  Aij  = *\"Ui(t) = l~g)  and  bi  = *..  As  usual,  Ci  >  O,Tij \\ni:;,RijfR  = \\n(-00,00),\\':.  =  ~ +E.f=IITiil,  Ri  >  O,Ii:  R+  = [0,00)  ~ R,Ii  is  continuous, \\nUi  =  ~,Gi :  R  ~ (-1,1), Gi  is  continuously  differentiable  and  strictly  monotoni(cid:173)\\ncally increasing (Le.,  Gi( uD  > Gi( u~\\') if and only if u~ > u~\\'), UiGi( Ui)  > 0 for all Ui  ::j;  0, \\nand Gi(O)  =  O.  In (1), C i  denotes  capacitance,  Rij  denotes  resistance  (possibly includ(cid:173)\\ning a sign inversion due to an inverter), Gi () denotes an amplifier nonlinearity, and Ii(\\') \\ndenotes an external input. \\n\\nIn  the  literature it is  frequently  assumed  that  Tij  =  Tji  for  all  i,j =  1, ... , N  and \\nthat Tii  =  0 for  all  i  =  1, ... , N.  We  will  make  these assumptions only when explicitly \\nstated. \\n\\nWe  are  interested  in  the  qualitative  behavior  of solutions  of (1)  near  equilibrium \\npoints  (rest  positions  where  Ui  ==  0,  for i  =  1, ... , N).  By  setting the external inputs \\nUi(t),  i = 1, ... , N, equal to zero, we  define U*  =  [ui, ... , u\"NV fRN  to be an equilibrium \\nfor  (1)  provided  that  -biui\\' + E.f=l Aij  Gj(uj) = 0, \\nfor  i  = 1, ... ,N. The locations \\nof such  equilibria in  RN  are  determined  by  the  interconnection  pattern of the  neural \\nnetwork (i.e., by the parameters Aij, i,j =  1,. \", N) as  well as by the parameters bi  and \\nthe nature of the nonlinearities  Gi(\\')\\' i  = 1, ... ,N. \\n\\nThroughout, we  will  assume that a given equilibrium u*  being analyzed is  an isolated \\nequilibrium for  (1), i.e., there exists an r  > 0 such that in the neighborhood  B( u*, r) = \\n{( u  - u*)fRN  : lu - u*1  < r}  no equilibrium for  (1), other than u  =  u*, exists. \\n\\nWhen analyzing the stability properties of a given equilibrium point, we  will be able \\nto assume, without loss of generality, that this equilibrium is located at the origin u  =  0 \\nof RN.  If this is not the case, a  trivial transformation can be employed which  shifts the \\nequilibrium point  to the origin and which  leaves  the structure of (1)  the same. \\n\\n\\x0c556 \\n\\nINTERCONNECTED SYSTEMS  VIEWPOINT \\n\\nWe  will  find  it convenient  to  view  system  (1)  as  an interconnection of N  free  sub(cid:173)\\n\\nsystems (or  isolated sUbsystems)  described  by equations of the form \\n\\nUnder this  viewpoint, the interconnecting structure of the system (1) is  given  by \\n\\nPi  =  -biPi + Aii Gi(Pi) + Ui(t). \\n\\nGi(Xb\"  . ,xn )  ~  L  AijGj(Xj),  i = 1, ... ,N. \\n\\nN \\n\\nj=1 \\nii:i \\n\\n(2) \\n\\n(3) \\n\\nFollowing  the  method  of analysis  advanced  in6 ,  we  will  establish  stability  results \\nwhich  are phrased in terms of the qualitative properties of the free  subsystems  (2)  and \\nin  terms  of the  properties  of the interconnecting  structure given  in  (3).  This  method \\nof analysis  makes  it  often  possible  to  circumvent  difficulties  that  arise  in the analysis \\nof complex  high-dimensional  systems.  Furthermore,  results  obtained  in  this  manner \\nfrequently  yield  insight into the dynamic behavior of systems in  terms of system  com-\\nponents and interconnections. \\n\\n. \\n\\nGENERAL STABILITY CONDITIONS \\n\\nWe  demonstrate below  an  example of a  result  for  exponential  stability  of an  equi(cid:173)\\n\\nlibrium  point.  The principal Lyapunov stability results for  such systems are presented, \\ne.g., in Chapter 5 of Ref.  7. \\n\\nWe  will  utilize the following  hypotheses in our first  result. \\n\\n(A-I)  For  system  (1), the external inputs are all zero, i.e., \\n\\nUi(t)  ==  0, \\n\\ni  =  1, ... , N. \\n\\n(A-2)  For system (1), the interconnections satisfy the estimate \\n\\nfor  all Ixil  < ri,  Ix;1  < rj,  i,j =  1, ... , N, where the ail  are real  constants. \\n\\n(A-3)  There exists an N-vector a>  (i.e., aT =  (al, ... ,aN) and ai >  0, \\n\\n1, ... ,N) such that the  test  matrix S = [Sij] \\n\\nfor all  ~  = \\n\\nis  negative  definite,  where  the bi  are  defined  in (1)  and the  aij  are  given  in  (A-2). \\n\\n\\x0c557 \\n\\nWe  are  now in a  position to state and prove the following  result. \\n\\nTheorem 1  The equilibrium x = 0  of the  neural network (1)  is exponentially stable \\nif hypotheses  (A-l),  (A-2)  and  (A-3)  are  satisfied. \\n\\nProof.  For (1)  we  choose the  Lyanpunov function \\n\\nwhere  the  ai  are  given  in  (A-3).  This  function  is  clearly  positive  definite.  The  time \\nderi vati ve  of v  along the solutions of (1) is  given  by \\n\\n(4) \\n\\nDV(1)(X)  = 2: 2ai(2xd[-biXi + 2: Aij Gj(Xj)] \\n\\nN  1 \\n\\ni=1 \\n\\nwhere  (A-l) has been invoked.  In view of (A-2)  we  have \\n\\nN \\n\\nj=1 \\n\\nN \\n\\nj=1 \\n\\nDV(1)( x)  <  2: ai( -bix~ + Xi 2: aijX j) \\n\\nN \\n\\ni=1 \\n\\nwhere  r = mini(ri), IxI2 = (Ef:1 X~) 1/2, and  the matrix R =  [rij]  is  given  by \\n\\nfor  all IxI2  < r \\n\\nr;j = { ai( -bi + aii), \\n\\nai aij, \\n\\nt  = J \\ni ::J  j. \\n\\nBut it follows  that \\n\\nxT Rx =  xT ( R ~ RT)  X  =  xT Sx ::;  )w(S)  Ixl1 \\n\\n(5) \\n\\nwhere  S  is  the  matrix  given  in  (A-3)  and  AM(S)  denotes  the  largest  eigenvalue  of \\nthe  real  symmetric  matrix  S.  Since  S  is  by  assumption  negative  definite,  we  have \\nAM(S) < O.  It follows  from  (4)  and  (5)  that in some neighborhood of the origin x  =  0, \\nwe  have  c1lxl~  ~ v(x)  ~ c2lxl~  and  DV(1)(X)  ~ -c3Ixl~,  where  C1  =  ! mini ai  >  0, \\nC2  = ! maxi ai > 0, and C3  =  -AM(S) > O.  Hence,  the equilibrium  x  =  of the neural \\n\\nnetwork  (1)  is  exponentially stable (c.f.  Theorem 9.10 in Ref.  7). \\n\\nConsistent  with  the  philosophy  of viewing  the  neural  network  (1)  as  an  intercon(cid:173)\\n\\nnection of N  free  subsystems  (2),  we  think  of the  Lyapunov function  (4)  as  consisting \\nof a  weighted sum of Lyapunov functions for  each free  subsystem  (2)  (with  Ui(t)  ==  0) . \\nThe  weighting  vector  a  >  0  provides  flexibility  to emphasize  the  relative  importance \\nof the qualitative  properties of the  various individual subsystems.  Hypothesis  (A - 2) \\nprovides  a  measure of interaction between  the various subsystems  (3).  Furthermore, it \\nis  emphasized  that  Theorem  1 does  not  require  that the parameters  Aij in  (1)  form  a \\nsymmetric matrix. \\n\\n\\x0c558 \\n\\nWEAK  COUPLING CONDITIONS \\n\\nThe test  matrix S given in hypothesis (A - 3) has off-diagonal terms which may be \\npositive  or  nonpositive.  For  the  special  case  where  the  off-diagonal  terms  of the  test \\nmatrix S =  [Sij]  are non-negative, equivalent stability results may be obtained which are \\nmuch easier to apply than Theorem  1.  Such results  are called  weak-coupling  conditions \\nin  the literature6,9.  The  conditions  8ij  ~ 0  for  all  i  ::J  j  may  reflect  properties  of the \\nsystem (1)  or they may be the consequence of a  majorization process. \\n\\nIn  the  proof of the  subsequent  result,  we  will  make  use  of some of the  properties \\nof M- matrices  (see,  for  example,  Chapter  2  in  Ref.  6).  In  addition  we  will  use  the \\nfollowing  assumptions. \\n\\n(A-4)  For system (1),  the nonlinearity Gi(Xi) satisfies  the sector condition \\n\\n(A-S)  The successive principal minors of the  N  X  N  test  matrix D  =  [dij ] \\n\\nare all positive where,  the bi  and Aij  are defined in (1)  and Ui2  is defined in (A - 4). \\nTheorem 2  The  equilibrium x  = 0  of the  neural network  (1)  is asymptotically sta(cid:173)\\nble  if hypotheses  (A-1),  (A-4)  and  (A-5)  are  true. \\n\\nProof.  The  proof proceeds10  along  lines  similar  to  the  one  for  Theorem  1,  this  time \\nwith the following  Lyapunov function \\n\\nN \\n\\nv(x) = L: Qilxd \\n\\ni=l \\n\\n(6) \\n\\nThe above  Lyapunov function  again reflects  the interconnected nature of the whole \\nsystem.  Note  that  this  Lyapunov  function  may  be  viewed  as  a  generalized  Hamming \\ndistance of the state vector from  the origin. \\n\\nESTIMATES  OF TRAJECTORY BOUNDS \\n\\nIn  general,  one  is  not  only  interested  in  questions  concerning  the  stability  of  an \\n\\nequilibrium of the system  (1),  but also in performance.  One way  of assessing  the qual(cid:173)\\nitative  properties of the neural  system  (1)  is  by  investigating solution  bounds  near  an \\nequilibrium of interest.  We  present here  such a  result by assuming  that the hypotheses \\nof Theorem 2 are satisfied. \\n\\nIn the following,  we  will  not require  that the external inputs  Ui(t),  i  =  1, ... , N  be \\n\\nzero.  However, we  will need to make  the  additional assumptions enumerated below. \\n\\n\\x0c559 \\n\\n(A-6)  Assume that there exist  .xi  > 0,  for i  =  1, ... , N, and  an  (  > 0  such that \\n\\n(~~) IAjil  >  (  >  0, \\n\\ni  =  1, ... ,N \\n\\nN \\n\\nL: \\nj=1 \\ni:/;j \\n\\nwhere bi  and  Aij  are defined  in  (1)  and (Ti2  is  defined in (A-4). \\n\\n(A-7)  Assume that for  system  (1), \\n\\nN L: .xiIUi(t)1  ~ k  for  all \\n\\ni=l \\n\\nt ~ 0 \\n\\nfor  some constant  k  > 0  where the .xi,  i  = 1, ... , N  are defined  in  (A-6). \\n\\nIn  the  proof of our  next  theorem,  we  will  make  use  of a  comparison  result.  We \\nconsider  a  scalar comparison equation of the form  iJ  =  G(y)  where  y(R,G : B(r) - R \\nfor  some  r  > 0,  and G is continuous on B(r) =  {XfR: Ixl  < r}.  We  can  then prove the \\nfollowing  auxiliary  theorem:  Let  p(t)  denote  the  maximal  solution  of the  comparison \\nequation  with  p(to)  =  Yo(B(r), \\nt  ~ to  ~ 0  is  a  continuous \\nfunction  such  that  r(to)  $  Yo,  and  if  r(t)  satisfies  the  differential  inequality  Dr(t)  = \\nlimk-+O+  t sup[r(t + k)  - r(t)]  $  G(r(t))  almost  everywhere,  then  r(t)  $  p(t) for  t  ~ \\nto  ~ 0,  for  as  long  as  both  r(t)  and  p(t) exist.  For  the proof of this  result,  as  well  as \\nother comparison  theorems, see e.g.,  Refs.  6  and 7. \\n\\nt  ~ to  >  O.  If r(t), \\n\\nFor  the  next  theorem,  we  adopt  the  following  notation.  We  let  6  =  mini (Til \\nwhere  (Til  is  defined  in  (A  - 4),  we  let  c  =  (6  ,  where  (  is  given  in  (A-6),  and \\nwe  let  (t,to,xo)  =  [I(t,to,xo)\\'\\'\\'\\',</>N(t,to,xo)]T  denote  the  solution  of  (1)  with \\n(to, to, xo)  =  Xo  =  (XlO,\"\"  xNol for some to  ~ O. \\n\\nWe  are  now  in a  position  to  prove  the following  result,  which  provides  bounds  for \\n\\nthe solution of(1). \\n\\nTheorem 3  Assume  that  hypotheses  (A-6)  and  (A-7)  are  satisfied.  Then \\n\\nk) \\n11(t, to, xo)11  =  L...\" .xili(t, to, xo)  ::;  (a - - e-\\nC \\n\\ni=l \\n\\nc(t  t) \\n\\n- 0  + -,  t  ~ to  ~ 0 \\n\\nk \\nC \\n\\n~ ~ \\n\\nI \\n\\nprovided that a  > k/c and  IIxoll  =  E~l .xilxiOI  ~ a,  where  the  .xi,  i  =  1,. \", N  are \\n\\ngiven  in  (A-6)  and k  is given  in  (A-7). \\n\\nProof.  For (1)  we  choose the Lyapunov function \\n\\nN \\n\\nv(x) = L .xilxil \\n\\ni=l \\n\\n(7) \\n\\n\\x0c560 \\n\\nAlong the  solutions of (1), we  obtain \\n\\nDV(l)(X)  ~ AT Dw + z: Ai!Ui(t)\\\\ \\n\\nN \\n\\ni=l \\n\\n(8) \\n\\nwhere  wT = [G1J;d\\\\Xl\\\\,\\'\\'\\'\\' G\\'Z~N)lxN\\\\]\\' A = (A}, ... ,ANf, and  D = [dij]  is  the test \\nmatrix  given  in  (A-5).  Note  that  when  (A-6)  is  satisfied,  as  in  the  present  theorem, \\ni = 1, ... , N) \\nthen  (A-5)  is  automatically satisfied.  Note also  that w  ~ 0  (Le.,  Wi  ~ 0, \\nand w  =  0 if and only if x = O. \\nUsing manipulations involving (A-6), (A-7) and (8), it is easy to show that DV(l)(X)  ~ \\n-cv(x) + k.  This ineqUality  yields  now  the  comparison equation  iJ  =  -cy + k,  whose \\nunique solution is  given  by \\n\\npet, to, Po)  =  (Po - ~) e-c(t-to) +~,  for all  t ~ to. \\n\\nH we  let  r  = v, then we  obtain from  the comparison result \\n\\npet)  ~ ret) =  v(4)(t,to,xo)) = 2: Ail4>i(t,to,xo)1  =  114>(t,to,xo)\\\\I, \\n\\nN \\n\\ni=l \\n\\ni.e.,  the desired  estimate is  true, provided  that  Ir(to)\\\\  = Ef:l Ai/XiOI  = IIxoll  ~ a  and \\na> kjc. \\n\\nESTIMATES  OF DOMAINS  OF ATTRACTION \\n\\nNeural  networks  of the  type  considered  herein  have  many  equilibrium  points.  If \\na  given  equilibrium  is  asymptotically  stable,  or exponentially  stable,  then  the  extent \\nof this  stability  is  of interest.  As  usual,  we  assume  that  x  =  0  is  the equilibrium  of \\ninterest.  If 4>(t, to, xo)  denotes a solution of the network (1)  with 4>(to, to, xo) = xo,  then \\nwe  would like  to know for  which  points Xo  it is  true that 4>( t, to, xo)  tends to the origin \\nas t  ---+  00.  The set of all such points Xo  makes up the domain of attraction (the basin  of \\nattraction)  of the equilibrium  x  =  O.  In general,  one  cannot  determine such  a  domain \\nin  its  entirety.  However,  several  techniques  have  been  devised  to estimate  subsets  of \\na  domain  of attraction.  We  apply  one  such  method  to  neural  networks,  making  use \\nof Theorem  1.  This  technique  is  applicable  to  our  other  results  as  well,  by  making \\nappropriate modifications. \\n\\nWe  assume that the hypotheses (A-I), (A-2) and (A-3) are satisfied and for  the free \\n\\nsubsystem  (2)  we  choose  the Lyapunov  function \\n\\n1  2 \\nVi(Pi)  = 2 Pi\\' \\n\\n(9) \\nThen  DVi(2) (Pi)  ~ (-bi + aii)p~,  \\\\Pi/  < ri  for  some  ri  > O.  If (A-3)  is  satisfied,  we \\n\\nmust have (-bi + aii) < 0 and DVi(2)(Pi)  is  negative definite over  B(ri). \\n\\nLet Gvo;  = {PifR : Vi(Pi)  = !p~ < trl ~ Voi}.  Then GVo ;  is  contained in the domain \\n\\nof attraction of the equilibrium Pi  =  0 for  the free  subsystem (2). \\n\\nTo  obtain  an  estimate  for  the  domain  of attraction of x  =  0 for  the  whole  neural \\n\\nnetwork  (1),  we  use the Lyapunov function \\n\\n\\x0cN  1 \\n\\nN \\n\\nv(x) - \\'\"\\' -\"\\'x~ - \\'\"\\' ov(x) \\n\\n-LJ2 .....  -LJ     .  \\n\\n561 \\n\\n(10) \\n\\nIt is now  an  easy  matter to show that the set \\n\\ni=l \\n\\ni=l \\n\\nC>.  = {uRN: v(x) = LOiVi(Xi) < oX} \\n\\nN \\n\\ni=l \\n\\nwill  be  a  subset of the domain of attraction of x = 0 for  the neural network  (1), where \\n\\noX  =  min  (OiVOi)  =  min  (~Oir~) . \\n\\n1$.i$.N  2 \\n\\n \\n\\nl$.i$.N \\n\\nIn  order  to obtain  the  best  estimate  of the  domain  of attraction  of x  = 0  by  the \\npresent method, we  must choose the 0i in an optimal fashion.  The reader is referred  to \\nthe literature9 ,l3,l4  where several  methods to accomplish  this are discussed. \\n\\nINSTABILITY  RESULTS \\n\\nSome  of the equilibrium  points  in  a  neural  network  may  be  unstable.  We  present \\nhere  a  sample instability  theorem  which  may  be  viewed  as  a  counterpart  to  Theorem \\n2.  Instability  results,  formulated  as  counterparts  to other stability  results  of the  type \\nconsidered herein  may  be obtained by making appropriate modifications. \\n\\n(A-B)  For system (1), the interconnections satisfy the estimates \\n\\nXiAiiGi(Xi)  <  OiAiiX;, \\n\\nIXiAjjGj(xj)1  $ \\n\\nIxdlAijlO\"j2l xil,  if; j \\n\\nwhere  OJ  = O\"il  when  Aii  < 0 and Oi  =  O\"i2  when  Aii  > 0 for  all  IXil  < ri,  and for \\nalllXjl < Tj,i,j = 1, ... ,N. \\n\\n(A-9)  The successive  principal minors of the N  x  N  test matrix D = [dij ] given by \\n\\nare  positive,  where  O\"i  =  ~ - Au  when  ifFIl  (i.e.,  stable subsystems)  and  O\"i \\n-!:; + Aji  when  ifFu  (i.e.,  unstable  subsystems)  with  F  = FII  U  Fu  and  F  = \\n{I, ., . , N} and Fu  f;  </>. \\n\\nWe  are now in a  position to prove the following  result. \\n\\nTheorem 4  The  equilibrium x  =  0  of the  neural network (1)  is unstable if hypotheses \\n(A-l),  (A-8)  and (A-g)  are  satisfied.  If in  addition,  FII  = </> \\n(</>  denotes  the  empty set), \\nthen  the  equilibrium  x  = 0  is completely unstable. \\n\\n\\x0c562 \\n\\nProof.  We choose the Lyapunov function \\n\\nifF .. \\n\\nifF. \\n\\nwhere ai  > 0,  i  =  1, ... ,N.  Along  the solutions of (1)  we  have  (following  the proof of \\nr  = miniri  where  aT =  (a}, ... ,aN), \\nTheorem  2),  DV(l)(X)  $  -aTDw for  all  xB(r), \\n[ G1l;d IXll, ... , GNx~N) IXNI].  We  conclude  that \\nD  is  defined  in  (A-9),  and  wT  = \\nDV(l)(X)  is  negative  definite  over  B(r).  Since  every  neighborhood of the origin  x  =  \\ncontains at least one point x\\' where v(x\\') < 0, it follows  that the equilibrium  x =  0 for \\n(1)  is  unstable.  Moreover,  when F, =  </>,  then the function  v(x) is  negative definite and \\nthe equilibrium  x  =  0 of (1) is in fact  completely unstable (c.f.  Chapter 5 in Ref.  7). \\n\\n(11) \\n\\nSTABILITY UNDER STRUCTURAL PERTURBATIONS \\n\\nIn specific applications involving adaptive schemes for  learning algorithms in neural \\nnetworks,  the  interconnection  patterns  (and  external  inputs)  are  changed  to  yield  an \\nevolution of different  sets  of desired  asymptotica.l1y  stable equilibrium  points  with  ap(cid:173)\\npropriate domains of attraction.  The present  diagonal dominance conditions  (see, e.g., \\nhypothesis  (A-6))  can  be  used  as  constraints  to guarantee  that  the  desired  equilibria \\nalways  have the desired  stability properties. \\n\\nTo be more specific, we assume that a given neural network has been designed with a \\nset of interconnections whose strengths can be varied from  zero to some specified values. \\nWe express  this  by writing in place of (1), \\n\\nXi  =  -biXi + L:8ij Aij Gj(Xj) + Ui(t), \\n\\nN \\n\\nj=l \\n\\nfor  i =  1, ... ,N, \\n\\n(12) \\n\\nwhere  0  $  8ij  $  1.  We  also  assume  that in  the given  neural network  things  have been \\narranged  in  such  a  manner  that  for  some  given  desired  value  ~ >  0,  it  is  true  that \\n~ =  mini (!:; - 8iiAii).  From  what  has  been  said  previously,  it should  now  be  clear \\nthat if Ui( t)  ==  0,  i = 1, ... ,N and if the diagonal  dominance conditions \\n\\n~ - t  (~~) 18ij Aiji  > 0,  for  i  =  1, ... ,N \\n\\n(13) \\n\\nj  = 1 \\ni:f;j \\n\\nare satisfied  for  some  Ai  > 0,  i  = 1, ... , N,  then the equilibrium  x  =  for  (12)  will  be \\n\\nasymptotically stable.  It is important to recognize that condition (13) constitutes a sin(cid:173)\\ngle stability condition for  the neural network under structural perturbations.  Thus, the \\nstrengths of interconnections of the neural network  may  be rearranged in  any  manner \\nto achieve  some  desired  set  of equilibrium  points.  If (13)  is  satisfied,  then  these  equi(cid:173)\\nlibria will  be  asymptotically  stable.  (Stability under  structural perturbations is  nicely \\nsurveyed in Ref.  15.) \\n\\n\\x0c563 \\n\\nCONCLUDING  REMARKS \\n\\nIn  the  present  paper  we  surveyed  and  applied  results  from  the  qualitative  theory \\n\\nof large  scale  interconnected  dynamical  systems  in order to  develop  a  qualitative the(cid:173)\\nory  for  neural  networks  of the  Hopfield  type.  Our  results  are  local  and  use  as  much \\ninformation  as  possible  in  the  analysis  of a  given  eqUilibrium.  In  doing  so,  we  estab(cid:173)\\nlished  cri-teria  for  the  exponential  stability,  asymptotic  stability,  and instability  of an \\nequilibrium  in such  networks.  We  also  devised  methods  for  estimating the  domain  of \\nattraction of an  asymptotically stable equilibrium and for estimating trajectory bounds \\nfor  such  networks.  Furthermore,  we  showed  that  our  stability  results  are  applicable \\nto  systems  under  structural  perturbations  (e.g.,  as  experienced  in neural  networks  in \\nadaptive learning schemes). \\n\\nIn  arriving  at  the  above  results,  we  viewed  neural  networks  as  an  interconnection \\n\\nof many  single  neurons,  and we  phrased our results  in terms  of the qualitative proper(cid:173)\\nties  of the free  single  neurons  and  in  terms  of  the  network  interconnecting  structure. \\nThis viewpoint is  particularly well  suited for  the study of hierarchical structures which \\nnaturally lend themselves to implementations16  in VLSI.  Furthermore, this type of ap(cid:173)\\nproach  makes  it  possible  to  circumvent  difficulties  which  usually  arise  in  the  analysis \\nand synthesis of complex high  dimensional systems. \\n\\nREFERENCES \\n\\n[1]  For a review, see, Neural Networks for Computing, J. S. Denker, Editor, American \\n\\nInstitute of Physics Conference Proceedings 151, Snowbird,  Utah, 1986. \\n\\n[2]  J.  J. Hopfield and D.  W.  Tank,  Science 233, 625  (1986). \\n[3]  J.  J. Hopfield,  Proc.  Natl.  Acad.  Sci.  U.S.A.  79,2554 (1982),  and  ibid.  81,3088 \\n\\n(1984). \\n\\n[4]  G.  E. Hinton and J. A.  Anderson, Editors, Parallel Models  of Associative Memory, \\n\\nErlbaum,  1981. \\n\\n[5]  T. Kohonen,  Self-Organization  and Associative  Memory,  Springer-Verlag,  1984. \\n[6]  A.  N.  Michel  and  R.  K.  Miller,  Qualitative  Analysis  of Large  Scale  Dynamical \\n\\nSystems, Academic Press,  1977. \\n\\n[7]  R.  K.  Miller  and A.  N.  Michel,  Ordinary  Differential Equations, Academic Press, \\n\\n1982. \\n\\n[8]  I.  W.  Sandberg, Bell System  Tech.  J.  48, 35  (1969). \\n[9]  A.  N.  Michel,  IEEE  Trans.  on  Automatic  Control 28, 639  (1983). \\n[10]  A.  N.  Michel, J. A.  Farrell, and W.  Porod, submitted for  publication. \\n[11]  J.-H.  Li, A.  N.  Michel, and W.  Porod, IEEE  Trans.  Cire.  and Syst., in press. \\n[12]  G.  A.  Carpenter, M.  A.  Cohen,  and  S.  Grossberg,  Science  235,  1226  (1987). \\n[13]  M.  A.  Pai,  Power System  Stability, Amsterdam, North Holland,  1981. \\n[14]  A.  N.  Michel,  N.  R.  Sarabudla,  and  R.  K.  Miller,  Circuits,  Systems  and  Signal \\n\\nProcessing 1, 171  (1982). \\n\\n[15]  Lj.  T.  Grujic, A.  A.  Martynyuk and  M.  Ribbens-Pavella,  Stability  of Large-Scale \\nSystems  Under Structural and Singular Perturbations,  Nauka Dumka, Kiev,  1984. \\n\\n[16]  D.  K.  Ferry and W.  Porod,  Superlattices  and  Microstructures 2, 41  (1986). \\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrT-55Qvqtiz",
        "outputId": "617d4a89-fb46-4102-a8c0-a43784a941fa"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "gpe = [] # countries, cities, states\n",
        "\n",
        "city2 = []\n",
        "country2 = []\n",
        "\n",
        "for i in range(10):\n",
        "  gpe = []\n",
        "  doc = nlp(papers_df['full_text'][i].lower().split('abstract')[0].replace('\\n',''))\n",
        "  for ent in doc.ents:\n",
        "      if (ent.label_ == 'GPE'):\n",
        "          gpe.append(ent.text)\n",
        "  print('---------- Iteration Number ',i)\n",
        "  print(gpe)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Iteration Number  0\n",
            "['mayfield road', 'edinburgh', 'scotland']\n",
            "---------- Iteration Number  1\n",
            "['california', 'pasadena']\n",
            "---------- Iteration Number  2\n",
            "['israel']\n",
            "---------- Iteration Number  3\n",
            "[]\n",
            "---------- Iteration Number  4\n",
            "['athens', 'athens', 'greece']\n",
            "---------- Iteration Number  5\n",
            "[]\n",
            "---------- Iteration Number  6\n",
            "['los angeles']\n",
            "---------- Iteration Number  7\n",
            "[]\n",
            "---------- Iteration Number  8\n",
            "['hampton va']\n",
            "---------- Iteration Number  9\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "h5qQM7u44jLh",
        "outputId": "1c03cf95-594d-447e-eb90-8ad90685c6ef"
      },
      "source": [
        "papers_df['full_text'][i].lower()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'573 \\n\\nbit - serial neural  networks \\n\\nalan f.  murray,  anthony v . w.  smith  and zoe f.  butler. \\n\\ndepartment of electrical engineering,  university of edinburgh, \\n\\nthe king\\'s buildings, mayfield road,  edinburgh, \\n\\nscotland,  eh93jl. \\n\\nabstract \\n\\na  bit  - serial  vlsi  neural  network  is  described  from  an  initial  architecture  for  a \\nsynapse array through to silicon layout and board design.  the issues surrounding bit \\n- serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel \\ndevelopment  of  a  hybrid  analog/digital  neural  network  is  outlined.  learning  and \\nrecall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected \\nspecification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 mhz.  this tech(cid:173)\\nnique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms, \\nusing  a  \"paging\"  technique  to  time  - multiplex  calculations  through  the  synapse \\narray. \\n\\n1. introduction \\n\\nthe functions a  synthetic neural network may aspire to mimic are the ability to con(cid:173)\\nsider  many  solutions  simultaneously,  an  ability  to  work  with  corrupted  data  and  a \\nnatural  fault  tolerance.  this  arises  from  the  parallelism  and  distributed  knowledge \\nrepresentation  which  gives  rise  to  gentle  degradation  as  faults  appear.  these func(cid:173)\\ntions  are  attractive  to implementation  in vlsi  and  wsi.  for example,  the natural \\nfault  - tolerance  could  be  useful  in  silicon  wafers  with  imperfect  yield,  where  the \\nnetwork  degradation  is  approximately  proportional  to  the  non-functioning  silicon \\narea. \\nto cast  neural networks in engineering language,  a  neuron is a  state machine that is \\neither  \"on\"  or  \"off\\',  which  in  general  assumes  intermediate  states  as  it  switches \\nsmoothly  between  these  extrema.  the  synapses  weighting  the  signals  from  a \\ntransmitting neuron  such that it is more or less excitatory or inhibitory to the receiv(cid:173)\\ning  neuron.  the  set  of synaptic weights  determines  the stable  states and  represents \\nthe learned  information in a system. \\nthe  neural  state,  vi\\'  is  related  to  the  total  neural  activity  stimulated  by  inputs  to \\nthe  neuron  through  an  activation junction,  f.  neural  activity  is  the  level  of excita(cid:173)\\ntion  of the  neuron  and the  activation  is  the way  it  reacts  in a  response to a  change \\nin activation. the neural output state at time t, v[,  is related to x[ by \\n\\nv[  = f (xf) \\n\\n(1) \\n\\nthe  activation  function  is  a  \"squashing\"  function  ensuring  that  (say)  vi  is  1  when \\nxi  is large  and  -1  when xi  is  small.  the neural update function  is therefore straight(cid:173)\\nforward: \\n\\n. \\n\\n,+1  - ,   + ~  ~ t  v\\' \\nj \\nxi \\n\\ni-n-l \\n0  ~  ii \\n\\n- xi \\n\\n  \\n\\nj-o \\n\\n(2) \\n\\nwhere  8  represents  the  rate  of change  of neural  activity,  tij \\nand n  is  the number of terms giving an n  - neuron array [1]. \\nalthough  the  neural function  is  simple  enough,  in  a  totally  interconnected  n  - neu(cid:173)\\nron  network  there  are n 2  synapses requiring n 2  multiplications  and  summations and \\n\\nis  the  synaptic  weight \\n\\n american institute of physics 1988 \\n\\n\\x0c574 \\n\\na large number of interconnects.  the challenge in vlsi is therefore to design a  sim(cid:173)\\nple,  compact  synapse  that  can  be  repeated  to  build  a  vlsi  neural  network  with \\nin  a  network  with  fixed  functionality,  this  is  relatively \\nmanageable  interconnect. \\nstraightforward.  h the  network  is to be able to learn,  however,  the synaptic weights \\nmust  be programmable, and therefore more complicated. \\n\\n2. designing  a neural  network in  vlsi \\n\\nthere  are  fundamentally  two  approaches  to  implementing  any  function  in  silicon  -\\ndigital and analog.  each technique has its advantages and  disadvantages,  and these \\nare  listed  below,  along  with  the  merits  and  demerits  of bit  - serial  architectures  in \\ndigital (synchronous) systems. \\ndigital  vs.  analog:  the  primary  advantage  of digital  design  for  a  synapse  array  is \\nthat  digital  memory  is  well  understood,  and  can  be  incorporated  easily.  learning \\nnetworks are  therefore  possible  without  recourse  to unusual  techniques  or technolo(cid:173)\\ngies.  other strengths of a digital approach are that design techniques are advanced, \\nautomated  and  well  understood  and  noise  immunity  and  computational  speed  can \\nbe  high.  unattractive features  are  that  digital  circuits  of this complexity need  to  be \\nsynchronous  and  all  states  and  activities  are  quantised,  while  real  neural  networks \\nare  asynchronous  and  unquantised.  furthermore,  digital  multipliers  occupy  a  large \\nsilicon  area, giving a low synapse count on  a single chip. \\nthe  advantages  of  analog  circuitry  are  that  asynchronous  behaviour  and  smooth \\nneural  activation  are  automatic.  circuit  elements can  be  small,  but  noise  immunity \\nis relatively  low  and  arbitrarily  high  precision is not  possible.  most  importantly,  no \\nreliable  analog,  non  - volatile  memory  technology  is  as  yet  readily  available.  for \\nthis  reason,  learning  networks  lend  themselves  more  naturally to  digital  design  and \\nimplementation. \\nseveral  groups  are  developing  neural  chips  and  boards,  and  the  following  listing \\ndoes  not  pretend  to  be  exhaustive.  it is  included,  rather,  to indicate  the spread  of \\nactivity  in  this  field.  analog  techniques  have  been  used  to  build  resistor  i  opera(cid:173)\\ntional  amplifier  networks [2,3]  similar to  those  proposed  by  hopfield  and tank [4]. \\na  large  group  at  caltech  is  developing  networks  implementing  early  vision  and \\nauditory  processing  functions  using the intrinsic nonlinearities of mas transistors in \\nthe subthreshold  regime  [5,6].  the problem of implementing analog  networks with \\nelectrically  programmable  synapses  has  been  addressed  using  ccdimnos technol(cid:173)\\nogy  [7].  finally,  garth  [8]  is  developing  a  digital  neural  accelerator  board  (\"net(cid:173)\\nsim\")  that  is  effectively  a  fast  simd  processor  with  supporting  memory  and  com(cid:173)\\nmunications chips. \\nbit - serial  vs.  bit  - parallel:  bit  - serial  arithmetic and  communication  is  efficient \\nfor  computational  processes,  allowing  good  communication  within  and  between \\nvlsi  chips  and  tightly  pipelined  arithmetic  structures.  it  is  ideal  for  neural  net(cid:173)\\nworks  as  it  minimises  the  interconnect  requirement  by  eliminating  multi  - wire \\nbusses.  although  a  bit  - parallel  design  would  be  free  from  computational  latency \\n(delay  between  input  and  output),  pipelining  makes  optimal  use  of  the  high  bit  -\\nrates possible in serial systems,  and  makes for  efficient circuit usage. \\n2.1  an asynchronous pulse stream vlsi neural network: \\nin  addition  to  the  digital  system  that  forms  the  substance  of  this  paper,  we  are \\ndeveloping  a  hybrid  analog/digital  network  family.  this work  is  outlined  here,  and \\nhas  been  reported  in  greater  detail  elsewhere  [9, 10, 11].  the  generic  (logical  and \\nlayout)  architecture  of a  single  network  of n  totally  interconnected neurons is  shown \\n\\n\\x0c575 \\n\\nschematically  in  figure  1.  neurons  are  represented  by  circles,  which  signal  their \\nstates,  vi  upward  into  a  matrix  of  synaptic  operators.  the  state  signals  are  con(cid:173)\\nnected  to  a  n  - bit  horizontal  bus  running  through  the  synaptic  array,  with  a  con(cid:173)\\nnection  to  each  synaptic  operator  in  every  column.  all  columns  have  n  operators \\n(denoted  by  squares)  and  each  operator adds its synaptic contribution,  tij v j\\n,  to the \\nrunning  total  of  activity  for  the  neuron  i  at  the  foot  of  the  column.  the  synaptic \\nfunction  is  therefore  to  multiply  the  signalling  neuron  state,  vj\\n,  by  the  synaptic \\nweight,  tij ,  and  to  add  this  product  to  the  running  total.  this  architecture  is com(cid:173)\\nmon to both  the bit - serial and pulse - stream networks. \\n\\nsynapse \\n\\nstates { vj  } \\n\\nfigure 1. generic architecture for  a  network of n totally interconnected neurons. \\n\\nneurons \\n\\nj=o \\n\\nj=ii -1 \\n\\nthis type of architecture has many attractions for  implementation in 2  - dimensional \\nsilicon  as  the  summation  2  tij vj  is  distributed  in  space.  the  interconnect \\nrequirement  (n  inputs  to  each  neuron)  is  therefore  distributed  through  a  column, \\nreducing the need  for  long - range wiring.  the architecture is modular,  regular and \\ncan be easily expanded. \\nin  the  hybrid  analog/digital  system,  the  circuitry  uses  a  \"pulse  stream\"  signalling \\nmethod  similar  to  that  in  a  natural  neural  system.  neurons  indicate  their  state  by \\nthe  presence  or  absence  of  pulses  on  their  outputs,  and  synaptic  weighting  is \\nachieved  by  time  - chopping  the  presynaptic  pulse  stream  prior  to  adding  it  to  the \\npostsynaptic  activity  summation.  it  is  therefore  asynchronous  and  imposes  no fun(cid:173)\\ndamental  limitations  on  the  activation  or  neural  state.  figure  2  shows  the  pulse \\nstream  mechanism  in  more  detail.  the synaptic  weight  is  stored  in  digital  memory \\nlocal to the operator.  each synaptic operator has an  excitatory and inhibitory  pulse \\nstream  input  and  output.  the  resultant  product  of  a  synaptic  operation,  tij vj\\n,  is \\nadded  to  the  running  total  propagating  down  either  the  excitatory  or  inhibitory \\nchannel.  one binary bit  (the  msbit)  of the  stored  tij  determines whether  the con(cid:173)\\ntribution  is excitatory or inhibitory. \\nthe  incoming  excitatory  and  inhibitory  pulse  stream  inputs  to  a  neuron  are \\nintegrated  to  give  a  neural  activation  potential  that varies  smoothly  from  0  to  5  v. \\nthis  potential controls a  feedback  loop with  an odd number of logic  inversions and \\n\\n\\x0c576 \\n\\n.   \\n\\nxt  \\n\\nv , \\n.u.u, \\n \\n\\nfigure  2.  pulse  stream  arithmetic.  neurons  are  denoted  by  0  and synaptic  operators \\nby  d. \\n\\nthus  forms  a  switched  \"ring - oscillator\".  h the inhibitory input dominates,  the feed(cid:173)\\nback  loop  is  broken.  h  excitatory  spikes  subsequently  dominate  at  the  input,  the \\nneural activity rises  to 5v and the feedback  loop oscillates with  a period determined \\nby a  delay  around  the loop.  the resultant  periodic waveform is then converted to a \\nseries  of voltage  spikes,  whose  pulse  rate  represents  the  neural  state,  vi\\'  interest(cid:173)\\ningly,  a  not  dissimilar  technique is  reported  elsewhere  in this volume,  although  the \\nsynapse function  is executed differently [12]. \\n\\n3. a 5  - state bit - serial neural  network \\n\\nthe  overall  architecture  of  the  5  - state  bit  - serial  neural  network  is  identical  to \\nthat  of  the  pulse  stream  network.  it  is  an  array  of n 2  interconnected  synchronous \\nsynaptic  operators,  and  whereas  the  pulse  stream  method  allowed  vj  to  assume  all \\nvalues  between  \"off\\' and  \"on\",  the  5 - state network vj  is constrained  to 0,  0.5 qr \\n 1.  the resultant  activation  function  is  shown  in  figure 3.  full  digital  multiplica(cid:173)\\ntion  is  costly  in  silicon  area,  but  multiplication  of  tij  by  vj  =  0.5  merely  requires \\nthe synaptic  weight  to be right  - shifted  by  1 bit.  similarly,  multiplication  by  0.25 \\ninvolves  a  further  right  - shift  of til\\'  and  multiplication  by 0.0  is  trivially  easy.  vj \\n<  0 is not  problematic,  as  a  switchable adder/subtractor  is  not much  more complex \\nthan  an  adder.  five  neural  states  are  therefore  feasible  with  circuitry  that  is  only \\nslightly more complex  than  a  simple serial adder.  the neural state expands from a  1 \\nbit  to  a  3  bit  (5  - state)  representation,  where  the  bits  represent  \"add/subtract?\", \\n\"shift?\" and \"multiply by o?\". \\nfigure 4  shows  part of the synaptic  array.  each synaptic operator includes an 8 bit \\nshift  register  memory  block  holding  the  synaptic  weight,  til\\'  a  3  bit  bus  for  the  5 \\nneural  states  runs  horizontally  above  each  synaptic  row.  single  phase  dynamic \\ncmos  has  been  used  with  a  clock  frequency  in  excess  of 20  mhz  [13).  details of \\na synaptic operator are  shown  in  figure 5.  the synaptic weight  til  cycles around the \\nshift  register  and  the  neural  state  vj  is  present  on  the  state  bus.  during  the  first \\nclock  cycle,  the  synaptic  weight  is  multiplied  by  the  neural  state  and  during  the \\nsecond,  the  most  significant  bit (msbit)  of the resultant  tij vj  is sign  - extended for \\n\\n\\x0c577 \\n\\nlhreshold \\n\\nstate vj \\n\\n..... -------=-------.. activity sj \\n\\ns \\n\\n\"5  state\" \\n\\n\"sharper\" \\n\\n\"smoother\" \\n\\n~.....::~-\"\\'--x.&..t------ activity \"j \\n\\nfigure 3.  \"hard - threshold\",  5  - state and sigmoid activation functions. \\n\\nj-a-1t  v \\n~  ..  j \\nj-li \\n\\nv, \\n\\nv, \\n\\nfigure 4.  section  of the  synaptic  array  of the  5  - state activation function  neural net(cid:173)\\nwork. \\n\\n8  bits  to  allow  for  word  growth  in  the  running  summation.  a  least  significant  bit \\n(lsbit)  signal  running down  the  synaptic  columns indicates the arrival  of the lsbit \\nof  the  xj  running  total.  if  the  neural  state  is  o.5  the  synaptic  weight  is  right \\nshifted  by  1 bit and then added to or subtracted from  the running total.  a  multipli(cid:173)\\ncation  of   1  adds  or  subtracts  the  weight  from  the  total  and  multiplication  by  0 \\n\\n\\x0c578 \\n\\n.0.5 \\n.0.0 \\n\\nadd/subtract \\n\\nadd! \\nsubtract \\n\\ncarry \\n\\nfigure s.  the  synaptic operator with a 5 - state activation function. \\n\\ndoes not alter the running summation. \\nthe  final  summation  at  the  foot  of the  column  is  thresholded  externally  according \\nto  the  5  - state activation function  in  figure  3.  as  the  neuron activity xj\\'  increases \\nthrough  a  threshold  value  x\" \\nideal  sigmoidal  activation  represents  a  smooth  switch \\nof  neural  state  from  -1  to  1.  the 5  - state  \"staircase\"  function  gives a  superficially \\nmuch  better  approximation  to  the  sigmoid  form  than  a  (much  simpler  to  imple(cid:173)\\nment)  threshold  function.  the  sharpness  of  the  transition  can  be  controlled  to \\n\"tune\"  the  neural dynamics for  learning and computation.  the control parameter is \\nreferred  to  as  temperature  by  analogy  with  statistical  functions  with  this  sigmoidal \\nform.  high  \"temperature\" gives a  smoother staircase and sigmoid,  while a tempera(cid:173)\\nture  of  0  reduces  both  to  the  \\'\\'hopfield\\'\\'  - like  threshold  function.  the  effects  of \\ntemperature  on  both  learning  and  recall  for  the  threshold  and  5  - state  activation \\noptions are discussed in section 4. \\n\\n4. learning and  recall  with vlsi  constraints \\n\\nbefore  implementing  the  reduced  - arithmetic  network  in  vlsi,  simulation  experi(cid:173)\\nments  were  conducted  to  verify  that  the  5  - state  model  represented  a  worthwhile \\nenhancement  over  simple  threshold  activation.  the  \"benchmark\"  problem  was \\nchosen  for  its  ubiquitousness,  rather  than  for  its  intrinsic  value.  the  implications \\nfor  learning  and  recall  of the  5  - state  model,  the  threshold  (2  - state)  model  and \\n- state)  were  compared  at  varying  temperatures \\nsmooth  sigmoidal  activation  (  00 \\nin  each  simulation  a  totally \\nwith  a  restricted  dynamic  range  for  the  weights  tij  \\ninterconnected  64  node  network  attempted  to  learn  32  random  patterns  using  the \\ndelta  rule  learning  algorithm  (see  for  example  [14]).  each  pattern  was  then  cor(cid:173)\\nrupted  with  25%  noise  and  recall  attempted  to  probe  the  content  addressable \\nmemory properties under the three different activation options. \\nduring  learning,  individual  weights  can  become  large  (positive  or  negative).  when \\nweights  are  \"driven\"  beyond  the  maximum  value  in  a  hardware  implementation, \\n\\n\\x0c579 \\n\\nwhich  is  determined  by  the  size  of  the  synaptic  weight  blocks,  some  limiting \\nmechanism  must  be  introduced.  for  example,  with  eight  bit  weight  registers,  the \\nlimitation is  -128  s  tij  s  127.  with integer weights,  this can be seen to be a prob(cid:173)\\nlem  of  dynamic  range,  where  it  is  the  relationship  between  the  smallest  possible \\nweight  ( 1) and the largest  (+ 127/-128) that is the issue. \\nresults:  fig.  6  shows  examples  of the  results  obtained,  studying  learning  using  5  -\\nstate  activation  at  different  temperatures,  and  recall  using  both  5  - state  and  thres(cid:173)\\nhold  activation.  at  temperature  t=o,  the  5  - state  and  threshold  models  are \\ndegenerate,  and  the results identical.  increasing smoothness of activation  (tempera(cid:173)\\nture)  during  learning  improves  the  quality  of  learning  regardless  of  the  activation \\nfunction  used  in  recall,  as more patterns are recognised  successfully.  using 5 - state \\nactivation  in recall  is more effective  than simple  threshold  activation.  the effect of \\ndynamic  range  restrictions  can  be  assessed  from  the  horizontal  axis,  where  t/j:6.  is \\nshown.  the results  from  these and  many  other experiments may  be  summarised  as \\nfollows:-\\n5 - state activation  vs.  threshold: \\n1)  learning with 5  - state activation was  protracted  over the threshold  activation, \\nas  binary  patterns  were  being  learnt,  and  the  inclusion  of  intermediate  values \\nadded extra degrees of freedom. \\n\\n2)  weight  sets  learnt  using  the  5  - state  activation  function  were  \"better\"  than \\nthose  learnt  via  threshold  activation,  as  the  recall  properties  of both  5  - state \\nand  threshold  networks  using  such  a  weight  set  were  more  robust  against \\nnoise. \\nfull  sigmoidal  activation  was  better  than  5  - state,  but  the  enhancement  was \\nless  significant  than  that  incurred  by  moving  from  threshold  - 5 - state.  this \\nsuggests  that the law  of diminishing returns  applies to  addition of levels to the \\nneural  state  vi\\'  this  issue  has  been  studied  mathematically  [15],  with  results \\nthat agree  qualitatively with  ours. \\n\\n3) \\n\\nweight saturation: \\nthree  methods  were  tried  to  deal  with  weight  saturation.  firstly,  inclusion  of  a \\ndecay,  or  \"forgetting\"  term  was  included  in  the  learning  cycle  [1].  it  is  our  view \\nthat  this  technique can  produce the desired weight limiting property,  but in  the time \\navailable  for  experiments,  we  were  unable  to  \"tune\"  the  rate  of  decay  sufficiently \\nwell  to  confirm  it.  renormalisation  of the  weights  (division  to  bring large  weights \\nback  into  the  dynamic  range)  was  very  unsuccessful,  suggesting  that  information \\ndistributed  throughout  the  numerically small  weights  was  being  destroyed.  finally, \\nthe  weights were  allowed  to  \"clip\"  (ie any weight  outside the dynamic range  was  set \\nto  the  maximum  allowed  value).  this method  proved  very  successful,  as  the learn(cid:173)\\ning  algorithm  adjusted the weights  over which  it still  had control  to  compensate for \\nthe  saturation effect.  it is  interesting to note  that  other experiments have indicated \\nthat  hopfield  nets  can  \"forget\"  in a  different  way,  under different learning control, \\ngiving  preference  to  recently acquired  memories [16].  the results  from  the  satura(cid:173)\\ntion experiments were:-\\n1) \\n\\nfor  the  32  pattemj64  node  problem,  integer  weights  with  a  dynamic  range \\ngreater than  30 were necessary to give enough  storage capability. \\nfor weights  with  maximum  values  tij  = 50-70,  \"clipping\"  occurs,  but  net(cid:173)\\nwork  performance  is  not  seriously  degraded  over  that  with  an  unrestricted \\nweight set. \\n\\n2) \\n\\n\\x0c580 \\n\\n15 \\n\\n\"0  10 \\nc = \\n.2 \\nen e u \\n5 --~ \\n\\n0 \\n\\n0 \\n\\ni \\n\\n\".\\' \\n\\n., ... \\n\\n.... ----------\\n\\n,-\\ne  ~ ;a ....... ;.. f:\\'-:\\' :::::7.:::.::-:::-: f\\'-. \\n,  ,. \\ni \\n! \\n! , \\ni \\ni \\ni , \\n\\n20  30 \\n\\n40  50  60  70 \\n\\nlimit \\n\\n15 \\n\\nt=30  _._.-.-\\nt=20 \\nt=10 \\nt=o \\n\\n-.-._.-.. \\n\\n,.. .. -..... -.. _ .. \\n, \\n.. \\ni \\nj\\'\\'\\'\\'--\\n,,\\'i \\n\\n- . . .,. \\'\" \\n\\nj \\n\\n~-------------\\n   \\n\\nj \\ni \\n\\no~~~~--~~ __ ~~ __ \\no \\n\\n20  30  40  50  60  70 \\n\\nlimit \\n\\n5 . state activation function  recal1 \\n\\ntlhopficld\" activation  function  recall \\n\\nfigure 6.  recall  of patterns  learned  with  the  5  .  state  activation function  and  subse(cid:173)\\nquently restored using  the 5-state and the  hard - threshold activation functions. \\nt  is  the  \"temperature\",  or smoothness  of the  activation function,  and \"limit\"  the  value \\nofti;   \\n\\nthese  results  showed  that  the  5  - state  model  was  worthy  of implementation  as  a \\nvlsi neural board, and suggested that 8 - bit weights were sufficient. \\n\\ns.  projected specification of a hardware neural  board \\n\\nthe specification of a  64  neuron board is  given  here,  using a  5 - state bit  - serial 64 \\nx 64  synapse array with  a derated clock speed  of 20 mhz.  the synaptic weights are \\n8  bit words and the word  length  of the running summation xi  is  16  bits to  allow for \\ngrowth.  a  64  synapse  column  has  a  computational  latency  of  80  clock  cycles  or \\nbits,  giving  an  update  time  of 4 .... s  for  the  network.  the  time  to  load  the  weights \\ninto  the  array  is  limited  to  6o .... s  by  the  supporting  ram,  with  an  access  time  of \\n12ons.  these  load  and  update  times  mean  that  the  network  is  executing  1  x  10\\' \\noperations/second,  where  one  operation  is    tlj  vj   this  is  much  faster  than  a \\nnatural  neural  network,  and  much  faster  than  is  necessary  in  a  hardware  accelera(cid:173)\\ntor.  we  have  therefore  developed  a  \"paging\"  architecture,  that  effectively  \"trades -\\noff\" some of this excessive speed against increased network size. \\na  \"moving  - patch\"  neural  board:  an  array  of  the  5  - state  synapses  is  currently \\nbeing  fabricated  as  a  vlsi  integrated  circuit.  the  shift  registers  and \\nthe \\nadderlsubtractor for  each  synapse  occupy a  disappointingly large silicon  area,  allow(cid:173)\\ning only a  3  x 9 synaptic  array.  to achieve  a  suitable size  neural  network  from  this \\narray,  several chips need to be  included on a  board with  memory and control circu(cid:173)\\nitry.  the  \"moving  patch\"  concept  is  shown  in  figure  7,  where  a  small  array  of \\nsynapses is passed over a much larger n  x n  synaptic array. \\neach  time  the  array  is  \"moved\"  to  represent  another set  of  synapses,  new  weights \\nmust be  loaded  into it.  for example,  the  first  set of weights will  be t 11  . ,  t;j  ... t 21 \\n...  t 2j  to tjj ,  the second  set  tj + 1,l  to t u  etc..  the final  weight  to be loaded will  be \\n\\n\\x0c581 \\n\\nn  neurons .. om synaptic array \\n\\nsmaller \"patch\" \\n\\nmoves over array \\n\\nrr~ _____ ) __ -.. \\n> \\n~\\'-\\n\\nfigure 7.  the  \"moving  patch\" concept,  passing  a  small synaptic \"patch\"  over  a larger \\nrun synapse array. \\n\\ntnt  static,  off - the  - shelf ram is  used  to store the weights and the  whole opera(cid:173)\\ntion  is  pipelined for  maximum efficiency.  figure 8 shows the board level design for \\nthe network. \\n\\nsynaptic  accelerator chips \\n\\ncontrol \\n\\nhost \\nfigure 8. a  \"moving  patch\" neural network board. \\n\\nthe small  \"patch\" that moves  around  the array  to  give  n  neurons comprises 4 vlsi \\nsynaptic accelerator chips to give  a 6 x 18 synaptic array. the number of neurons to \\nbe  simulated  is 256  and  the weights for  these  are stored  in 0.5  mb of ram  with a \\nload  time  of 8ms.  for  each  \"patch\"  movement,  the  partial  runnin~ summatinn \\n\\n;. \\n\\n\\x0c582 \\n\\ncalculated  for  each  column,  is  stored  in  a  separate  ram  until  it is  required  to  be \\nadded  into  the  next  appropriate  summation.  the  update  time  for  the  board  is  3ms \\ngiving  2  x  107  operations/second.  this  is  slower  than  the  64  neuron  specification, \\nbut  the  network  is  16  times  larger,  as  the  arithmetic  elements are  being  used  more \\nefficiently.  to  achieve  a  network  of  greater  than  256  neurons,  more  ram  is \\nrequired to store the weights.  the network is then slower unless a larger number of \\naccelerator chips is  used  to give  a larger moving \"patch\". \\n\\n6.  conclusions \\n\\na  strategy  and  design  method  has  been  given  for  the  construction  of  bit  - serial \\nvlsi neural network chips and  circuit  boards.  bit - serial  arithmetic,  coupled  to  a \\nreduced  arithmetic  style,  enhances  the  level  of  integration  possible  beyond  more \\nconventional digital,  bit - parallel schemes.  the restrictions imposed  on both synap(cid:173)\\ntic  weight  size  and  arithmetic  precision  by  vlsi  constraints  have  been  examined \\nand shown to be tolerable,  using the associative memory problem as a test. \\nwhile  we  believe  our  digital  approach  to  represent  a  good  compromise  between \\narithmetic  accuracy  and  circuit  complexity,  we  acknowledge  that  the  level  of \\nintegration  is  disappointingly  low. \\nit  is  our  belief  that,  while  digital  approaches \\nmay  be interesting and  useful  in the medium  term,  essentially as  hardware accelera(cid:173)\\ntors for  neural simulations,  analog techniques represent the best  ultimate option in 2 \\n- dimensional  silicon.  to this  end,  we  are currently pursuing techniques for  analog \\nin any  event,  the  full \\npseudo  - static  memory,  using  standard  cmos  technology. \\ndevelopment  of a  nonvolatile  analog  memory  technology,  such  as  the  mnos  tech(cid:173)\\nnique [7],  is key to the long - term  future of vlsi neural nets that can learn. \\n\\n7. acknowledgements \\n\\nthe  authors  acknowledge  the  support  of  the  science  and  engineering  research \\ncouncil (uk) in the execution of this work. \\n\\nreferences \\n\\n1. \\n\\ns.  grossberg,  \"some  physiological  and  biochemical  consequences  of psycho(cid:173)\\nlogical postulates,\" proc.  natl.  acad.  sci.  usa,  vol.  60,  pp.  758  - 765,  1968. \\n\\n2.  h.  p.  graf,  l.  d.  jackel,  r.  e.  howard,  b.  straughn,  j.  s.  denker,  w. \\nhubbard,  d.  m.  tennant,  and  d.  schwartz,  \"vlsi  implementation  of  a \\nneural  network  memory  with  several  hundreds  of  neurons,\"  proc.  alp \\nconference on neural networks for  computing.  snowbird,  pp.  182 - 187,  1986. \\n3.  w.  s.  mackie,  h.  p.  graf,  and  j.  s.  denker,  \"microelectronic  implementa(cid:173)\\n\\ntion  of  connectionist  neural  network  models,\"  ieee  conference  on  neural \\ninformation processing systems.  denver,  1987. \\nj . j. hopfield  and d.  w.  tank, \"neural\" computation of decisions in  optim(cid:173)\\nisation problems,\" bioi.  cybern.,  vol.  52,  pp.  141  - 152,  1985. \\n\\n4. \\n\\n5.  m.  a.  sivilotti,  m.  a.  mahowald,  and  c.  a.  mead, real - time  visual com(cid:173)\\n\\nputations using  analog cmos  processing arrays, 1987.  to be published \\n\\n6.  c.  a.  mead,  \"networks  for  real  - time  sensory  processing,\"  ieee  confer(cid:173)\\n\\nence  on  neural information  processing systems,  denver,  1987. \\n\\n\\x0c583 \\n\\n7. \\n\\n8. \\n\\nj.  p.  sage,  k.  thompson.  and  r. s.  withers,  \"an artificial neural  network \\nintegrated  circuit  based on mnoslccd  principles,\"  proc. alp conference on \\nneural networlcs for computing,  snowbird,  pp.  381  - 385,  1986. \\ns.  c.  j.  garth, \"a chipset for  high speed  simulation of neural network  sys(cid:173)\\ntems,\"  ieee conference on neural networlc.s,  san diego,  1987. \\n\\n9.  a.  f.  murray and  a.  v.  w.  smith,  \"a novel  computational  and  signalling \\nmethod  for  vlsi neural networks,\"  european  solid state circuits conference \\n, 1987. \\n\\n10.  a.  f.  murray  and  a.  j.  w.  smith,  \"asynchronous  arithmetic  for  vlsi \\n\\nneural systems,\"  electronics letters, vol.  23, no.  12, p.  642, june, 1987. \\n\\n11.  a.  f.  murray  and  a.  v.  w.  smith,  \"asynchronous  vlsi  neural  networks \\n\\nusing  pulse  stream  arithmetic,\"  ieee  journal  of solid-state  circuits  and sys(cid:173)\\ntems,  1988.  to be published \\n\\n12.  m.  e.  gaspar,  \"pulsed  neural  networks:  hardware,  software  and  the  hop(cid:173)\\nfield  aid  converter  example,\"  ieee  conference  on  neural  information  pro(cid:173)\\ncessing systems.  denver,  1987. \\n\\n13.  m.  s.  mcgregor,  p.  b.  denyer,  and a.  f.  murray,  \"a single - phase  clock(cid:173)\\ning scheme for  cmos  vlsi,\"  advanced research  in  vlsi  \" proceedings of the \\n1987 stanford conference,  1987. \\n\\n14.  d.  e.  rumelhart,  g.  e.  hinton,  and  r.  j.  williams,  \"learning  internal \\nrepresentations  by  error  propagation,\"  parallel  distributed  processing  \" \\nexplorations  in  the  microstructure of cognition,  vol.  1,  pp.  318 - 362,  1986. \\n\\n15.  m.  fleisher  and  e.  levin,  \"the  hopfiled  model  with  multilevel  neurons \\nmodels,\"  ieee  conference  on  neural  information  processing  systems.  denver, \\n1987. \\n\\n16.  g.  parisi,  \"a  memory  that  forgets,\"  j.  phys.  a  .\\'  math.  gen.,  vol.  19,  pp. \\n\\nl617  - l620,  1986. \\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dgG7dfCksQdS",
        "outputId": "80cf6b3a-62a3-4e04-db46-6116345cd1ce"
      },
      "source": [
        "i = 3\n",
        "papers_df['full_text'][i].lower().split('abstract')[0].replace('\\n','')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'442 alan  lapedes robert  farber theoretical division how  neural  nets  work los  alamos  national laboratory los  alamos,  nm  87545 '"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cE6uLVP5zHN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iYzYo_Fm27eI",
        "outputId": "87d51df4-7425-4f8f-82a9-1cd29aced3e7"
      },
      "source": [
        "papers_df['full_text'][i].lower().split('abstract')[0]"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'442 \\n\\nalan  lapedes \\nrobert  farber \\n\\ntheoretical division \\n\\nhow  neural  nets  work \\n\\nlos  alamos  national laboratory \\n\\nlos  alamos,  nm  87545 \\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "fM8dVlfa5StQ",
        "outputId": "d16dbf66-cd60-490b-ea5d-2eba3eca8023"
      },
      "source": [
        "papers_df['full_text'][i]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nTheoretical Division \\n\\nHow  Neural  Nets  Work \\n\\nLos  Alamos  National Laboratory \\n\\nLos  Alamos,  NM  87545 \\n\\nAbstract: \\n\\nThere is  presently great interest in the abilities of neural networks to mimic \\n\"qualitative reasoning\"  by manipulating neural incodings of symbols.  Less work \\nhas  been performed on using neural networks to process floating  point numbers \\nand it is  sometimes stated that neural networks are somehow inherently inaccu(cid:173)\\nrate  and  therefore  best  suited  for  \"fuzzy\"  qualitative reasoning.  Nevertheless, \\nthe  potential  speed  of massively  parallel  operations  make  neural  net  \"number \\ncrunching\"  an interesting topic  to explore.  In this paper we  discuss some of our \\nwork in which we  demonstrate that for  certain applications neural networks can \\nachieve  significantly  higher  numerical  accuracy  than  more  conventional  tech(cid:173)\\nniques.  In  particular,  prediction  of future  values  of a  chaotic  time  series  can \\nbe  performed  with  exceptionally  high  accuracy.  We  analyze  how  a  neural  net \\nis  able  to do  this  ,  and in  the process  show  that  a  large class  of functions  from \\nRn.  ~ Rffl  may  be  accurately  approximated  by  a  backpropagation  neural  net \\nwith just two  \"hidden\"  layers.  The network  uses  this functional  approximation \\nto perform either interpolation (signal processing applications)  or extrapolation \\n(symbol processing applicationsJ.  Neural nets therefore use quite familiar meth(cid:173)\\nods to perform. their tasks.  The geometrical viewpoint advocated here seems to \\nbe a  useful  approach  to analyzing  neural  network  operation  and  relates  neural \\nnetworks  to well  studied topics  in  functional  approximation. \\n1.  Introduction \\n\\nAlthough  a  great  deal  of interest  has  been  displayed  in  neural  network\\'s \\ncapabilities to perform a  kind of qualitative reasoning, relatively  little work  has \\nbeen  done  on  the  ability  of neural  networks  to  process  floating  point  numbers \\nIn  this \\nin  a  massively  parallel  fashion.  Clearly,  this  is  an  important  ability. \\npaper we  discuss some of our work  in  this  area and show  the  relation  between \\nnumerical,  and symbolic  processing.  We  will  concentrate on  the  the subject  of \\naccurate  prediction  in  a  time  series.  Accurate  prediction  has  applications  in \\nmany areas of signal processing.  It is  also a useful, and fascinating  ability, when \\ndealing with  natural,  physical systems.  Given  some .data from  the past  history \\nof a system, can one accurately predict what  it will  do in  the future? \\n\\nMany conventional signal processing tests, such as correlation function anal(cid:173)\\n\\nysis,  cannot  distinguish  deterministic  chaotic  behavior  from  from  stochastic \\nnoise.  Particularly difficult  systems to  predict are those that are  nonlinear  and \\nchaotic.  Chaos has a technical definition based on nonlinear, dynamical systems \\ntheory,  but  intuitivly means  that the system is  deterministic  but  \"random,\"  in \\na  rather  similar  manner  to  deterministic,  pseudo  random  number  generators \\nused on conventional computers.  Examples of chaotic systems in nature include \\nturbulence  in  fluids  (D.  Ruelle,  1971;  H.  Swinney,  1978), chemical  reactions  (K. \\nTomita,  1979),  lasers  (H.  Haken,  1975),  plasma  physics  (D.  Russel,  1980)  to \\nname  but  a  few.  Typically,  chaotic  systems  also  display  the  full  range  of  non(cid:173)\\nlinear behavior (fixed  points, limit cycles etc.)  when parameters are varied,  and \\ntherefore provide a  good  testbed in which to investigate techniques of nonlinear \\nsignal  processing.  Clearly,  if one can  uncover  the  underlying,  deterministic  al(cid:173)\\ngorithm from  a  chaotic  time  series,  then one  may  be  able  to  predict  the  future \\ntime series  quite  accurately, \\n\\n American Institute of Physics 1988 \\n\\n\\x0c443 \\n\\nIn  this  paper  we  review  and  extend  our  work  (Lapedes  and  Farber ,1987) \\non predicting the behavior of a  particular dynamical system, the  Glass-Mackey \\nequation.  We  feel  that  the  method  will  be  fairly  general,  and  use  the  Glass(cid:173)\\nMackey  equation  solely  for  illustrative  purposes.  The  Glass-Mackey  equation \\nhas  a  strange  attractor with fractal  dimension controlled by a  constant  param(cid:173)\\neter appearing in  the differential equation.  We  present results on a  neural net(cid:173)\\nwork\\'s  ability  to predict this system at  two values of this parameter, one value \\ncorresponding  to the  onset  of chaos,  and  the  other value  deeply  in  the chaotic \\nregime.  We  also present the results of more conventional predictive methods and \\nshow that a neural net is  able to achieve significantly better numerical accuracy. \\nThis  particular system was  chosen  because  of D.  Farmer\\'s and  J.  Sidorowich\\'s \\n(D.  Farmer,  J .  Sidorowich,  1987)  use  of it  in  developing  a  new,  non-neural  net \\nmethod for  predicting chaos.  The accuracy of this non-neural net method,  and \\nthe  neural net  method, are  roughly equivalent, with various  advantages or dis(cid:173)\\nadvantages  accruing  to  one  method  or  the  other  depending  on  one\\'s  point  of \\nview.  We  are happy to acknowledge many valuable discussions with Farmer and \\nSidorowich that has led  to further improvements in  each method. \\n\\nWe  also show that a  neural net never needs more than two hidden layers to \\nsolve most problems.  This statement arises from  a more general argument that \\na  neural  net  can  approximate  functions  from  Rn.  -+  Rm  with only  two  hidden \\nlayers, and that the  accuracy of the approximation is  controlled by the number \\nof neurons in each layer.  The argument assumes that the global minimum to the \\nbackpropagation  minimization  problem  may  be  found,  or  that  a  local  minima \\nvery  close  in  value  to  the  global  minimum  may  be  found.  This  seems  to  be \\nthe  case  in  the  examples  we  considered,  and in  many  examples  considered  by \\nother  researchers,  but  is  never  guaranteed.  The conclusion of an upper  bound \\nof two hidden layers is  related to a similar conclusion of R. Lipman  (R.  Lipman, \\n1987)  who has previously  analyzed the number of hidden layers  needed to form \\narbitrary decision  regions  for  symbolic  processing  problems.  Related  issues  are \\ndiscussed by  J. Denker (J.  Denker et.al.  1987)  It is  easy to extend the argument \\nto  draw  similar  conclusions  about  an  upper  bound  of  two  hidden  layers  for \\nsymbol  processing  and  to  place  signal  processing,  and  symbol  processing  in  a \\ncommon theoretical framework. \\n2.  Backpropagation \\n\\nBackpropagation  is  a  learning  algorithm for  neural  networks  that seeks  to \\nfind  weights,  T ij, such that given  an input  pattern from  a  training set of pairs \\nof Input/Output patterns, the network will  produce the  Output of the training \\nset  given  the  Input.  Having  learned  this  mapping  between  I  and  0  for  the \\ntraining  set,  one  then  applies  a  new,  previously  unseen  Input,  and  takes  the \\nOutput  as  the  \"conclusion\"  drawn  by  the  neural  net  based  on  having  learned \\nfundamental  relationships  between  Input  and  Output from  the training set.  A \\npopular configuration  for  backpropagation  is  a  totally  feedforward  net  (Figure \\n1)  where  Input feeds  up through  \"hidden layers\"  to an Output  layer. \\n\\n\\x0c444 \\n\\nOUTPUT \\n\\nFigure  1. \\n\\nA  feedforward  neural \\nnet.  Arrows  schemat(cid:173)\\nically  indicate  full \\nfeedforward  connect(cid:173)\\nivity \\n\\nEach  neuron  forms  a  weighted  sum of the  inputs  from  previous  layers  to \\nwhich  it is  connected, adds a threshold value,  and produces a nonlinear function \\nof this sum as  its output value.  This  output value serves  as  input to the future \\nlayers  to which the neuron is connected, and the process is  repeated.  Ultimately \\na  value  is  produced for  the outputs of the neurons  in  the  Output  layer.  Thus, \\neach neuron performs: \\n\\n(1) \\n\\nwhere Tii are continuous valued,  positive or negative weights,  9.  is  a  constant, \\nand g(x)  is  a  nonlinear function  that is  often chosen  to be of a  sigmoidal form. \\nFor example, one may choose \\n\\n1 \\n\\ng(z)  = 2\"  (1 + tanhz) \\n\\n(2) \\n\\nwhere tanh is  the hyperbolic tangent, although the exact formula of the sigmoid \\nis  irrelevant  to the results. \\nIf t!\")  are  the  target  output  values  for  the  pth  Input  pattern  then  ones  trains \\nthe network  by  minimizing \\n\\nE = L L (t~P) - o!P)) 2 \\n\\np \\n\\ni \\n\\n(3) \\n\\nwhere  t~p)  is  the  target  output  values  (taken  from  the  training  set)  and  O~pl \\nis  the  output  of the  network  when  the  pth  Input  pattern of  the  training  set  is \\npresented  on  the  Input  layer.  i  indexes  the  number  of  neurons  in  the  Output \\nlayer. \\n\\nAn  iterative procedure is  used to  minimize  S.  For example,  the commonly \\nused steepest descents procedure is  implemented by changing Tii and S,  by  AT\\'i \\nand  AS,  where \\n\\n\\x0c~T. .. =  - - \\'E  \\n\\n\\'1 \\n\\naE \\naT. .. \\n\\'1 \\n\\n445 \\n\\n(4a) \\n\\n( 4b) \\n\\nThis  implies  that  ~E < 0  and  hence  E  will  decrease  to a  local  minimum. \\nUse  o~ the chain .rule  and  definition  of some  intermediate quantities  allows  the \\nfollowmg  expressIons for  ~Tij to  be obtained  (Rumelhart,  1987): \\n\\n~Tij = L E6lp)o~.p) \\n\\np \\n\\nwhere \\n\\nif i  is  labeling  a  neuron  in  the  Output layer;  and \\n\\n6Jp)  =  O!p) (1  - o~p) LTi j 6;p) \\n\\nj \\n\\n(Sa) \\n\\n(Sb) \\n\\n(6) \\n\\n(7) \\n\\nif  i  labels  a  neuron  in  the  hidden  layers.  Therefore  one  computes  6Jp)  for  the \\nOutput  layer  first,  then  uses  Eqn.  (7)  to  computer 6i p )  for  the  hidden  layers, \\nand finally uses Eqn.  (S)  to make an adjustment to the weights.  We remark that \\nthe steepest descents procedure in common use  is  extremely slow  in simulation, \\nand that a better minimization procedure, such as the classic conjugate gradient \\nprocedure  (W.  Press,  1986),  can  offer  quite  significant  speedups.  Many  appli(cid:173)\\ncations  use  bit representations  (0,1)  for  symbols,  and attempt  to have  a  neural \\nnet  learn  fundamental  relationships  between  the symbols.  This  procedure  has \\nbeen successfully  used  in converting text  to speech  (T.  Sejnowski,  1986)  and  in \\ndetermining  whether  a  given  fragment  of  DNA  codes  for  a  protein  or  not  (A. \\nLapedes,  R.  Farber,  1987). \\n\\nThere is  no fundamental reason, however, to use integer\\'s as values for Input \\nand Output.  If the Inputs and Outputs are instead a collection of floating  point \\nnumbers,  then the network,  after  training, yields  a  specific  continuous function \\nin  n  variables  (for n  inputs)  involving g(x)  (Le.  hyperbolic tanh\\'s)  that provides \\na  type  of nonlinear,  least  mean  square  interpolant  formula  for  the  discrete  set \\nof  data  points  in  the  training  set.  Use  of  this  formula  a = 1(11, 1\", ... 1\\'1) \\nwhen  given  a  new  input  not  in  the  training  set,  is  then  either  interpolation  or \\nextrapolation. \\n\\nSince  the  Output values, when  assumed  to be  floating  point  numbers  may \\nhave  a  dynamic  range great  than 10,1\\\\,  one  may  modify  the g(x)  on the Output \\nlayer  to be  a  linear function,  instead of sigmoidal, so  as  to encompass the larger \\ndynamic range.  Dynamic range of the Input values  is  not so critical, however we \\nhave  found  that  numerical  problems  may  be avoided  by  scaling  the  Inputs  (and \\n\\n\\x0c446 \\n\\nalso  the  Outputs)  to  [0,1],  training the network,  and then  rescaling  the Ti;,  (J, \\nto  encompass  the  original  dynamic  range.  The  point  is  that  scale  changes  in \\nI  and  0  may,  for  feedforward  networks,  always  be  absorbed  in  the  T ijJ  (J,  and \\nvice  versa.  We  use  this  procedure  (backpropagation, conjugate gradient,  linear \\noutputs and scaling)  in  the following section to predict points in  a chaotic  time \\nseries. \\n3.  Prediction \\n\\nLet us consider situations in  Nature where a system is  described by  nonlin(cid:173)\\n\\near differential equations.  This is faily generic.  We choose a particular nonlinear \\nequation  that  has  an  infinite  dimensional  phase space,  so  that  it  is  similar  to \\nother infinite dimensional systems such as partial differential equations.  A differ(cid:173)\\nential equation with an infinite dimensional phase space (i.e.  an infinite number \\nof values  are  necessary  to  describe  the  initial  condition)  is  a  delay,  differential \\nequation.  We choose to consider the time series generated by  the  Glass-Mackey \\nequation: \\n\\nX= \\n\\naz(t - 1\\') \\n\\n1 + Z 10 (t  _ 1\\')  -\\n\\nb  t \\nZ (  ) \\n\\n(8) \\n\\nThis is  a  nonlinear differential, delay equation with an initial condition specified \\nby  an  initial  function  defined  over  a  strip  of  width  l\\'  (hence  the  infinite  di(cid:173)\\nmensional phase space i.e.  initial functions,  not  initial constants  are  required). \\nChoosing this function to be a constant function, and a = .2, b = .1, and l\\' =  17 \\nyields a time series, x(t), (obtained by integrating Eqn.  (8)), that is chaotic with \\na fractal attractor of dimension 2.1.  Increasing  l\\' to 30 yields  more complicated \\nevolution and a fractal  dimension of 3.5.  The time series for  500 time steps for \\n1\\'=30  (time  in  units of 1\\')  is  plotted in Figure 2.  The nonlinear evolution of the \\nsystem collapses  the  infinite  dimensional  phase space  down  to  a  low  (approxi(cid:173)\\nmately  2 or  3  dimensional)  fractal,  attracting set.  Similar  chaotic  systems  are \\nnot  uncommon in  Nature. \\n\\nFigure  2.  Example  time  series  at  tau  ~  30. \\n\\n\\x0c447 \\n\\nThe  goal  is  to  take  a  set  of values  of  xO  at  discrete  times  in  some  time \\nwindow  containing  times  less  than  t,  and  use  the values  to  accurately  predict \\nx(t  + P), where  P  is  some  prediction  time  step  into  the  future.  One  may  fix \\nP,  collect  statistics  on  accuracy  for  many  prediction  times  t  (by  sliding  the \\nwindow  along  the time series),  and  then  increase  P  and again  collect  statistics \\non accuracy.  This one may observe how an average index of accuracy changes as \\nP  is  increased.  In terms of Figure 2 we  will select various  prediction time steps, \\nP,  that  correspond  to  attempting  to  predict  within  a  \"bump,\"  to  predicting \\na  couple  of  \"bumps\"  ahead.  The  fundamental  nature  of  chaos  dictates  that \\nprediction  accuracy  will  decrease  as  P  is  increased.  This  is  due  to  inescapable \\ninaccuracies of finite  precision in specifying the x( t)  at discrete times in the past \\nthat are used for predicting the future.  Thus, all predictive methods will degrade \\nas  P  is  increased  - the  question  is  \"How  rapidly  does  the error  increase  with \\nP?\"  We will demonstrate that the neural net method can be orders of magnitude \\nmore  accurate than conventional methods at large prediction time steps, P. \\n\\nOur goal is to use backpropagation, and a neural net, to construct a function \\n\\nO(t + P)  =  f  (11(t), 12(t - A) ... lm(t - mA)) \\n\\n(9) \\nwhere O(t + P) is  the output of a single neuron in the Output layer, and 11  ~ 1m \\nare  input  neurons  that  take  on values  z(t), z(t - A)  ... z(t -\\nrnA),  where  A  is \\na  time  delay.  O(t  + P)  takes  on  the  value  x(t  + P).  We  chose  the  network \\nconfiguation of Figure 1. \\n\\nWe  construct a  training set by selecting a set of input values: \\n\\n(10) \\n\\n1m  =  x(t p  -\\n\\nrnA) \\n\\nwith  associated output values  0  =  x(tp  + P), for  a  collection of discrete  times \\nthat  are  labelled  by  tp.  Typically  we  used  500  I/O  pairs  in  the  training  set \\nso  that  p  ranged  from  1~ 500.  Thus  we  have  a  collection  of  500  sets  of \\n{lip), l~p), ... , 1::); O(p)}  to  use  in  training  the  neural  net.  This  procedure  of \\nusing  delayed  sampled  values  of x{t)  can  be  implemented by  using  tapped  de(cid:173)\\nlay  lines,  just  as  is  normally  done  in  linear  signal  processing  applications,  (B. \\nWidrow,  1985).  Our prediction procedure is  a straightforward  nonlinear exten(cid:173)\\nsion of the linear Widrow Hoff algorithm.  After training is completed, prediction \\nis  performed  on a  new  set of times, t p,  not  in the training set  i.e.  for  p  = 500. \\nWe  have not yet specified what m or A  should be, nor given any indication \\nwhy a formula like Eqn.  (9)  should work  at all.  An important theorem of Takens \\n(Takens, 1981)  states that for  flows  evolving to compact attracting manifolds of \\ndimension  d.A\" \\nthat  a  functional  relation  like  Eqn.  (9)  does  exist,  and  that  m \\nlies  in the range d.A,  < m  + 1 < 2d.A,  + 1.  We  therefore choose m = 4, for  T  = 30. \\nTakens  provides  no  information  on  A  and  we  chose  A  = 6  for  both cases.  We \\nfound that a few  different choices of m and A can affect accuracy by a factor of 2 -\\na  somewhat significant but not overwhelming sensitivity, in view of the fact  that \\nneural  nets  tend  to  be  orders  of magnitude  more  accurate than other methods. \\nTakens  theorem gives  no information on  the form of fO  in Eqn.  (9).  It therefore \\n\\n\\x0c448 \\n\\nis  necessary  to show that neural nets provide a  robust approximating procedure \\nfor  continuous fO,  which we  do  in the following  section.  It is  interesting to note \\nthat attempts to predict future values of a  time series  using  past  values of x(t) \\nfrom  a  tapped  delay  line  is  a  common  procedUre  in signal  processing,  and yet \\nthere is  little,  if any,  reference to results of nonlinear dynamical systems theory \\nshowing why  any such attempt is  reasonable. \\n\\nAfter trainin, the neural net as  described above,  we  used  it  to predict  500 \\nnew  values  of x(tJ  in  the future  and computed  the  average  accuracy for  these \\npoints.  The accuracy is defined to be the average root mean square error, divided \\nby  a  constant  scale  factor,  which  we  took  to  be  the  standard  deviation  of the \\ndata.  It is  necessary to remove the scale dependence of the data and dividing by \\nthe standard deviation  of the data provides  a  scale  to  use.  Thus  the  resulting \\n\"index of accuracy\"  is  insensitive to the dynamic  range of x( t). \\n\\nAs just described, if one wanted to use a neural net to continuously predict \\nx(t)  values  at,  say,  6  time  steps  past  the  last  observed  value  (i.e.  wanted  to \\nconstruct  a  net  predicting  x( t  + 6))  then  one  would  train  one  network,  at  P \\n=  6,  to  do  this.  If one  wanted  to  always  predict  12  time  steps  past  the  last \\nobserved  x( t)  then  a  separate,  P  = 12,  net  would  have  to  be  trained.  We,  in \\nfact,  trained separate networks  for  P  ranging  between 6  and 100  in steps  of 6. \\nThe index of accuracy for  these  networks  (as  obtained by  computing the index \\nof accuracy  in  the prediction phase)  is  plotted  as  curve  D  in  Figure  3.  There \\nis  however  an alternate way  to predict.  If one wished  to predict, say, x(t + 12) \\nusing  a  P  =  6  net,  then  one can  iterate  the  P  =  6  net.  That  is,  one  uses  the \\nP  = 6  net  to  predict  the x(t  +6)  values,  and  then feeds  x(t  +6)  back  into  the \\ninput  line  to  predict  x(t  + 12)  using  the  predicted x(t  + 6)  value  instead of \\nthe observed x(t +  6)  value.  in fact,  one can\\'t use the observed x(t  +6)  value, \\nbecause  it  hasn\\'t  been  observed yet  - the  rule  of the  game  is  to  use  only  data \\noccurring at time t  and before, to predict x( t  + 12).  This procedure corresponds \\nto iterating the map given  by  Eqn.  (9)  to perform prediction at  multiples  of P. \\nOf course, the delays,  ~, must  be chosen commensurate with  P. \\nThis iterative method of prediction has potential dangers.  Because  (in  our \\nexample  of  iterating  the  P  =  6  map)  the  predicted  x(t  + 6)  is  always  made \\nwith some error, then this error is  compounded in iteration, because predicted, \\nand  not  observed  values,  are  used  on  the  input  lines.  However,  one  may  pre(cid:173)\\ndict  more  accurately  for  smaller  P,  so  it  may  be  the case  that choosing  a  very \\naccurate  small  P  prediction,  and iterating, can ultimately  achieve  higher  accu(cid:173)\\nracy  at  the  larger  P\\'s  of interest.  This  tUrns  out  to  be  true,  and  the  iterated \\nnet  method  is  plotted  as  curve  E  in  Figure 3.  It  is  the best  procedure  to  use. \\nCurves  A,B,C  are  alternative  methods  (iterated  polynomial, Widrow-Hoff,  and \\nnon-iterated  polynomial  respectively.  More  information on  these  conventional \\nmethods  is  in  (Lapedes and Farber,  1987)  ). \\n\\n\\x0cB \\n\\nD \\n\\n449 \\n\\nE \\n\\n~ , : \\n\\nA  C \\n1 \\nI, \\n/\\' \\n!I: \\n\"  :J \\nI \\n\\\\f  I \\n/ \\n.  , \\n: .\\' \\n/ \\nI \\n/ \\nI .. ,,: \\n\\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\n\\n, \\n\\nI \\n\\n, \\nI , \\n\\nI \\n\\nP1-~~ictlon ~~. P  (T.U3~ 30) \\n\\n400 \\n\\nFigure  3. \\n\\n1 \\n\\n.8 \\n\\n.6 \\n\\n~ \\n~ \\n~ \\n\\n-\\n= \\n\\n.4 \\n\\n.2 \\n\\no \\n\\no \\n\\n4.  Why It Works \\n\\nConsider  writing  out  explicitly  Eqn.  (9)  for  a  two  hidden  layer  network \\nwhere  the output is  assumed to be a  linear neuron.  We  consider Input connects \\nto  Hidden  Layer  1,  Hidden  Layer  1 to  Hidden  Layer  2,  and Hidden  Layer  2  to \\nOutput,  Therefore: \\n\\nRecall  that  the output neurons a  linear computing element so that only two gOs \\noccur  in  formula  (11),  due  to  the two  nonlinear hidden  layers.  For ease in  later \\nanalysis,  let  us  rewrite  this  formula  as \\n\\nOt  =  L TtJcg (SU Mle  + Ole)  + Ot \\n\\nIe tH 2 \\n\\nwhere \\n\\n(12a) \\n\\n(12b) \\n\\n\\x0c450 \\n\\nThe  T\\'s  and  (Ps  are  specific  numbers  specified  by  the  training  algorithm, \\nso  that  after training is  finished  one  has  a  relatively  complicated formula  (12a, \\n12b)  that expresses the Output value as  a specific, known, function of the Input \\nvalues: \\n\\nOt  ==  1(117 12,\"  .lm). \\n\\nA  functional  relation  of this form,  when  there  is  only  one output, may  be \\nviewed  as  surface  in  m  +  1  dimensional  space,  in  exactly  the  same  manner \\none  interprets  the  formula  z  ==  f(x,y)  as  a  two  dimensional  surface  in  three \\n\\' dimensional  space.  The  general  structure of fO  as  determined  by  Eqn.  (12a, \\n12b)  is  in fact  quite simple.  From Eqn.  (12b)  we see that one first  forms  a  sum \\nof gO  functions  (where  gO  is  s  sigmoidal  function)  and then  from  Eqn.  (12a) \\none  (orms  yet  another sum involving  gO  functions.  It  may  at  first  be  thought \\nthat  this  special,  simple  form  of fO  restricts  the  type  of surface  that  may  be \\nrepresented  by  Ot  =  f(Ii)\\'  This  initial  tl.ought  is  wrong - the special  form  of \\nEqn.  (12)  is  actually a  general representation for  quite arbitrary surfaces. \\n\\nTo  prove  that  Eqn. \\n\\n(12)  is  a  reasonable  representation  for  surfaces  we \\nfirst  point  out  that  surfaces  may  be  approximated  by  adding  up  a  series  of \\n\"bumps\"  that  are  appropriately  placed.  An  example of this  occurs  in  familiar \\nFourier  analysis,  where  wave  trains  of  suitable  frequency  and  amplitude  are \\nadded  together  to  approximate  curves  (or  surfaces).  Each  half period  of each \\nwave  of fixed  wavelength  is  a  \"bump,\"  and one adds  all  the bumps  together  to \\nform  the  approximant.  Let  us  noW  see  how  Eqn.  (12)  may  be  interpreted  as \\nadding  together bumps of specified heights  and positions.  First consider SUMk \\nwhich  is  a sum of g(  )  functions.  In  Figure (4)  we  plot  an example of such a gO \\nfunction  for  the case  of two  inputs. \\n\\nFigure  4.  A  sigmoidal  surface. \\n\\n\\x0c451 \\n\\nThe orientation of this sigmoidal surface  is  determined  by  T sit  the  position  by \\n8;\\'1  and height  by T\"\\'i.  Now  consider another gO  function that occurs in SUM\",. \\nThe 8;,  of the second gO  function  is  chosen  to  displace  it from  the first,  the Tii \\nis  chosen so  that  it  has  the  same  orientation  as  the  first,  and  T \"\\'i  is  chosen  to \\nhave  opposite  sign  to  the  first.  These  two  g(  )  functions  occur  in  SUM\"\"  and \\nso to determine  their contribution to SUM\",  we  sum them together and plot  the \\nresult  in Fi  ure  5.  The result  is  a  ridged surface. \\n\\nFigure  5.  A  ridge. \\n\\nSince our goal is to obtain localized bumps we select another pair of gO  functions \\nin  SUMk,  add  them together  to  get  a  ridged  surface  perpendicular to the  first \\nridged surface,  and then add the  two  perpendicular ridged surfaces  together to \\nsee the contribution to SUMk.  The result  is  plotted in  Figure  (6). \\n\\nFigure  6.  A  pseudo-bump . \\n\\n\\x0c452 \\n\\nWe  see  that  this  almost worked,  in  so  much  as  one  obtains  a  local  maxima by \\nthis procedure.  However  there are also  saddle-like configurations at the corners \\nwhich  corrupt  the  bump  we  were  trying  to  obtain.  Note  that  one  way  to  fix \\nthis  is  to  take  g(SUMk + Ok)  which  will,  if Ole  is  chosen  appropriately,  depress \\nthe  local  minima and saddles  to  zero  while  simultaneously  sending  the central \\nmaximum towards  1.  The result  is  plotted in  Figure  (7)  and is  the sought after \\nb~~ ____________________________________________ ___ \\n\\nFigure  7.  A  bump. \\n\\nFurthermore, note that the necessary gO  function  is  supplied by Eqn.  (12). \\nTherefore Eqn.  (12)  is a procedure to obtain localized bumps of arbitrary height \\nand position.  For two  inputs,  the kth  bump is  obtained by  using four  gO  func(cid:173)\\ntions  from  SUMk  (two  gO  functions  for  each  ridged  surface  and  two  ridged \\nsurfaces  per bump)  and then taking gO  of the result  in Eqn.  (12a).  The height \\nof the kth  bump is  determined by T tJe  in  Eqn.  (12a)  and the k bumps are  added \\ntogether  by  that equation as  well.  The general network  architecture which  cor(cid:173)\\nresponds to the above  procedure of adding two  gO  functions  together to form  a \\nridge,  two  perpendicular ridges  together  to form  a  pseudo-bump,  and  the  final \\ngO  to form  the  final  bump is  represented  in  Figure  (8).  To  obtain any  number \\not bumps  one  adds more  neurons  to  the  hidden  layers  by  repeatedly  using  the \\nconnectivity of Figure  (8)  as  a  template  (Le.  four  neurons  per bump in  Hidden \\nLayer  1, and one neuron per bump  in  HiClden  Layer  2). \\n\\n\\x0c453 \\n\\nFigure  8.  Connectivity  needed \\nto  obtain  one  bump.  Add  four \\nmore  neurons  to  Hidden  layer \\n1,  and  one  more  neuron  to \\nHidden  Layer  2,  for  each \\nadditional  bump. \\n\\nOne  never  needs  more  than  two  layers,  or  any  other  type  of connectivity \\nthan  that  already  schematically  specified  by  Figure  (8).  The  accuracy  of the \\napproximation  depends  on  the  number  of  bumps,  whIch  in  turn  is  specified, \\nby  the number  of neurons  per layer.  This result  is  easily  generalized  to higher \\ndimensions  (more  than  two  Inputs)  where  one  needs  2m  hiddens  in  the  first \\nhidden layer,  and one hidden neuron in  the second layer for  each bump. \\n\\nThe  argument  given  above also  extends to the situation where  one  is  pro-(cid:173)\\n\\ncessing  symbolic  information  with  a  neural  net.  In  this  situation,  the  Input \\ninformation is  coded into bits  (say Os  and  Is)  and similarly for  the Output.  Or, \\nthe  Inputs  may  still  be real  valued  numbers,  in  which  case  the  binary  output \\nis  attempting  to  group  the  real  valued  Inputs  into  separate  classes.  To  make \\nthe  Output values  tend  toward 0  and  lone takes  a  third and  final  gO  on  the \\noutput  layer,  i.e.  each output neuron is  represented  by g(Ot)  where  Ot  is  given \\nin  Eqn.  (11) .  Recall  that  up  until  now  we  have  used  hnear  neurons  on  the \\noutput layer.  In typical backpropagation examples, one  never actually  achieves \\na  hard 0 or 1 on the output layers  but achieves  instead some value between 0.0 \\nand 1.0.  Then typically  any value over 0.5  is  called 1,  and values  under 0.5  are \\ncalled O.  This  \"postprocessing\"  step  is  not really  outside the framework  of the \\nnetwork formalism,  because it may be performed by merely increasing the slope \\nof the sigmoidal function  on the Output layer.  Therefore the only effect  of the \\nthird  and  final  gO  function  used on the  Output  layer  in  symbolic  information \\nprocessing  is  to  pass  a  hyperplane  through  the surface  we  have  just  been  dis(cid:173)\\ncussing.  This  plane cuts  the surface,  forming  \"decision  regions,\"  in which  high \\nvalues are called 1 and low values are called O.  Thus we see that the heart of the \\nproblem  is  to  be  able  to  form  surfaces  in  a  general  manner,  which  is  then  cut \\nby a hyperplane into general decision regions.  We  are therefore able to conclude \\nthat the network architecture consisting of just two hidden layers is sufficient for \\nlearning any symbol processing training set.  For Boolean symbol mappings one \\nneed  not  use  the second  hidden  layer  to  remove  the saddles  on  the  bump  (c.f. \\nFig.  6).  The saddles  are  lower  than  the  central  maximum so  one  may  choose \\na  threshold  on  the output layer  to cut  the  bump at a  point over  the saddles  to \\nyield  the  correct  decision  region.  Whether  this  representation  is  a  reasonable \\none for  subsequently  achieving  good prediction on a  prediction set,  as  opposed \\nto  \"memorizing\"  a  training set, is  an  issue  that we  address below. \\n\\n\\x0c454 \\n\\nWe also note that use of Sigma IIi  units (Rummelhart, 1986)  or high order \\ncorrelation  nets  (Y.-C.  Lee,  1987)  is  an  attempt  to  construct  a  surface  by  a \\ngeneral  polynomial  expansion,  which  is  then cut by  a  hyperplane into decision \\nregions,  as  in  the above.  Therefore  the essential  element of all  these  neural net \\nlearning  algorithms are  identical  (Le.  surface construction), only  the particular \\nmethod of parameterizing the surface varies from one algorithm to another.  This \\ngeometrical viewpoint, which provides a unifying framework for many neural net \\nalgorithms,  may  provide  a  useful  framework  in  which  to  attempt  construction \\nof new algorithms. \\n\\nAdding together bumps to approximate surfaces  is  a  reasonable  procedure \\nto use  when  dealing with real valued inputs.  It ties in to general approximation \\ntheory  (c.f.  Fourier series,  or better yet,  B splines), and can be quite successful \\nas we have seen.  Clearly some economy is gained by giving the neural net bumps \\nto start with, instead of having the neural net form its own bumps from sigmoids. \\nOne  way  to do  this would  be  to use  multidimensional  Gaussian functions  with \\nadjustable parameters. \\n\\nThe situation is  somewhat different  when  processing symbolic  (binary val(cid:173)\\n\\nued)  data.  When input symbols are encoded into N bit bit-strings then one has \\nwell defined input values  in an N dimensional input space.  As shown above, one \\ncan learn the training set of input patterns by appropriately forming and placing \\nbump surfaces over this space.  This is  an effective method for memorizing the \\ntraining set,  but a  very  poor method for  obtaining correct  predictions  on  new \\ninput data.  The point is  that, in contrast to real valued inputs that come from, \\nsay,  a  chaotic  time series, the input points  in symbolic processing problems are \\nwidely separated and  the  bumps do  not add together to form smooth surfaces. \\nFurthermore, each  input  bit string  is  a  corner of an  2N  vertex  hypercube,  and \\nthere is  no sense in which one corner of a hypercube is  surrounded by the other \\ncorners.  Thus the commonly used  input representation for  symbolic processing \\nproblems  requires  that  the neural  net  extrapolate the surface  to make  a  new \\nprediction for  a  new  input  pattern  (i.e.  new  corner of the hypercube)  and not \\ninterpolate, as  is  commonly  the case for  real valued  inputs.  Extrapolation  is \\na farmore dangerous procedure than interpolation, and in view of the separated \\nbumps of the  training set one might  expect on the basis of this  argument  that \\nneural nets would fail  dismally  at symbol processing.  This  is  not the case. \\n\\nThe solution  to  this  apparent conundrum, of course,  is  that although  it  is \\nsufficient  for  a  neural net  to  learn a  symbol  processing  training set  by  forming \\nbumps it is  not necessary for  it to operate in  this manner.  The simplest exam(cid:173)\\nple  of this  occurs  in  the  XOR  problem.  One  can  implement  the  input/output \\nmapping for  this problem by  duplicating the hidden layer architecture of Figure \\n(8)  appropiately for  two  bumps ( i.e.  8 hid dens  in  layer 1, 2 hid dens in layer 2). \\nAs  discussed  above,  for  Boolean  mappings, one can even  eliminate  the  second \\nhidden layer.  However  the architecture of Figure  (9)  will  also suffice. \\n\\nFigure  9.  Connectivity  for  XOR \\n\\nOUTPUT \\n\\nHIDDEN \\n\\nINPUT \\n\\n\\x0c455 \\n\\nPlotting the output of this network,  Figure(9),  as  a  function  of the  two  inputs \\nyields  a  ridge  orientated  to  run  between  (0,1)  and  (1,0)  Figure(lO).  Thus  a \\nneural  net  may  learn a  symbolic  training set  without  using  bumps,  and a  high \\ndimensional  version  of this  process  takes  place  in  more  complex  symbol  pro(cid:173)\\ncessing tasks.Ridge/ravine representations of the training data are considerably \\nmore  efficient  than bumps  (less  hidden neurons and weights)  and the extended \\nnature of the surface allows  reasonable  predictions i.e.  extrapolations. \\n\\nFigure  10 \\n\\nXOR  surface \\n\\n(1, 1) \\n\\n5.  Conclusion. \\n\\nNeural  nets,  in  contrast  to  popular  misconception,  are  capable  of  quite \\naccurate  number  crunching,  with  an  accuracy  for  the  prediction  problem  we \\nconsidered  that exceeds  conventional  methods  by  orders of magnitude.  Neural \\nnets work  by constructing surfaces  in a  high dimensional space, and their oper(cid:173)\\nation  when  performing signal  processing  tasks  on real  valued  inputs,  is  closely \\nrelated  to  standard methods  of functional  ,,-pproximation.  One does  not  need \\nmore  than  two  hidden  layers for  processing real valued  input data,  and  the ac(cid:173)\\ncuracy  of  the  approximation  is  controlled  by  the number  of neurons  per  layer, \\nand not the number of layers.  We emphasize that although two layers of hidden \\nneurons are sufficient they may not be efficient.  Multilayer architectures may \\nprovide very efficient  networks  (in the sense of number of neurons and number \\nof weights)  that can perform accurately  and with minimal cost. \\n\\nEffective  prediction for symbolic  input data is  achieved  by  a slightly differ(cid:173)\\n\\nent  method  than that  used  for  real  value  inputs.  Instead of forming  localized \\nbumps  (which  would  accurately  represent  the training data but would  not pre(cid:173)\\ndict  well  on  new  inputs)  the  network  can  use  ridge/ravine  like  surfaces  (and \\ngeneralizations  thereof)  to efficiently  represent  the scattered input data.  While \\nneural  nets  generally  perform  prediction  by  interpolation for  real  valued  data, \\nthey  must  perform extrapolation for  symbolic  data if the usual  bit  representa(cid:173)\\ntions  are used.  An outstanding problem is  why  do tanh representations seem to \\nextrapolate well  in symbol processing problema?  How  do other functional bases \\ndo?  How  does the representation for symbolic inputs affect the ability to extra~ \\nolate?  This  geometrical  viewpoint  provides  a  unifyimt  framework  for  examimr: \\n\\n\\x0c456 \\n\\nmany neural net algorithms, for suggesting questions about neural net operation, \\nand for  relating current neural net approaches to conventional methods. \\nAcknowledgment. \\n\\nWe  thank  Y.  C.  Lee,  J.  D.  Farmer,  and  J.  Sidorovich  for  a  number  of \\n\\nvaluable discussions. \\n\\nReferences \\n\\nC.  Barnes,  C.  Burks, R.  Farber, A.  Lapedes,  K.  Sirotkin, \"Pattern Recognition \\nby  Neural Nets  in  Genetic  Databases\", manuscript in  preparation \\n\\nJ.  Denker  et.  al.,\" Automatic  Learning,  Rule  Extraction,and  Generalization\", \\nATT, Bell  Laboratories preprint, 1987 \\n\\nD.  Farmer, J.Sidorowich,  Phys.Rev.  Lett., 59(8),  p.  845,1987 \\n\\nH.  Haken,  Phys.  Lett.  A53,  p77  (1975) \\n\\nA.  Lapedes,  R.  Farber  \"Nonlinear  Signal  Processing  Using  Neural  Networks: \\nPrediction and System Modelling\", LA-UR87-2662,1987 \\n\\nY.C.  Lee, Physica 22D,(1986) \\n\\nR.  Lippman, IEEE ASAP  magazine,p.4,  1987 \\n\\nD.  Ruelle,  F. Takens,  Comm.  Math.  Phys.  20,  p167  (1971) \\n\\nD.  Rummelhart,  J.  McClelland  in  \"Parallel  Distributed  Processing\"  Vol.  1, \\nM.I.T.  Press Cambridge, MA  (1986) \\n\\nD.  Russel  et al.,  Phys.  Rev.  Lett.  45,  pU75  (1980) \\n\\nT. Sejnowski et al., \"Net Talk:  A Parallel Network that Learns to Read Aloud,\" \\nJohns  Hopkins  Univ.  preprint (1986) \\n\\nH.  Swinney et al.,  Physics  Today 31  (8),  p41  (1978) \\n\\nF. Takens, \"Detecting Strange Attractor in Turbulence,\"  Lecture Notes in Math(cid:173)\\nematics,  D.  Rand,  L.  Young  (editors), Springer  Berlin,  p366  (1981) \\n\\nK.  Tomita et aI.,  J. Stat.  Phys.  21,  p65  (1979) \\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZjze_JS5ZHf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}