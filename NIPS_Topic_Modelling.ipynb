{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NIPS_Topic_Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlFU9N5YWS0ivlR7Gvk6ga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meiruv/NIPS-Papers-Analysis/blob/main/NIPS_Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4anFrszWawnR"
      },
      "source": [
        "# Fetching the Data From Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-tnninDaMMU",
        "outputId": "9cf1ece5-2f9c-4247-fcce-15d908f097b8"
      },
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! kaggle datasets download -d rowhitswami/nips-papers-1987-2019-updated"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading nips-papers-1987-2019-updated.zip to /content\n",
            " 84% 89.0M/106M [00:01<00:00, 61.3MB/s]\n",
            "100% 106M/106M [00:01<00:00, 66.8MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNz1BwpVawEo"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = 'nips-papers-1987-2019-updated.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('nips_papers')\n",
        "zip_ref.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGALC_9a6G5",
        "outputId": "1769b82b-b15a-458b-aa3a-acac01ec23e5"
      },
      "source": [
        "!ls nips_papers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "authors.csv  papers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9olgz90Fa7Uw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "8d1ef14f-67f6-41ec-9e66-ce75e7036cc0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "papers_df = pd.read_csv('nips_papers/papers.csv')\n",
        "papers_df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27</td>\n",
              "      <td>1987</td>\n",
              "      <td>Bit-Serial Neural Networks</td>\n",
              "      <td>NaN</td>\n",
              "      <td>573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63</td>\n",
              "      <td>1987</td>\n",
              "      <td>Connectivity Versus Entropy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>1987</td>\n",
              "      <td>The Hopfield Model with Multi-Level Neurons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59</td>\n",
              "      <td>1987</td>\n",
              "      <td>How Neural Nets Work</td>\n",
              "      <td>NaN</td>\n",
              "      <td>442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>1987</td>\n",
              "      <td>Spatial Organization of Neural Networks: A Pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   source_id  year  ... abstract                                          full_text\n",
              "0         27  1987  ...      NaN  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...\n",
              "1         63  1987  ...      NaN  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...\n",
              "2         60  1987  ...      NaN  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...\n",
              "3         59  1987  ...      NaN  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...\n",
              "4         69  1987  ...      NaN  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dXiiwK1zqT1-",
        "outputId": "b9f86b11-5e65-4283-d426-090d3e251d44"
      },
      "source": [
        "papers_df.loc[0,'full_text']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan F.  Murray,  Anthony V . W.  Smith  and Zoe F.  Butler. \\n\\nDepartment of Electrical Engineering,  University of Edinburgh, \\n\\nThe King\\'s Buildings, Mayfield Road,  Edinburgh, \\n\\nScotland,  EH93JL. \\n\\nABSTRACT \\n\\nA  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a \\nsynapse array through to silicon layout and board design.  The issues surrounding bit \\n- serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel \\ndevelopment  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and \\nrecall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected \\nspecification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173)\\nnique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms, \\nusing  a  \"paging\"  technique  to  time  - multiplex  calculations  through  the  synapse \\narray. \\n\\n1. INTRODUCTION \\n\\nThe functions a  synthetic neural network may aspire to mimic are the ability to con(cid:173)\\nsider  many  solutions  simultaneously,  an  ability  to  work  with  corrupted  data  and  a \\nnatural  fault  tolerance.  This  arises  from  the  parallelism  and  distributed  knowledge \\nrepresentation  which  gives  rise  to  gentle  degradation  as  faults  appear.  These func(cid:173)\\ntions  are  attractive  to implementation  in VLSI  and  WSI.  For example,  the natural \\nfault  - tolerance  could  be  useful  in  silicon  wafers  with  imperfect  yield,  where  the \\nnetwork  degradation  is  approximately  proportional  to  the  non-functioning  silicon \\narea. \\nTo cast  neural networks in engineering language,  a  neuron is a  state machine that is \\neither  \"on\"  or  \"off\\',  which  in  general  assumes  intermediate  states  as  it  switches \\nsmoothly  between  these  extrema.  The  synapses  weighting  the  signals  from  a \\ntransmitting neuron  such that it is more or less excitatory or inhibitory to the receiv(cid:173)\\ning  neuron.  The  set  of synaptic weights  determines  the stable  states and  represents \\nthe learned  information in a system. \\nThe  neural  state,  VI\\'  is  related  to  the  total  neural  activity  stimulated  by  inputs  to \\nthe  neuron  through  an  activation junction,  F.  Neural  activity  is  the  level  of excita(cid:173)\\ntion  of the  neuron  and the  activation  is  the way  it  reacts  in a  response to a  change \\nin activation. The neural output state at time t, V[,  is related to x[ by \\n\\nV[  = F (xf) \\n\\n(1) \\n\\nThe  activation  function  is  a  \"squashing\"  function  ensuring  that  (say)  Vi  is  1  when \\nXi  is large  and  -1  when Xi  is  small.  The neural update function  is therefore straight(cid:173)\\nforward: \\n\\n. \\n\\n,+1  - ,   + ~  ~ T  V\\' \\nJ \\nXI \\n\\ni-n-l \\n0  ~  ii \\n\\n- XI \\n\\n• •••• \\n\\nJ-O \\n\\n(2) \\n\\nwhere  8  represents  the  rate  of change  of neural  activity,  Tij \\nand n  is  the number of terms giving an n  - neuron array [1]. \\nAlthough  the  neural function  is  simple  enough,  in  a  totally  interconnected  n  - neu(cid:173)\\nron  network  there  are n 2  synapses requiring n 2  multiplications  and  summations and \\n\\nis  the  synaptic  weight \\n\\n© American Institute of Physics 1988 \\n\\n\\x0c574 \\n\\na large number of interconnects.  The challenge in VLSI is therefore to design a  sim(cid:173)\\nple,  compact  synapse  that  can  be  repeated  to  build  a  VLSI  neural  network  with \\nIn  a  network  with  fixed  functionality,  this  is  relatively \\nmanageable  interconnect. \\nstraightforward.  H the  network  is to be able to learn,  however,  the synaptic weights \\nmust  be programmable, and therefore more complicated. \\n\\n2. DESIGNING  A NEURAL  NETWORK IN  VLSI \\n\\nThere  are  fundamentally  two  approaches  to  implementing  any  function  in  silicon  -\\ndigital and analog.  Each technique has its advantages and  disadvantages,  and these \\nare  listed  below,  along  with  the  merits  and  demerits  of bit  - serial  architectures  in \\ndigital (synchronous) systems. \\nDigital  vs.  analog:  The  primary  advantage  of digital  design  for  a  synapse  array  is \\nthat  digital  memory  is  well  understood,  and  can  be  incorporated  easily.  Learning \\nnetworks are  therefore  possible  without  recourse  to unusual  techniques  or technolo(cid:173)\\ngies.  Other strengths of a digital approach are that design techniques are advanced, \\nautomated  and  well  understood  and  noise  immunity  and  computational  speed  can \\nbe  high.  Unattractive features  are  that  digital  circuits  of this complexity need  to  be \\nsynchronous  and  all  states  and  activities  are  quantised,  while  real  neural  networks \\nare  asynchronous  and  unquantised.  Furthermore,  digital  multipliers  occupy  a  large \\nsilicon  area, giving a low synapse count on  a single chip. \\nThe  advantages  of  analog  circuitry  are  that  asynchronous  behaviour  and  smooth \\nneural  activation  are  automatic.  Circuit  elements can  be  small,  but  noise  immunity \\nis relatively  low  and  arbitrarily  high  precision is not  possible.  Most  importantly,  no \\nreliable  analog,  non  - volatile  memory  technology  is  as  yet  readily  available.  For \\nthis  reason,  learning  networks  lend  themselves  more  naturally to  digital  design  and \\nimplementation. \\nSeveral  groups  are  developing  neural  chips  and  boards,  and  the  following  listing \\ndoes  not  pretend  to  be  exhaustive.  It is  included,  rather,  to indicate  the spread  of \\nactivity  in  this  field.  Analog  techniques  have  been  used  to  build  resistor  I  opera(cid:173)\\ntional  amplifier  networks [2,3]  similar to  those  proposed  by  Hopfield  and Tank [4]. \\nA  large  group  at  Caltech  is  developing  networks  implementing  early  vision  and \\nauditory  processing  functions  using the intrinsic nonlinearities of MaS transistors in \\nthe subthreshold  regime  [5,6].  The problem of implementing analog  networks with \\nelectrically  programmable  synapses  has  been  addressed  using  CCDIMNOS technol(cid:173)\\nogy  [7].  Finally,  Garth  [8]  is  developing  a  digital  neural  accelerator  board  (\"Net(cid:173)\\nsim\")  that  is  effectively  a  fast  SIMD  processor  with  supporting  memory  and  com(cid:173)\\nmunications chips. \\nBit - serial  vs.  bit  - parallel:  Bit  - serial  arithmetic and  communication  is  efficient \\nfor  computational  processes,  allowing  good  communication  within  and  between \\nVLSI  chips  and  tightly  pipelined  arithmetic  structures.  It  is  ideal  for  neural  net(cid:173)\\nworks  as  it  minimises  the  interconnect  requirement  by  eliminating  multi  - wire \\nbusses.  Although  a  bit  - parallel  design  would  be  free  from  computational  latency \\n(delay  between  input  and  output),  pipelining  makes  optimal  use  of  the  high  bit  -\\nrates possible in serial systems,  and  makes for  efficient circuit usage. \\n2.1  An asynchronous pulse stream VLSI neural network: \\nIn  addition  to  the  digital  system  that  forms  the  substance  of  this  paper,  we  are \\ndeveloping  a  hybrid  analOg/digital  network  family.  This work  is  outlined  here,  and \\nhas  been  reported  in  greater  detail  elsewhere  [9, 10, 11].  The  generic  (logical  and \\nlayout)  architecture  of a  single  network  of n  totally  interconnected neurons is  shown \\n\\n\\x0c575 \\n\\nschematically  in  figure  1.  Neurons  are  represented  by  circles,  which  signal  their \\nstates,  Vi  upward  into  a  matrix  of  synaptic  operators.  The  state  signals  are  con(cid:173)\\nnected  to  a  n  - bit  horizontal  bus  running  through  the  synaptic  array,  with  a  con(cid:173)\\nnection  to  each  synaptic  operator  in  every  column.  All  columns  have  n  operators \\n(denoted  by  squares)  and  each  operator adds its synaptic contribution,  Tij V j\\n,  to the \\nrunning  total  of  activity  for  the  neuron  i  at  the  foot  of  the  column.  The  synaptic \\nfunction  is  therefore  to  multiply  the  signalling  neuron  state,  Vj\\n,  by  the  synaptic \\nweight,  Tij ,  and  to  add  this  product  to  the  running  total.  This  architecture  is com(cid:173)\\nmon to both  the bit - serial and pulse - stream networks. \\n\\nSynapse \\n\\nStates { Vj  } \\n\\nFigure 1. Generic architecture for  a  network of n totally interconnected neurons. \\n\\nNeurons \\n\\nj=O \\n\\nj=II -1 \\n\\nThis type of architecture has many attractions for  implementation in 2  - dimensional \\nsilicon  as  the  summation  2  Tij Vj  is  distributed  in  space.  The  interconnect \\nrequirement  (n  inputs  to  each  neuron)  is  therefore  distributed  through  a  column, \\nreducing the need  for  long - range wiring.  The architecture is modular,  regular and \\ncan be easily expanded. \\nIn  the  hybrid  analog/digital  system,  the  circuitry  uses  a  \"pulse  stream\"  signalling \\nmethod  similar  to  that  in  a  natural  neural  system.  Neurons  indicate  their  state  by \\nthe  presence  or  absence  of  pulses  on  their  outputs,  and  synaptic  weighting  is \\nachieved  by  time  - chopping  the  presynaptic  pulse  stream  prior  to  adding  it  to  the \\npostsynaptic  activity  summation.  It  is  therefore  asynchronous  and  imposes  no fun(cid:173)\\ndamental  limitations  on  the  activation  or  neural  state.  Figure  2  shows  the  pulse \\nstream  mechanism  in  more  detail.  The synaptic  weight  is  stored  in  digital  memory \\nlocal to the operator.  Each synaptic operator has an  excitatory and inhibitory  pulse \\nstream  input  and  output.  The  resultant  product  of  a  synaptic  operation,  Tij Vj\\n,  is \\nadded  to  the  running  total  propagating  down  either  the  excitatory  or  inhibitory \\nchannel.  One binary bit  (the  MSBit)  of the  stored  Tij  determines whether  the con(cid:173)\\ntribution  is excitatory or inhibitory. \\nThe  incoming  excitatory  and  inhibitory  pulse  stream  inputs  to  a  neuron  are \\nintegrated  to  give  a  neural  activation  potential  that varies  smoothly  from  0  to  5  V. \\nThis  potential controls a  feedback  loop with  an odd number of logic  inversions and \\n\\n\\x0c576 \\n\\n. • • \\n\\nXT •• \\n\\nV , \\n.u.u, \\n• \\n\\nFigure  2.  Pulse  stream  arithmetic.  Neurons  are  denoted  by  0  and synaptic  operators \\nby  D. \\n\\nthus  forms  a  switched  \"ring - oscillator\".  H the inhibitory input dominates,  the feed(cid:173)\\nback  loop  is  broken.  H  excitatory  spikes  subsequently  dominate  at  the  input,  the \\nneural activity rises  to 5V and the feedback  loop oscillates with  a period determined \\nby a  delay  around  the loop.  The resultant  periodic waveform is then converted to a \\nseries  of voltage  spikes,  whose  pulse  rate  represents  the  neural  state,  Vi\\'  Interest(cid:173)\\ningly,  a  not  dissimilar  technique is  reported  elsewhere  in this volume,  although  the \\nsynapse function  is executed differently [12]. \\n\\n3. A 5  - STATE BIT - SERIAL NEURAL  NETWORK \\n\\nThe  overall  architecture  of  the  5  - state  bit  - serial  neural  network  is  identical  to \\nthat  of  the  pulse  stream  network.  It  is  an  array  of n 2  interconnected  synchronous \\nsynaptic  operators,  and  whereas  the  pulse  stream  method  allowed  Vj  to  assume  all \\nvalues  between  \"off\\' and  \"on\",  the  5 - state network VJ  is constrained  to 0,  ±0.5 Qr \\n± 1.  The resultant  activation  function  is  shown  in  Figure 3.  Full  digital  multiplica(cid:173)\\ntion  is  costly  in  silicon  area,  but  multiplication  of  Tij  by  Vj  =  0.5  merely  requires \\nthe synaptic  weight  to be right  - shifted  by  1 bit.  Similarly,  multiplication  by  0.25 \\ninvolves  a  further  right  - shift  of Til\\'  and  multiplication  by 0.0  is  trivially  easy.  VJ \\n<  0 is not  problematic,  as  a  switchable adder/subtractor  is  not much  more complex \\nthan  an  adder.  Five  neural  states  are  therefore  feasible  with  circuitry  that  is  only \\nslightly more complex  than  a  simple serial adder.  The neural state expands from a  1 \\nbit  to  a  3  bit  (5  - state)  representation,  where  the  bits  represent  \"add/subtract?\", \\n\"shift?\" and \"multiply by O?\". \\nFigure 4  shows  part of the synaptic  array.  Each synaptic operator includes an 8 bit \\nshift  register  memory  block  holding  the  synaptic  weight,  Til\\'  A  3  bit  bus  for  the  5 \\nneural  states  runs  horizontally  above  each  synaptic  row.  Single  phase  dynamic \\nCMOS  has  been  used  with  a  clock  frequency  in  excess  of 20  MHz  [13).  Details of \\na synaptic operator are  shown  in  figure 5.  The synaptic weight  Til  cycles around the \\nshift  register  and  the  neural  state  Vj  is  present  on  the  state  bus.  During  the  first \\nclock  CYCle,  the  synaptic  weight  is  multiplied  by  the  neural  state  and  during  the \\nsecond,  the  most  significant  bit (MSBit)  of the resultant  Tij Vj  is sign  - extended for \\n\\n\\x0c577 \\n\\nlHRESHOLD \\n\\nState VJ \\n\\n..... -------=-------.. Activity sJ \\n\\ns· \\n\\n\"5  STATE\" \\n\\n\"Sharper\" \\n\\n\"Smoother\" \\n\\n~.....::~-\"\\'--x.&..t------ Activity \"J \\n\\nFigure 3.  \"Hard - threshold\",  5  - state and sigmoid activation functions. \\n\\nJ-a-1T  v \\n~  ..  J \\nJ-li \\n\\nv, \\n\\nv, \\n\\nFigure 4.  Section  of the  synaptic  array  of the  5  - state activation function  neural net(cid:173)\\nwork. \\n\\n8  bits  to  allow  for  word  growth  in  the  running  summation.  A  least  significant  bit \\n(LSBit)  signal  running down  the  synaptic  columns indicates the arrival  of the LSBit \\nof  the  Xj  running  total.  If  the  neural  state  is  ±O.5  the  synaptic  weight  is  right \\nshifted  by  1 bit and then added to or subtracted from  the running total.  A  multipli(cid:173)\\ncation  of  ± 1  adds  or  subtracts  the  weight  from  the  total  and  multiplication  by  0 \\n\\n\\x0c578 \\n\\n.0.5 \\n.0.0 \\n\\nAdd/Subtract \\n\\nAdd! \\nSubtract \\n\\nCarry \\n\\nFigure S.  The  synaptic operator with a 5 - state activation function. \\n\\ndoes not alter the running summation. \\nThe  final  summation  at  the  foot  of the  column  is  thresholded  externally  according \\nto  the  5  - state activation function  in  figure  3.  As  the  neuron activity Xj\\'  increases \\nthrough  a  threshold  value  x\" \\nideal  sigmoidal  activation  represents  a  smooth  switch \\nof  neural  state  from  -1  to  1.  The 5  - state  \"staircase\"  function  gives a  superficially \\nmuch  better  approximation  to  the  sigmoid  form  than  a  (much  simpler  to  imple(cid:173)\\nment)  threshold  function.  The  sharpness  of  the  transition  can  be  controlled  to \\n\"tune\"  the  neural dynamics for  learning and computation.  The control parameter is \\nreferred  to  as  temperature  by  analogy  with  statistical  functions  with  this  sigmoidal \\nform.  High  \"temperature\" gives a  smoother staircase and sigmoid,  while a tempera(cid:173)\\nture  of  0  reduces  both  to  the  \\'\\'Hopfield\\'\\'  - like  threshold  function.  The  effects  of \\ntemperature  on  both  learning  and  recall  for  the  threshold  and  5  - state  activation \\noptions are discussed in section 4. \\n\\n4. LEARNING AND  RECALL  WITH VLSI  CONSTRAINTS \\n\\nBefore  implementing  the  reduced  - arithmetic  network  in  VLSI,  simulation  experi(cid:173)\\nments  were  conducted  to  verify  that  the  5  - state  model  represented  a  worthwhile \\nenhancement  over  simple  threshold  activation.  The  \"benchmark\"  problem  was \\nchosen  for  its  ubiquitousness,  rather  than  for  its  intrinsic  value.  The  implications \\nfor  learning  and  recall  of the  5  - state  model,  the  threshold  (2  - state)  model  and \\n- state)  were  compared  at  varying  temperatures \\nsmooth  sigmoidal  activation  (  00 \\nIn  each  simulation  a  totally \\nwith  a  restricted  dynamic  range  for  the  weights  Tij • \\ninterconnected  64  node  network  attempted  to  learn  32  random  patterns  using  the \\ndelta  rule  learning  algorithm  (see  for  example  [14]).  Each  pattern  was  then  cor(cid:173)\\nrupted  with  25%  noise  and  recall  attempted  to  probe  the  content  addressable \\nmemory properties under the three different activation options. \\nDuring  learning,  individual  weights  can  become  large  (positive  or  negative).  When \\nweights  are  \"driven\"  beyond  the  maximum  value  in  a  hardware  implementation, \\n\\n\\x0c579 \\n\\nwhich  is  determined  by  the  size  of  the  synaptic  weight  blocks,  some  limiting \\nmechanism  must  be  introduced.  For  example,  with  eight  bit  weight  registers,  the \\nlimitation is  -128  S  Tij  S  127.  With integer weights,  this can be seen to be a prob(cid:173)\\nlem  of  dynamic  range,  where  it  is  the  relationship  between  the  smallest  possible \\nweight  (± 1) and the largest  (+ 127/-128) that is the issue. \\nResults:  Fig.  6  shows  examples  of the  results  obtained,  studying  learning  using  5  -\\nstate  activation  at  different  temperatures,  and  recall  using  both  5  - state  and  thres(cid:173)\\nhold  activation.  At  temperature  T=O,  the  5  - state  and  threshold  models  are \\ndegenerate,  and  the results identical.  Increasing smoothness of activation  (tempera(cid:173)\\nture)  during  learning  improves  the  quality  of  learning  regardless  of  the  activation \\nfunction  used  in  recall,  as more patterns are recognised  successfully.  Using 5 - state \\nactivation  in recall  is more effective  than simple  threshold  activation.  The effect of \\ndynamic  range  restrictions  can  be  assessed  from  the  horizontal  axis,  where  T/j:6.  is \\nshown.  The results  from  these and  many  other experiments may  be  summarised  as \\nfollows:-\\n5 - State activation  vs.  threshold: \\n1)  Learning with 5  - state activation was  protracted  over the threshold  activation, \\nas  binary  patterns  were  being  learnt,  and  the  inclusion  of  intermediate  values \\nadded extra degrees of freedom. \\n\\n2)  Weight  sets  learnt  using  the  5  - state  activation  function  were  \"better\"  than \\nthose  learnt  via  threshold  activation,  as  the  recall  properties  of both  5  - state \\nand  threshold  networks  using  such  a  weight  set  were  more  robust  against \\nnoise. \\nFull  sigmoidal  activation  was  better  than  5  - state,  but  the  enhancement  was \\nless  significant  than  that  incurred  by  moving  from  threshold  - 5 - state.  This \\nsuggests  that the law  of diminishing returns  applies to  addition of levels to the \\nneural  state  Vi\\'  This  issue  has  been  studied  mathematically  [15],  with  results \\nthat agree  qualitatively with  ours. \\n\\n3) \\n\\nWeight Saturation: \\nThree  methods  were  tried  to  deal  with  weight  saturation.  Firstly,  inclusion  of  a \\ndecay,  or  \"forgetting\"  term  was  included  in  the  learning  cycle  [1].  It  is  our  view \\nthat  this  technique can  produce the desired weight limiting property,  but in  the time \\navailable  for  experiments,  we  were  unable  to  \"tune\"  the  rate  of  decay  sufficiently \\nwell  to  confirm  it.  Renormalisation  of the  weights  (division  to  bring large  weights \\nback  into  the  dynamic  range)  was  very  unsuccessful,  suggesting  that  information \\ndistributed  throughout  the  numerically small  weights  was  being  destroyed.  Finally, \\nthe  weights were  allowed  to  \"clip\"  (ie any weight  outside the dynamic range  was  set \\nto  the  maximum  allowed  value).  This method  proved  very  successful,  as  the learn(cid:173)\\ning  algorithm  adjusted the weights  over which  it still  had control  to  compensate for \\nthe  saturation effect.  It is  interesting to note  that  other experiments have indicated \\nthat  Hopfield  nets  can  \"forget\"  in a  different  way,  under different learning control, \\ngiving  preference  to  recently acquired  memories [16].  The results  from  the  satura(cid:173)\\ntion experiments were:-\\n1) \\n\\nFor  the  32  pattemJ64  node  problem,  integer  weights  with  a  dynamic  range \\ngreater than  ±30 were necessary to give enough  storage capability. \\nFor weights  with  maximum  values  TiJ  = 50-70,  \"clipping\"  occurs,  but  net(cid:173)\\nwork  performance  is  not  seriously  degraded  over  that  with  an  unrestricted \\nweight set. \\n\\n2) \\n\\n\\x0c580 \\n\\n15 \\n\\n\"0  10 \\nc = \\n.2 \\nen e u \\n5 --~ \\n\\n0 \\n\\n0 \\n\\nI \\n\\n\".\\' \\n\\n., ... \\n\\n.... ----------\\n\\n,-\\ne  ~ ;A ....... ;.. f:\\'-:\\' :::::7.:::.::-:::-: f\\'-. \\n,  ,. \\ni \\n! \\n! , \\ni \\nI \\nI , \\n\\n20  30 \\n\\n40  50  60  70 \\n\\nLimit \\n\\n15 \\n\\nT=30  _._.-.-\\nT=20 \\nT=10 \\nT=O \\n\\n-.-._.-.. \\n\\n,.. .•. -..... -.•. _ .•. \\n, \\n.. \\ni \\nj\\'\\'\\'\\'--\\n,,\\'i \\n\\n- . . .,. \\'\" \\n\\nj \\n\\n~-------------\\n••••••• •••••••••••••••• •••••• \\n\\nj \\nI \\n\\nO~~~~--~~ __ ~~ __ \\no \\n\\n20  30  40  50  60  70 \\n\\nLimit \\n\\n5 . state activation function  recal1 \\n\\ntlHopficld\" activation  function  recall \\n\\nFigure 6.  Recall  of patterns  learned  with  the  5  .  state  activation function  and  subse(cid:173)\\nquently restored using  the 5-state and the  hard - threshold activation functions. \\nT  is  the  \"temperature\",  or smoothness  of the  activation function,  and \"limit\"  the  value \\nofTI; ·  \\n\\nThese  results  showed  that  the  5  - state  model  was  worthy  of implementation  as  a \\nVLSI neural board, and suggested that 8 - bit weights were sufficient. \\n\\nS.  PROJECTED SPECIFICATION OF A HARDWARE NEURAL  BOARD \\n\\nThe specification of a  64  neuron board is  given  here,  using a  5 - state bit  - serial 64 \\nx 64  synapse array with  a derated clock speed  of 20 MHz.  The synaptic weights are \\n8  bit words and the word  length  of the running summation XI  is  16  bits to  allow for \\ngrowth.  A  64  synapse  column  has  a  computational  latency  of  80  clock  cycles  or \\nbits,  giving  an  update  time  of 4 .... s  for  the  network.  The  time  to  load  the  weights \\ninto  the  array  is  limited  to  6O .... s  by  the  supporting  RAM,  with  an  access  time  of \\n12Ons.  These  load  and  update  times  mean  that  the  network  is  executing  1  x  10\\' \\noperations/second,  where  one  operation  is  ±  Tlj  Vj •  This  is  much  faster  than  a \\nnatural  neural  network,  and  much  faster  than  is  necessary  in  a  hardware  accelera(cid:173)\\ntor.  We  have  therefore  developed  a  \"paging\"  architecture,  that  effectively  \"trades -\\noff\" some of this excessive speed against increased network size. \\nA  \"moving  - patch\"  neural  board:  An  array  of  the  5  - state  synapses  is  currently \\nbeing  fabricated  as  a  VLSI  integrated  circuit.  The  shift  registers  and \\nthe \\nadderlsubtractor for  each  synapse  occupy a  disappointingly large silicon  area,  allow(cid:173)\\ning only a  3  x 9 synaptic  array.  To achieve  a  suitable size  neural  network  from  this \\narray,  several chips need to be  included on a  board with  memory and control circu(cid:173)\\nitry.  The  \"moving  patch\"  concept  is  shown  in  figure  7,  where  a  small  array  of \\nsynapses is passed over a much larger n  x n  synaptic array. \\nEach  time  the  array  is  \"moved\"  to  represent  another set  of  synapses,  new  weights \\nmust be  loaded  into it.  For example,  the  first  set of weights will  be T 11  •. ,  T;J  ... T 21 \\n...  T 2j  to Tjj ,  the second  set  Tj + 1,l  to T u  etc..  The final  weight  to be loaded will  be \\n\\n\\x0c581 \\n\\nn  neurons .. om synaptic array \\n\\nSmaller \"Patch\" \\n\\nmoves over array \\n\\nrr~ _____ ) __ -.. \\n> \\n~\\'-\\n\\nFigure 7.  The  \"moving  patch\" concept,  passing  a  small synaptic \"patch\"  over  a larger \\nrun synapse array. \\n\\nTNt·  Static,  off - the  - shelf RAM is  used  to store the weights and the  whole opera(cid:173)\\ntion  is  pipelined for  maximum efficiency.  Figure 8 shows the board level design for \\nthe network. \\n\\nSynaptic  Accelerator Chips \\n\\nControl \\n\\nHOST \\nFigure 8. A  \"moving  patch\" neural network board. \\n\\nThe small  \"patch\" that moves  around  the array  to  give  n  neurons comprises 4 VLSI \\nsynaptic accelerator chips to give  a 6 x 18 synaptic array. The number of neurons to \\nbe  simulated  is 256  and  the weights for  these  are stored  in 0.5  Mb of RAM  with a \\nload  time  of 8ms.  For  each  \"patch\"  movement,  the  partial  runnin~ summatinn \\n\\n;. \\n\\n\\x0c582 \\n\\ncalculated  for  each  column,  is  stored  in  a  separate  RAM  until  it is  required  to  be \\nadded  into  the  next  appropriate  summation.  The  update  time  for  the  board  is  3ms \\ngiving  2  x  107  operations/second.  This  is  slower  than  the  64  neuron  specification, \\nbut  the  network  is  16  times  larger,  as  the  arithmetic  elements are  being  used  more \\nefficiently.  To  achieve  a  network  of  greater  than  256  neurons,  more  RAM  is \\nrequired to store the weights.  The network is then slower unless a larger number of \\naccelerator chips is  used  to give  a larger moving \"patch\". \\n\\n6.  CONCLUSIONS \\n\\nA  strategy  and  design  method  has  been  given  for  the  construction  of  bit  - serial \\nVLSI neural network chips and  circuit  boards.  Bit - serial  arithmetic,  coupled  to  a \\nreduced  arithmetic  style,  enhances  the  level  of  integration  possible  beyond  more \\nconventional digital,  bit - parallel schemes.  The restrictions imposed  on both synap(cid:173)\\ntic  weight  size  and  arithmetic  precision  by  VLSI  constraints  have  been  examined \\nand shown to be tolerable,  using the associative memory problem as a test. \\nWhile  we  believe  our  digital  approach  to  represent  a  good  compromise  between \\narithmetic  accuracy  and  circuit  complexity,  we  acknowledge  that  the  level  of \\nintegration  is  disappointingly  low. \\nIt  is  our  belief  that,  while  digital  approaches \\nmay  be interesting and  useful  in the medium  term,  essentially as  hardware accelera(cid:173)\\ntors for  neural simulations,  analog techniques represent the best  ultimate option in 2 \\n- dimensional  silicon.  To this  end,  we  are currently pursuing techniques for  analog \\nIn any  event,  the  full \\npseudo  - static  memory,  using  standard  CMOS  technology. \\ndevelopment  of a  nonvolatile  analog  memory  technology,  such  as  the  MNOS  tech(cid:173)\\nnique [7],  is key to the long - term  future of VLSI neural nets that can learn. \\n\\n7. ACKNOWLEDGEMENTS \\n\\nThe  authors  acknowledge  the  support  of  the  Science  and  Engineering  Research \\nCouncil (UK) in the execution of this work. \\n\\nReferences \\n\\n1. \\n\\nS.  Grossberg,  \"Some  Physiological  and  Biochemical  Consequences  of Psycho(cid:173)\\nlogical Postulates,\" Proc.  Natl.  Acad.  Sci.  USA,  vol.  60,  pp.  758  - 765,  1968. \\n\\n2.  H.  P.  Graf,  L.  D.  Jackel,  R.  E.  Howard,  B.  Straughn,  J.  S.  Denker,  W. \\nHubbard,  D.  M.  Tennant,  and  D.  Schwartz,  \"VLSI  Implementation  of  a \\nNeural  Network  Memory  with  Several  Hundreds  of  Neurons,\"  Proc.  AlP \\nConference on Neural Networks for  Computing.  Snowbird,  pp.  182 - 187,  1986. \\n3.  W.  S.  Mackie,  H.  P.  Graf,  and  J.  S.  Denker,  \"Microelectronic  Implementa(cid:173)\\n\\ntion  of  Connectionist  Neural  Network  Models,\"  IEEE  Conference  on  Neural \\nInformation Processing Systems.  Denver,  1987. \\nJ . J. Hopfield  and D.  W.  Tank, \"Neural\" Computation of Decisions in  Optim(cid:173)\\nisation Problems,\" BioI.  Cybern.,  vol.  52,  pp.  141  - 152,  1985. \\n\\n4. \\n\\n5.  M.  A.  Sivilotti,  M.  A.  Mahowald,  and  C.  A.  Mead, Real - Time  Visual Com(cid:173)\\n\\nputations Using  Analog CMOS  Processing Arrays, 1987.  To be published \\n\\n6.  C.  A.  Mead,  \"Networks  for  Real  - Time  Sensory  Processing,\"  IEEE  Confer(cid:173)\\n\\nence  on  Neural Information  Processing Systems,  Denver,  1987. \\n\\n\\x0c583 \\n\\n7. \\n\\n8. \\n\\nJ.  P.  Sage,  K.  Thompson.  and  R. S.  Withers,  \"An Artificial Neural  Network \\nIntegrated  Circuit  Based on MNOSlCCD  Principles,\"  Proc. AlP Conference on \\nNeural Networlcs for Computing,  Snowbird,  pp.  381  - 385,  1986. \\nS.  C.  J.  Garth, \"A Chipset for  High Speed  Simulation of Neural Network  Sys(cid:173)\\ntems,\"  IEEE Conference on Neural Networlc.s,  San Diego,  1987. \\n\\n9.  A.  F.  Murray and  A.  V.  W.  Smith,  \"A Novel  Computational  and  Signalling \\nMethod  for  VLSI Neural Networks,\"  European  Solid State Circuits Conference \\n, 1987. \\n\\n10.  A.  F.  Murray  and  A.  J.  W.  Smith,  \"Asynchronous  Arithmetic  for  VLSI \\n\\nNeural Systems,\"  Electronics Letters, vol.  23, no.  12, p.  642, June, 1987. \\n\\n11.  A.  F.  Murray  and  A.  V.  W.  Smith,  \"Asynchronous  VLSI  Neural  Networks \\n\\nusing  Pulse  Stream  Arithmetic,\"  IEEE  Journal  of Solid-State  Circuits  and Sys(cid:173)\\ntems,  1988.  To be published \\n\\n12.  M.  E.  Gaspar,  \"Pulsed  Neural  Networks:  Hardware,  Software  and  the  Hop(cid:173)\\nfield  AID  Converter  Example,\"  IEEE  Conference  on  Neural  Information  Pro(cid:173)\\ncessing Systems.  Denver,  1987. \\n\\n13.  M.  S.  McGregor,  P.  B.  Denyer,  and A.  F.  Murray,  \"A Single - Phase  Clock(cid:173)\\ning Scheme for  CMOS  VLSI,\"  Advanced Research  in  VLSI  \" Proceedings of the \\n1987 Stanford Conference,  1987. \\n\\n14.  D.  E.  Rumelhart,  G.  E.  Hinton,  and  R.  J.  Williams,  \"Learning  Internal \\nRepresentations  by  Error  Propagation,\"  Parallel  Distributed  Processing  \" \\nExplorations  in  the  Microstructure of Cognition,  vol.  1,  pp.  318 - 362,  1986. \\n\\n15.  M.  Fleisher  and  E.  Levin,  \"The  Hopfiled  Model  with  Multilevel  Neurons \\nModels,\"  IEEE  Conference  on  Neural  Information  Processing  Systems.  Denver, \\n1987. \\n\\n16.  G.  Parisi,  \"A  Memory  that  Forgets,\"  J.  Phys.  A  .\\'  Math.  Gen.,  vol.  19,  pp. \\n\\nL617  - L620,  1986. \\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b3VhW4ba8sv"
      },
      "source": [
        "# Pre Processing Papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMx-Snm9a_d4"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\",disable=['parser', 'tagger', 'ner'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfQG1W6tqlwu"
      },
      "source": [
        "def clear_new_line(textt):\n",
        "  '''\n",
        "  replaces '\\n' with white space and then several white spaces with one.\n",
        "  '''\n",
        "  return re.sub(' +', ' ', textt.replace('\\n',' '))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWEr8G8obBz6"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"\\w+\\'?\\w+|\\w+\")\n",
        "\n",
        "def make_token(textt):\n",
        "    return tokenizer.tokenize(str(textt))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDKfLnFZbB2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46137abf-11da-43f3-d45f-1a9ce5c716f6"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = stopwords.words('english')\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "stop_words = set(stop_words_nltk).union(STOP_WORDS)\n",
        "stop_words.add('https')\n",
        "stop_words.add('nan')\n",
        "\n",
        "def remove_stopwords(textt):\n",
        "    return [token for token in textt if token not in stop_words]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP2XXim93yuD"
      },
      "source": [
        "for i in string.ascii_lowercase:\n",
        "  if i not in stop_words:\n",
        "    stop_words.add(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPz_GQtp3JC2"
      },
      "source": [
        "for i in stop_words:\n",
        "  if len(i)==1:\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzEypRSU3lYX"
      },
      "source": [
        ""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJwCU0NMbB42"
      },
      "source": [
        "def lemmatization(textt):\n",
        "    lemma_result = []\n",
        "    \n",
        "    for words in textt:\n",
        "        doc = nlp(words)\n",
        "        for token in doc:\n",
        "            lemma_result.append(token.lemma_)\n",
        "    return lemma_result"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuPpv5A0xy3g"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from typing import List\n",
        "\n",
        "def remove_numbers_from_doc(doc:str)->str:\n",
        "    \"\"\"\n",
        "    Removes numbers from given text.\n",
        "    \"\"\"\n",
        "    if(type(doc)==str):\n",
        "        text_no_nums = re.sub(r'[0-9]+', '',doc)\n",
        "        return text_no_nums"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "qgINW64nxePd",
        "outputId": "0089ca08-05f8-466d-b1ef-caf5033152e5"
      },
      "source": [
        "papers_df.loc[1,'full_text'].lower()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1 \\n\\nconnectivity versus entropy \\n\\nyaser  s.  abu-mostafa \\n\\ncalifornia  institute  of technology \\n\\npasadena, ca 91125 \\n\\nabstract \\n\\nhow  does  the  connectivity  of a  neural  network  (number  of synapses  per \\nneuron)  relate  to  the complexity  of the  problems  it  can  handle  (measured  by \\nthe entropy)?  switching theory would suggest no relation at all, since all boolean \\nfunctions  can be  implemented  using  a  circuit  with very  low  connectivity  (e.g., \\nusing  two-input  nand  gates).  however,  for  a  network  that  learns  a  problem \\nfrom  examples  using  a  local  learning  rule,  we  prove  that  the  entropy  of  the \\nproblem becomes  a  lower  bound for  the connectivity of the network. \\n\\nintroduction \\n\\nthe most  distinguishing feature of neural networks  is  their  ability to spon(cid:173)\\n\\ntaneously  learn  the  desired  function  from  \\'training\\' samples,  i.e.,  their  ability \\nto  program themselves.  clearly,  a  given  neural  network  cannot just learn  any \\nfunction,  there  must  be  some  restrictions  on which  networks  can  learn which \\nfunctions.  one obvious restriction, which is  independent of the learning aspect, \\nis  that  the network  must  be big enough  to  accommodate the  circuit  complex(cid:173)\\nity of the function  it  will  eventually simulate.  are there restrictions  that  arise \\nmerely from the fact  that the network  is  expected to learn the function,  rather \\nthan being purposely designed for the function?  this paper reports a restriction \\nof this kind. \\n\\nthe result imposes a  lower bound on the connectivity of the network  (num(cid:173)\\n\\nber  of synapses  per neuron).  this  lower  bound  can  only  be  a  consequence of \\nthe learning aspect, since switching theory provides purposely designed circuits \\nof low  connectivity  (e.g.,  using only  two-input  nand  gates)  capable of imple(cid:173)\\nmenting any boolean function  [1,2] .  it also follows  that the learning mechanism \\nmust  be restricted for  this  lower  bound to hold;  a  powerful  mechanism can be \\n\\n© american institute of physics 1988 \\n\\n\\x0c2 \\n\\ndesigned  that will  find  one of the low-connectivity circuits  (perhaps byexhaus(cid:173)\\ntive search), and hence the lower bound on connectivity cannot hold in general. \\nindeed, we  restrict the learning mechanism to be local;  when  a  training sample \\nis  loaded into the network, each neuron has access only to those bits carried by \\nitself and  the neurons  it  is  directly  connected  to.  this  is  a  strong  assumption \\nthat excludes sophisticated learning mechanisms used in neural-network models, \\nbut may be more plausible from a  biological point of view. \\n\\nthe  lower  bound  on  the  connectivity  of the  network  is  given  in  terms  of \\nthe entropy of the environment that provides the training samples.  entropy is  a \\nquantitative measure of the disorder or randomness in an environment or, equiv(cid:173)\\nalently,  the  amount  of information  needed  to specify  the environment.  there \\nare many different ways to define entropy, and many technical variations of this \\nconcept  [3].  in the next section,  we shall  introduce  the formal  definitions  and \\nresults, but we start here with an informal exposition of the ideas  involved. \\n\\nthe  environment  in  our  model  produces  patterns  represented  by  n  bits \\nx  =  xl ••• x n  (pixels in the picture of a visual scene if you will).  only h different \\npatterns can be generated by  a  given environment, where  h  < 2n  (the entropy \\nis  essentially  log2 h).  no  knowledge  is  assumed  about  which  patterns  the  en(cid:173)\\nvironment  is  likely  to generate, only  that there  are  h  of them.  in the  learning \\nprocess,  a  huge  number of sample  patterns  are  generated  at random from  the \\nenvironment  and input to the network,  one  bit per neuron.  the network  uses \\nthis  information to set its  internal parameters  and gradually tune itself to this \\nparticular environment.  because of the network architecture, each neuron knows \\nonly  its own bit and (at best)  the bits of the neurons it is  directly connected to \\nby  a  synapse.  hence,  the learning  rules  are local:  a  neuron  does  not have  the \\nbenefit of the entire global pattern that is  being learned. \\n\\nafter the learning process has taken place, each neuron is  ready to perform \\na  function  defined  by  what  it  has  learned.  the  collective  interaction  of  the \\nfunctions of the neurons is what defines the overall function of the network.  the \\nmain  result  of this  paper  is  that  (roughly  speaking)  if the  connectivity  of the \\nnetwork  is  less  than the entropy of the environment, the network  cannot learn \\nabout the environment.  the idea of the proof is  to show that if the connectivity \\nis  small,  the  final  function  of each  neuron  is  independent  of the environment, \\nand hence to conclude that the overall network has accumulated no information \\nabout the environment it is  supposed to learn about. \\n\\nformal result \\n\\na neural network is an undirected graph (the vertices are the neurons and the \\nedges are the synapses).  label the neurons 1\"\", n  and define kn  c  {i\"\", n} \\nto  be  the  set  of neurons  connected  by  a  synapse  to  neuron  n,  together  with \\nneuron n  itself.  an environment is  a subset e  c  {o,i}n  (each x  e  e  is  a sample \\n\\n\\x0c3 \\n\\nfrom  the  environment).  during  learning,  xl,\"\\', xn  (the bits  of x)  are loaded \\ninto  the  neurons  1\"\", n,  respectively.  consider  an  arbitrary  neuron  nand \\nrelabel  everything  to make  kn  become  {i\"\", k}.  thus  the  neuron  sees  the \\nfirst  k  coordinates of each x. \\n\\nsince our result  is  asymptotic  in  n, we  will specify  k  as  a  function  of n; \\nk  = a.n  where  a.  = a.(n)  satifies  limn-+oo a.(n)  = 0.0  (0  < 0.0  < 1).  since  the \\nresult  is  also statistical, we will  consider the ensemble of environments e \\n\\ne=e(n)={ec{o,i}n  i  lel=h} \\n\\nwhere  h  =  2~n and  /3  = /3(n)  satifies  limn-+oo /3(n)  = /30  (0  < /30  <  1).  the \\nprobability  distribution on e is  uniform;  any  environment  e e  e is  as  likely  to \\noccur as  any other. \\n\\nthe  neuron  sees  only  the  first  k  coordinates  of each  x  generated  by  the \\nenvironment  e.  for  each  e,  we  define  the function  n  :  {o,i}k  -+  {o, 1,2,··.} \\nwhere \\n\\nn(al\" .ak) = i{x eel  xle  = ale  for  k = 1,\\'\" ,k}i \\n\\nand the normalized version \\n\\nthe function  v  describes the relative frequency of occurrence for  each of the 2k \\nbinary vectors xl\\'\"  xk  as x  =  xl ••• xn runs through all h vectors in e.  in other \\nwords,  v  specifies  the projection of e  as  seen by  the neuron.  clearly, veal  > 0 \\nfor  all a e  {o,l}k  and lae{o,l}k veal  =  1. \\n\\ncorresponding to two environments el and e2, we will have two functions  vi \\nand v2.  it vi  is  not distinguishable from v2,  the neuron cannot tell the difference \\nbetween  el  and  e2\\'  the  distinguishability  between  vi  and v2  can be measured \\nby \\n\\niv1(a)  - v2(a) i \\n\\n1 \\n\\nd(vl,v2)  = - 2: \\n2 ae{o,l}k \\n\\nthe  range  of d(vb v2)  is  0  <  d(vl\\' v2)  <  1, where  \\'0\\'  corresponds  to complete \\nindistinguishability  while  \\'1\\'  corresponds  to  maximum  distinguishability.  we \\nare now  in  a  position to state the main result. \\nlet el and e2 be independently selected environments from e according to the \\nuniform probability distribution.  d(vl\\' v2)  is now  a random variable, and we  are \\ninterested in the expected value  e(d(vl\\' v2))\\'  the case where  e(d(vb v2))  =  0 \\ncorresponds to the neuron getting no information about the environment, while \\nthe  case  where  e(d(vb v2))  =  1  corresponds  to  the  neuron  getting  maximum \\ninformation.  the theorem predicts, in the limit, one of these extremes depending \\non how the connectivity  (0.0) compares to the entropy  (/30)\\' \\n\\n\\x0c4 \\n\\ntheorem. \\n1.  h  q o  > po  , then limn ..... co e (d(vi, v2))  =  1. \\n2.  h  q o  < po  , then limn ..... co e  (d(v}, v2))  =  o. \\n\\nthe proof is  given  in the appendix,  but the idea is  easy  to illustrate  infor(cid:173)\\nmally.  suppose  h  =  2k +10  (corresponding to part  2  of the theorem).  for  most \\nenvironments  e  e  e,  the  first  k  bits  of x  e  e go  through  all  2k  possible  val(cid:173)\\nues  approximately  210  times each as  x  goes  through  all  h  possible values  once. \\ntherefore, the patterns seen by the neuron are drawn from the fixed ensemble of \\nall binary vectors of length k  with essentially uniform probability distribution, \\ni.e.,  v  is  the same for  most  environments.  this  means  that,  statistically,  the \\nneuron will  end  up  doing  the same  function  regardless  of the environment  at \\nhand. \\n\\nwhat about the opposite case, where h  =  2k - 10  (corresponding to part lof \\nthe theorem)?  now,  with only 2k - 10  patterns available from  the environment, \\nthe  first  k  bits  of x  can assume  at  most  2k- 10  values  out  of the  possible  2k \\nvalues a binary vector of length k  can assume in principle.  furthermore, which \\nvalues  can  be  assumed  depends  on  the  particular  environment  at  hand,  i.e., \\nv  does  depend  on the environment.  therefore,  although  the  neuron still  does \\nnot  have  the global  picture,  the  information  it  has  says  something  about  the \\nenvironment. \\n\\nacknowledgement \\n\\nthis work was supported by the air force office of scientific research under \\n\\ngrant  afosr-86-0296. \\n\\nappendix \\n\\nin this  appendix we  prove the main theorem.  we  start by  discussing some \\nbasic  properties  about  the ensemble  of environments  e.  since  the  probability \\ndistribution on e is  uniform and since ie i =  e:), we  have \\n\\npr(e)  = \\n\\n( 2n)-1 \\n\\nh \\n\\nwhich  is  equivalent  to  generating  e  by  choosing  h  elements  x  e  {o,l}n  with \\nuniform probability (without replacement).  it follows  that \\n\\nh \\npr(x e  e)  = 2n \\n\\n\\x0c5 \\n\\nh  h-l \\npr(xl e  e ,  x2  e  e)  = 2n  x  2n _  1 \\n\\nand so on. \\n\\nthe functions  n  and  v  are defined on k-bit vectors.  the statistics of n(a) \\n\\n(a random variable for  fixed a)  is  independent of a \\n\\npr(n(at} = m)  = pr(n(a2) = m) \\n\\nwhich follows  from the symmetry with respect to each bit of a.  the same holds \\nfor  the statistics of v(a).  the expected value e(n(a))  =  h2-k  (h  objects going \\ninto  2k  cells), hence e(v(a))  =  2- k .  we  now restate and prove the theorem. \\n\\ntheorem. \\n1.  if ao > po  , then limn_oo e  (d(vt, v2))  = 1. \\n2.  if ao < po  , then limn_oo e  (d(vt, v2))  = 0. \\n\\nproof. \\n\\nwe expand e  (d(vt, v2))  as follows \\n\\nwhere nl and n2  denote nl(o. ··0) and n2(0·· ·0), respectively, and the last step \\nfollows  from the fact  that the statistics of nl(a)  and  n2(a)  is  independent  of a. \\ntherefore, to prove the theorem, we  evaluate e(lnl - n21)  for  large  n. \\n\\n1.  assume  ao > po.  let  n  denote  n(o··· 0),  and consider  pr(n = 0).  for n  to \\nbe zero,  all  2n - k  strings  x  of n  bits  starting with  k  o\\'s  must  not  be  in  the \\nenvironment  e.  hence \\n\\npr(n = 0)  = (1  - - )  (1  -\\n\\nh \\n2n \\n\\nh \\n\\n2n  - 1 \\n\\n) ... (1  -\\n\\nh \\n\\n) \\n2n  - 2n- k  + 1 \\n\\nwhere  the first  term is  the probability  that 0· . ·00 f/.  e,  the second term is  the \\n\\n\\x0c6 \\n\\nprobability that o· .. 01  ~ f  given that o· .. 00  ~ f,  and so on. \\n\\n> (1- 2n _h2n _ k )\\'n-k \\n=  (1- h2- n(1- 2-k)-1) 2n - k \\n>  (1  - 2h2-n)2n - k \\n> 1- 2h2-n 2n - k \\n=  1- 2h2-k \\n\\nhence,  pr(nl = 0)  = pr(n2  = 0)  = pr(n = 0)  > 1 - 2h2-k •  however,  e(nl)  = \\ne( n2)  = h2-k.  therefore, \\n\\ne(lnl - n2\\\\)  = llpr(nl =  i,n2 =  j)li - jl \\n\\n\"  \" \\n\\ni=o;=o \\n\\n\"  \" \\n\\n= l l pr(nl =  i)pr(n2  =  j) ii - jl \\n\\ni=o;=o \\n\\n\" \\n> l pr(nl = 0)pr(n2 =  j)j \\n;=0 \\n\" + l pr(nl = i)pr(n2 = o)i \\n\\ni=o \\n\\nwhich follows  by throwing away  all the terms where neither i  nor j  is  zero  (the \\nterm where both i  an j  are zero appears twice for convenience, but this term is \\nzero  anyway). \\n\\n= pr(nl = 0)e(n2) + pr(n2  =  o)e(nl) \\n> 2(1 - 2h2-k )h2-k \\n\\nsubstituting this estimate in the expression for  e(d(vb v2)),  we get \\n\\ne(d(vl, v2))  =  2h e(lnl - n21) \\n\\n2k \\n\\nx  2(1 - 2h2- k )h2-k \\n\\n2k \\n> -\\n- 2h \\n=  1- 2h2- k \\n= 1 - 2  x  2(,8-a)n \\n\\nsince a o  > 130  by  assumption, this lower bound goes  to 1  as  n  goes to infinity. \\nsince  1 is  also  an upper bound for  d( vi, v2)  (and hence an upper bound for  the \\nexpected value e(d(vl, v2))) , limn_oo e(d(vl, v2))  must be 1. \\n\\n\\x0c7 \\n\\n2.  assume  a o < po.  consider \\n\\ne(lnl - n21)  = e  (i(nl - h2-k) -\\n\\n(n2  - h2- k )i) \\n\\n< e(\\\\nl - h2- k \\\\ + in2  - h2-ki) \\n= e(\\\\nl - h2- k i)  + e(ln2 - h2-k i) \\n= 2e(ln - h2-ki) \\n\\nto  evaluate  e(ln  - h2- k i),  we  estimate  the  variance  of  n  and  use  the  fact \\nthat  e(ln - h2- k i)  <  ..jvar(n)  (recall  that  h2-k  =  e(n»).  since  var(n)  = \\ne(n2) -\\n(e(n))2,  we  need an estimate for  e(n2).  we write n  =  e.e{o,l}n-k 6., \\nwhere \\n\\n6  -\\n•  -\\n\\nif 0 .. ·oa e  e· \\n, \\n\\n{ 1 \\n, \\n0,  otherwise. \\n\\nin this notation, e(n2 )  can be written as \\n\\ne(n2) = e  (i: \\n\\ni: \\n\\n6.6t,) \\n\\n.e{o,l}n-k be{o,l}n-k \\ni: \\n\\nl \\n\\ne(6.6t,) \\n\\n.e{o,l}n-k be{o,l}n-k \\n\\nfor the \\'diagonal\\' terms  (a =  b), \\n\\ne(6.6.)  = pr(6. = 1) \\n\\n= h2-n \\n\\nthere  are  2n - k  such  diagonal  terms,  hence  a  total  contribution  of  2n - k  x \\nh2- n  =  h2- k  to the sum.  for the \\'off-diagonal\\' terms  (a \\'# b), \\n\\ne(6.6b )  = pr( 6. = 1,6b = 1) \\n\\n= pr(6. = 1)pr(6b = 116. = 1) \\n\\nh  h-l \\n=-x--::-:::--\\n2n \\n2n_1 \\n\\nthere are 2n - k (2n - k  -1) such off-diagonal terms, hence a  total contribution of \\n2n - k(2 n - k  -1) x  2;~:n~1) < (h2-k)2 2~~1 to the sum.  putting the contributions \\n\\n\\x0c8 \\n\\nfrom the diagonal and off-diagonal terms together, we  get \\n\\ne(n2) < h2-k + (h2-k)2 2n  _  1 \\nvar(n) = e(n2) -\\n\\n(e(n))2 \\n\\n2n \\n\\n<  (h2- k  + (h2- k )\\' 2:: 1) - (h2- k )\\' \\n= h2-k + (h2 - k)2----:-:-_ \\n2n  -1 \\n\\n1 \\n\\n( \\n\\nh2- k ) \\n=  h2-k  1 + ---:-:--\\n2n -1 \\n< 2h2-k \\n\\nthe last step follows since h2-k is  much smaller than 2n -1. therefore, e(ln-\\nh2- k i)  < vvar(n) <  (2h2- k)?i.  substituting this estimate in the expression for \\ne( d( vb v2)),  we  get \\n\\n1 \\n\\ne(d(vb v2))  =  2h e(lnl - n21) \\n\\n2k \\n\\n2k \\n\\n<  2h  x  2e(ln - h2- ki) \\n\\n2k \\n\\n1 \\n\\n<  2h  x  2 x  (2h2-k)?i \\n_  (  2k) ~ \\n- 2-\\nh \\n=  v\\'2 x  2~(q-~)n \\n\\nsince a o < po  by assumption, this upper bound goes  to 0 as  n  goes to infinity. \\nsince  0  is  also  a  lower  bound for  d(vb v2)  (and  hence  a  lower  bound for  the \\nexpected value  e(d(vb v2))),  limn_oo e(d(vb v2))  must be o .• \\n\\nreferences \\n\\n[1]  y.  abu-mostafa,  \"neural  networks  for  computing?,\"  alp  conference  pro(cid:173)\\nceedings #  151,  neural networks for  computing, j. denker (ed.), pp.  1-6, 1986. \\n\\n[2]  z.  kohavi,  switching and finite  automata  theory, mcgraw-hill, 1978. \\n\\n[3]  y.  abu-mostafa,  \"the complexity  of information extraction,\"  ieee  trans. \\non information  theory, vol.  it-32,  pp.  513-525, july  1986. \\n\\n[4]  y. abu-mostafa,  \"complexity in neural systems,\"  in analog  vlsi and neural \\nsystems by c.  mead,  addison-wesley,  1988. \\n\\n\\x0c'"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyE4AWfhc0Ls"
      },
      "source": [
        "def pipeline(textt):\n",
        "  textt1 = remove_numbers_from_doc(textt.lower())\n",
        "  textt2 = clear_new_line(textt1)\n",
        "  textt3 = make_token(textt2)\n",
        "  textt4 = remove_stopwords(textt3)\n",
        "  return lemmatization(textt4)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPvpHGWJc0OL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094a4ee2-4cfa-4beb-9649-92cda7efafa2"
      },
      "source": [
        "%%time\n",
        "papers_df['abstract'] = papers_df['abstract'].astype(str)\n",
        "\n",
        "papers_df['abstract_clean'] = papers_df['abstract'].apply(lambda row: pipeline(row))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 28.2 s, sys: 94.1 ms, total: 28.3 s\n",
            "Wall time: 28.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQPxECsFbCPe"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKkqJ50_bPtQ"
      },
      "source": [
        "## First Way - Via Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8-JOGeYbDew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e674d3fd-b809-446f-b34a-0faeb21d544a"
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "texts = papers_df['abstract_clean']\n",
        "\n",
        "dictionary_abstracts = Dictionary(texts)\n",
        "corpus_abstracts = [dictionary_abstracts.doc2bow(text) for text in texts]\n",
        "\n",
        "import numpy\n",
        "numpy.random.seed(1) # setting random seed to get the same results each time.\n",
        "\n",
        "from gensim.models import ldamodel\n",
        "model_abstracts = ldamodel.LdaModel(corpus_abstracts, id2word=dictionary_abstracts, num_topics=10, minimum_probability=1e-8)\n",
        "model_abstracts.show_topics()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.021*\"nan\" + 0.016*\"capsule\" + 0.015*\"equivariant\" + 0.008*\"GCNs\" + 0.007*\"VQA\" + 0.006*\"MARL\" + 0.006*\"BN\" + 0.005*\"DTW\" + 0.004*\"Gromov\" + 0.004*\"GCN\"'),\n",
              " (1,\n",
              "  '0.030*\"We\" + 0.017*\"method\" + 0.014*\"learn\" + 0.013*\"datum\" + 0.012*\"network\" + 0.010*\"train\" + 0.009*\"base\" + 0.008*\"time\" + 0.007*\"propose\" + 0.007*\"neural\"'),\n",
              " (2,\n",
              "  '0.019*\"github\" + 0.017*\"https\" + 0.011*\"RNN\" + 0.010*\"Code\" + 0.009*\"LSTM\" + 0.009*\"RNNs\" + 0.009*\"observational\" + 0.008*\"DNNs\" + 0.008*\"Transformer\" + 0.007*\"intervention\"'),\n",
              " (3,\n",
              "  '0.034*\"algorithm\" + 0.015*\"problem\" + 0.012*\"gradient\" + 0.010*\"function\" + 0.008*\"In\" + 0.008*\"matrix\" + 0.007*\"1\" + 0.007*\"sample\" + 0.007*\"convergence\" + 0.007*\"The\"'),\n",
              " (4,\n",
              "  '0.005*\"nan\" + 0.005*\"replay\" + 0.004*\"VAEs\" + 0.004*\"SVGD\" + 0.004*\"ResNet\" + 0.004*\"Normalization\" + 0.003*\"GANs\" + 0.003*\"NTK\" + 0.003*\"decentralise\" + 0.003*\"2018\"'),\n",
              " (5,\n",
              "  '0.020*\"game\" + 0.019*\"communication\" + 0.014*\"fairness\" + 0.008*\"teach\" + 0.007*\"f\" + 0.007*\"worker\" + 0.007*\"strategy\" + 0.007*\"adversary\" + 0.006*\"Wasserstein\" + 0.006*\"auction\"'),\n",
              " (6,\n",
              "  '0.016*\"attention\" + 0.013*\"DNN\" + 0.011*\"fair\" + 0.009*\"mathcal\" + 0.006*\"disentangle\" + 0.006*\"SGD\" + 0.006*\"bite\" + 0.005*\"RNNs\" + 0.005*\"hash\" + 0.005*\"seller\"'),\n",
              " (7,\n",
              "  '0.107*\"nan\" + 0.008*\"GNNs\" + 0.004*\"ZO\" + 0.003*\"AdaMM\" + 0.003*\"areal\" + 0.003*\"SW\" + 0.003*\"RSNNs\" + 0.003*\"SAEs\" + 0.003*\"ProxSVRG\" + 0.003*\"SLOPE\"'),\n",
              " (8,\n",
              "  '0.065*\"model\" + 0.025*\"learn\" + 0.011*\"process\" + 0.009*\"propose\" + 0.009*\"The\" + 0.009*\"experiment\" + 0.007*\"In\" + 0.006*\"variable\" + 0.006*\"algorithm\" + 0.006*\"approach\"'),\n",
              " (9,\n",
              "  '0.044*\"GAN\" + 0.019*\"discriminator\" + 0.018*\"GANs\" + 0.018*\"Adversarial\" + 0.015*\"Generative\" + 0.011*\"mathbf\" + 0.011*\"encoder\" + 0.008*\"keypoints\" + 0.008*\"cloud\" + 0.007*\"CNN\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VkRXkr1yVCr",
        "outputId": "fa5045d7-7026-45e3-fe7e-32af4307b2ad"
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "texts2 = papers_df['abstract_clean']\n",
        "\n",
        "dictionary_abstracts2 = Dictionary(texts2)\n",
        "corpus_abstracts2 = [dictionary_abstracts2.doc2bow(text) for text in texts2]\n",
        "\n",
        "import numpy\n",
        "numpy.random.seed(1) # setting random seed to get the same results each time.\n",
        "\n",
        "from gensim.models import ldamodel\n",
        "model_abstracts2 = ldamodel.LdaModel(corpus_abstracts2, id2word=dictionary_abstracts2, num_topics=10, minimum_probability=1e-8)\n",
        "model_abstracts2.show_topics()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.026*\"algorithm\" + 0.022*\"problem\" + 0.016*\"function\" + 0.010*\"regret\" + 0.010*\"optimization\" + 0.009*\"convex\" + 0.009*\"bound\" + 0.009*\"set\" + 0.009*\"result\" + 0.009*\"optimal\"'),\n",
              " (1,\n",
              "  '0.023*\"network\" + 0.018*\"train\" + 0.017*\"base\" + 0.015*\"learn\" + 0.013*\"time\" + 0.011*\"provide\" + 0.011*\"image\" + 0.010*\"neural\" + 0.008*\"task\" + 0.008*\"use\"'),\n",
              " (2,\n",
              "  '0.032*\"agent\" + 0.018*\"word\" + 0.010*\"topic\" + 0.009*\"disentangle\" + 0.008*\"language\" + 0.007*\"anchor\" + 0.006*\"model\" + 0.006*\"cloud\" + 0.006*\"plan\" + 0.005*\"goal\"'),\n",
              " (3,\n",
              "  '0.037*\"gan\" + 0.016*\"rnn\" + 0.014*\"rnns\" + 0.014*\"transformer\" + 0.012*\"dnn\" + 0.012*\"gans\" + 0.011*\"ode\" + 0.010*\"discriminator\" + 0.010*\"sketch\" + 0.009*\"lstm\"'),\n",
              " (4,\n",
              "  '0.020*\"block\" + 0.013*\"path\" + 0.011*\"observational\" + 0.006*\"direction\" + 0.006*\"worker\" + 0.005*\"admm\" + 0.004*\"problem\" + 0.004*\"method\" + 0.004*\"svgd\" + 0.004*\"set\"'),\n",
              " (5,\n",
              "  '0.036*\"method\" + 0.024*\"datum\" + 0.011*\"model\" + 0.011*\"linear\" + 0.011*\"demonstrate\" + 0.008*\"propose\" + 0.008*\"analysis\" + 0.007*\"technique\" + 0.007*\"problem\" + 0.006*\"neural\"'),\n",
              " (6,\n",
              "  '0.039*\"communication\" + 0.013*\"github\" + 0.011*\"energy\" + 0.009*\"adversary\" + 0.008*\"proximal\" + 0.008*\"worker\" + 0.007*\"cnn\" + 0.007*\"vae\" + 0.007*\"speaker\" + 0.007*\"transport\"'),\n",
              " (7,\n",
              "  '0.039*\"model\" + 0.026*\"learn\" + 0.020*\"algorithm\" + 0.012*\"datum\" + 0.010*\"method\" + 0.009*\"propose\" + 0.008*\"problem\" + 0.008*\"gradient\" + 0.007*\"process\" + 0.006*\"sample\"'),\n",
              " (8,\n",
              "  '0.027*\"al\" + 0.026*\"et\" + 0.014*\"architecture\" + 0.012*\"attribute\" + 0.011*\"deep\" + 0.010*\"net\" + 0.009*\"kernel\" + 0.008*\"monte\" + 0.008*\"carlo\" + 0.008*\"shoot\"'),\n",
              " (9,\n",
              "  '0.035*\"game\" + 0.021*\"distance\" + 0.015*\"strategy\" + 0.013*\"wasserstein\" + 0.012*\"right\" + 0.009*\"teach\" + 0.008*\"leave\" + 0.007*\"dnns\" + 0.007*\"github\" + 0.007*\"graph\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n0s5G4SbVMe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyJthq4FbVP0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sampFq5fbVwR"
      },
      "source": [
        "## Second Way - Using Scikit Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwIrsnlgbec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f223fbf5-de83-43cf-e36d-f5068df74c34"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_df=0.9, min_df=2, stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
        "papers_tf_idf = tfidf.fit_transform(papers_df['abstract'])\n",
        "\n",
        "papers_tf_idf"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<9680x9946 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 435667 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDCgj6UMbfjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a5d44f-99c7-4b46-b06c-d3e1c6707434"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "LDA = LatentDirichletAllocation(n_components=15, random_state=101)\n",
        "\n",
        "# This can take a while, we are dealing with large number of documents here\n",
        "LDA.fit(papers_tf_idf)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                          evaluate_every=-1, learning_decay=0.7,\n",
              "                          learning_method='batch', learning_offset=10.0,\n",
              "                          max_doc_update_iter=100, max_iter=10,\n",
              "                          mean_change_tol=0.001, n_components=15, n_jobs=None,\n",
              "                          perp_tol=0.1, random_state=101, topic_word_prior=None,\n",
              "                          total_samples=1000000.0, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnZW9E4ibjUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dcfd21-c028-43e9-e6cf-b5e04bd6a3b5"
      },
      "source": [
        "# What topics did we get?\n",
        "\n",
        "for index, topic in enumerate(LDA.components_):\n",
        "    print(f\"THE TOP 15 WORDS FOR TOPIC #{index}\")\n",
        "    list_keywords = [tfidf.get_feature_names()[index] for index in topic.argsort()[-10:]]\n",
        "    print(list_keywords)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE TOP 15 WORDS FOR TOPIC #0\n",
            "['clique', 'bernstein', 'abc', 'motifs', 'market', 'schatten', 'mri', 'cloud', 'captions', 'nnz']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #1\n",
            "['vicinity', 'mre', 'byzantine', 'pc', 'couple', 'multiset', 'sbm', 'angular', 'contours', 'contour']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #2\n",
            "['hamiltonian', 'count', 'marginals', 'interventions', 'pomdps', 'mcmc', 'bo', 'copula', 'observational', 'causal']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #3\n",
            "['echo', 'persistence', 'steepest', 'fd', 'phoneme', 'als', 'bcd', 'speaker', 'taxonomy', 'voice']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #4\n",
            "['unitary', 'u', 'nu', 'bn', 'delays', 'kappa', 'mmd', 'cfr', 'vqa', 'lifted']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #5\n",
            "['image', 'deep', 'training', 'network', 'networks', 'neural', 'models', 'data', 'learning', 'model']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #6\n",
            "['violations', 'dcnns', 'volumetric', 'retina', 'assemblies', 'ts', 'attractor', 'buyer', 'mtl', 'captioning']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #7\n",
            "['nnls', 'slam', 'psd', 'quartet', 'commute', 'srht', 'oblique', 'batching', 'cirl', 'nan']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #8\n",
            "['dac', 'vcr', 'metastable', 'ventral', 'spore', 'mixup', 'variates', 'resting', 'degeneracy', 'adagrad']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #9\n",
            "['teaching', 'stimuli', 'synaptic', 'population', 'fairness', 'activity', 'brain', 'stimulus', 'spike', 'neurons']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #10\n",
            "['coalescent', 'hmm', 'routing', 'pds', 'capsules', 'nns', 'mips', 'ib', 'rf', 'capsule']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #11\n",
            "['comparator', 'affected', 'iht', 'factorizing', 'boldsymbol', 'sliced', 'sw', 'thoroughly', 'intensities', 'dtw']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #12\n",
            "['spd', 'std', 'progresses', 'dn', 'seriation', 'automl', 'regressive', 'abstention', 'sat', 'hogwild']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #13\n",
            "['stragglers', 'langle', 'rangle', 'triplets', 'gtd', 'integrators', 'apg', 'pes', 'listwise', 'sg']\n",
            "\n",
            "\n",
            "THE TOP 15 WORDS FOR TOPIC #14\n",
            "['gradient', 'method', 'n', 'optimization', 'function', 'data', 'learning', 'algorithms', 'problem', 'algorithm']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UjRfUc86wC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}